<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="https://fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      How we fine-tuned HAProxy to achieve 2,000,000 concurrent SSL connections &middot; Sachin Malhotra
    
  </title>

  <!-- CSS -->
  <script defer src="https://use.fontawesome.com/releases/v5.0.2/js/all.js"></script>
  <link rel="stylesheet" href="/public/css/custom.css">
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="layout-reverse">

    <div class="sidebar">
  <div class="sidebar-logo" style="align:center">
        <img src="http://www.gravatar.com/avatar/e51882718aefa4b1acf0f59ab2edc916?s=350" />
  </div>
  <div id="contact-list1" style="text-align:center">
    <!-- contact icons go here --> 
    
      <a href="https://github.com/edorado93">
          <span class="fa-stack fa-medium">
              <i class="fab fa-github fa-stack-2x"></i>
          </span>
      </a>
    
    
      <a href="https://www.linkedin.com/in/edorado93">
          <span class="fa-stack fa-medium">
              <i class="fab fa-linkedin fa-stack-2x"></i>
          </span>
      </a>
    
    
      <a href="https://www.medium.com/@sachinmalhotra">
          <span class="fa-stack fa-medium">
              <i class="fab fa-medium fa-stack-2x"></i>
          </span>
      </a>
    
    
      <a href="https://www.instagram.com/edorado93">
          <span class="fa-stack fa-medium">
              <i class="fab fa-instagram fa-stack-2x"></i>
          </span>
      </a>
    
    
      <a href="https://www.twitter.com/edorado93">
          <span class="fa-stack fa-medium">
              <i class="fab fa-twitter fa-stack-2x"></i>
          </span>
      </a>
    
  </div>
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a class="name" href="/">
          Sachin Malhotra
        </a>
      </h1>  
      <p class="lead">Programming made fun</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

      <a class="sidebar-nav-item" href="https://edorado.github.io/assets/cv.pdf">CV</a>
      <a class="sidebar-nav-item" href="https://edorado.github.io">GitHub project</a>
      <span class="sidebar-nav-item">Currently v2.1.0</span>
    </nav>

    <p>&copy; 2018. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">How we fine-tuned HAProxy to achieve 2,000,000 concurrent SSL connections</h1>
  <span class="post-date">16 Apr 2017</span>
  <!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>How we fine-tuned HAProxy to achieve 2,000,000 concurrent SSL connections</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 100%;
        margin: auto;
      }
      section {
        width: 100%;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 100%;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">How we fine-tuned HAProxy to achieve 2,000,000 concurrent SSL connections</h1>
</header>
<section data-field="subtitle" class="p-summary">
If you look at the above screenshot closely, you’ll find two important pieces of information:
</section>
<section data-field="body" class="e-content">
<section name="a192" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="6de9" id="6de9" class="graf graf--h3 graf--leading graf--title"></h3></div><div class="section-inner sectionLayout--outsetColumn"><figure name="b6df" id="b6df" class="graf graf--figure graf--layoutOutsetCenter graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 653px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 65.3%;"></div><img class="graf-image" data-image-id="1*H541cEeKLF2O_7wBoUOlPw.png" data-width="1704" data-height="1112" src="https://cdn-images-1.medium.com/max/1000/1*H541cEeKLF2O_7wBoUOlPw.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="82c9" id="82c9" class="graf graf--p graf-after--figure">If you look at the above screenshot closely, you’ll find two important pieces of information:</p><ol class="postList"><li name="e60e" id="e60e" class="graf graf--li graf-after--p">This machine has <strong class="markup--strong markup--li-strong">2.38 million TCP connections</strong> established, and</li><li name="d6d6" id="d6d6" class="graf graf--li graf-after--li">The amount of RAM being used is around <strong class="markup--strong markup--li-strong">48 Gigabytes</strong>.</li></ol><p name="f76c" id="f76c" class="graf graf--p graf-after--li">Pretty awesome right? What would be even more awesome is if someone provided the setup components, and the tunings required to achieve this kind of scale on a single HAProxy machine. Well, I’ll do just that in this post ;)</p><p name="afd2" id="afd2" class="graf graf--p graf-after--p">This is the final part of the multipart series on load testing HAProxy. If you have time, I recommend you go and read the first two parts in the series first. These will help you get the hang of the kernel level tunings required on all the machines in this setup.</p><div name="c70c" id="c70c" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/@sachinmalhotra/load-testing-haproxy-part-1-f7d64500b75d" data-href="https://medium.com/@sachinmalhotra/load-testing-haproxy-part-1-f7d64500b75d" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@sachinmalhotra/load-testing-haproxy-part-1-f7d64500b75d"><strong class="markup--strong markup--mixtapeEmbed-strong">Load Testing HAProxy (Part-1)</strong><br><em class="markup--em markup--mixtapeEmbed-em">Load Testing ? HAProxy ? If all this seems greek to you, don’t worry. I will provide inline links to read up on what…</em>medium.com</a><a href="https://medium.com/@sachinmalhotra/load-testing-haproxy-part-1-f7d64500b75d" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="bc71cf29ef089d2de5825936e454fd3a" data-thumbnail-img-id="1*4npSurj6b2n__CsxxnaUQw.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*4npSurj6b2n__CsxxnaUQw.png);"></a></div><div name="af15" id="af15" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://medium.com/@sachinmalhotra/load-testing-haproxy-part-2-4c8677780df6" data-href="https://medium.com/@sachinmalhotra/load-testing-haproxy-part-2-4c8677780df6" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@sachinmalhotra/load-testing-haproxy-part-2-4c8677780df6"><strong class="markup--strong markup--mixtapeEmbed-strong">Load Testing HAProxy (Part 2)</strong><br><em class="markup--em markup--mixtapeEmbed-em">This is the second part in the 3 part series on performance testing of the famous TCP load balancer and reverse proxy…</em>medium.com</a><a href="https://medium.com/@sachinmalhotra/load-testing-haproxy-part-2-4c8677780df6" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="47a280510ad193d480b7bb21d23dd851" data-thumbnail-img-id="1*s2S16ZXbIxtsYIG87aOadg.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*s2S16ZXbIxtsYIG87aOadg.png);"></a></div><p name="4fb3" id="4fb3" class="graf graf--p graf-after--mixtapeEmbed">There are a lot of small components that helped us bring together the entire setup and achieve these numbers.</p><p name="a293" id="a293" class="graf graf--p graf-after--p">Before I tell you the final HAProxy configuration we used (if you’re super impatient you can scroll to the bottom) I want to build up to it by walking you through our thinking.</p><h3 name="1658" id="1658" class="graf graf--h3 graf-after--p">What we wanted to test</h3><p name="f375" id="f375" class="graf graf--p graf-after--h3">The component we want to test was HAProxy version 1.6. We are using this in production right now on 4 core, 30 Gig machines. However, all the connectivity is non-SSL based.</p><p name="23e2" id="23e2" class="graf graf--p graf-after--p">We wanted to test two things out of this exercise:</p><ol class="postList"><li name="177f" id="177f" class="graf graf--li graf-after--p">The <strong class="markup--strong markup--li-strong">CPU percentage increase</strong> when we shift the entire load from non-SSL connections to SSL connections. The CPU usage should definitely increase, owing to the longer 5-way handshake and then the packet encryption.</li><li name="c019" id="c019" class="graf graf--li graf-after--li">Secondly, we wanted to test the limits of our current production setup in terms of number of requests and the max number of concurrent connections that can be supported before performance starts to degrade.</li></ol><p name="99f5" id="99f5" class="graf graf--p graf-after--li">We required the first part because of a major feature rollout that’s in full swing, which requires communication over SSL. We required the second part so that we could reduce the amount of hardware dedicated in production to HAProxy machines.</p><h3 name="ee55" id="ee55" class="graf graf--h3 graf-after--p">The Components Involved</h3><ul class="postList"><li name="55ae" id="55ae" class="graf graf--li graf-after--h3">Multiple client machines to stress the HAProxy.</li><li name="df41" id="df41" class="graf graf--li graf-after--li">Single HAProxy machine version 1.6 on various setups<br>* 4 core, 30 Gig<br>* 16 core, 30 Gig<br>* 16 core, 64 Gig</li><li name="bf32" id="bf32" class="graf graf--li graf-after--li">Backend servers that will help support all these concurrent connections.</li></ul><h3 name="fcb5" id="fcb5" class="graf graf--h3 graf-after--li">HTTP and MQTT</h3><p name="1843" id="1843" class="graf graf--p graf-after--h3">If you’ve gone through the <a href="https://medium.com/@sachinmalhotra/load-testing-haproxy-part-1-f7d64500b75d" data-href="https://medium.com/@sachinmalhotra/load-testing-haproxy-part-1-f7d64500b75d" class="markup--anchor markup--p-anchor" target="_blank">first article</a> in this series, you should know that our entire infrastructure is supported over two protocols:</p><ul class="postList"><li name="a21c" id="a21c" class="graf graf--li graf-after--p">HTTP and</li><li name="81e8" id="81e8" class="graf graf--li graf-after--li">MQTT.</li></ul><p name="aeec" id="aeec" class="graf graf--p graf-after--li">In our stack, we don’t use HTTP 2.0 and hence don’t have the functionality of persistent connections on HTTP. So on production the max number of TCP connections that we see is somewhere around (2 * 150k) on a single HAProxy machine (Inbound + Outbound). Although the number of concurrent connections is rather low, the number of requests per second is quite high.</p><p name="a86c" id="a86c" class="graf graf--p graf-after--p">On the other hand, MQTT is a different way altogether for communication. It offers great quality of service parameters and persistent connectivity as well. So bidirectional continuous communication can happen over a MQTT channel. As for HAProxy that supports MQTT (underlying TCP) connections, we see somewhere around 600–700k TCP connections at the peak time on a single machine.</p><p name="cdcb" id="cdcb" class="graf graf--p graf-after--p">We wanted to do a load test that will give us precise results for both HTTP and MQTT based connections.</p><p name="f4d6" id="f4d6" class="graf graf--p graf-after--p">There are a lot of tools out there that help us load test an HTTP server easily and a lot of these tools provide advanced functionalities like summarized results, converting text based results to graphs, etc. We could not, however, find any stress testing tool for MQTT. We do have a tool that we developed ourselves, but it was not stable enough to support this kind of load in the timeframe we had.</p><p name="ea08" id="ea08" class="graf graf--p graf-after--p">So we decided to go for load testing clients for HTTP and <em class="markup--em markup--p-em">simulating the MQTT setup using the same ;) </em>Interesting right?</p><p name="a2bf" id="a2bf" class="graf graf--p graf-after--p">Well read on.</p><h3 name="4955" id="4955" class="graf graf--h3 graf-after--p">The Initial Setup</h3><p name="942e" id="942e" class="graf graf--p graf-after--h3">This is going to be a long post as I will be providing a lot of details that I think would be really helpful to someone doing similar load testing or fine tunings.</p><ul class="postList"><li name="ba5e" id="ba5e" class="graf graf--li graf-after--p">We took a 16 core 30 Gig machine for setting up HAProxy initially. We did not go with our current production setup because we thought the CPU hit because of SSL termination happening at the HAProxy end would be tremendous.</li><li name="4ca7" id="4ca7" class="graf graf--li graf-after--li">For the server end, we went with a simple NodeJs server that replies with <code class="markup--code markup--li-code">pong</code> on receiving a <code class="markup--code markup--li-code">ping</code> request.</li><li name="3182" id="3182" class="graf graf--li graf-after--li">As for the client, we ended up using <a href="https://httpd.apache.org/docs/2.4/programs/ab.html" data-href="https://httpd.apache.org/docs/2.4/programs/ab.html" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Apache Bench</a> initially. The reason we went with <code class="markup--code markup--li-code">ab</code> was because it was a very well known and stable tool for load testing HTTP end points and also because it provides beautiful summarized results that would help us a lot.</li></ul><p name="3927" id="3927" class="graf graf--p graf-after--li">The <code class="markup--code markup--p-code">ab</code> tool provides a lot of interesting parameters that we used for our load test like:</p><ul class="postList"><li name="ae20" id="ae20" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">- c, concurrency</code> Specifies the number of concurrent requests that would hit the server.</li><li name="0cce" id="0cce" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">-n, no. of requests</code> As the name suggests, specifies the total number of requests of the current load run.</li><li name="6c4e" id="6c4e" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">-p POST file</code> Contains the body of the POST request (if that is what you want to test.)</li></ul><p name="d7f0" id="d7f0" class="graf graf--p graf-after--li">If you look at these parameters closely, you will find that a lot of permutations are possible by tweaking all three. A sample ab request would look something like this</p><pre name="4faf" id="4faf" class="graf graf--pre graf-after--p">ab -S -p post_smaller.txt -T application/json -q -n 100000 -c 3000 http://test.haproxy.in:80/ping</pre><p name="3620" id="3620" class="graf graf--p graf-after--pre">A sample result of such a request looks something like this</p><figure name="f8a9" id="f8a9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 803px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 114.7%;"></div><img class="graf-image" data-image-id="1*R40IjhjQTBDJS6xJ3oMHqg.png" data-width="1458" data-height="1672" src="https://cdn-images-1.medium.com/max/800/1*R40IjhjQTBDJS6xJ3oMHqg.png"></div></figure><p name="a84d" id="a84d" class="graf graf--p graf-after--figure">The numbers that we were interested in were</p><ul class="postList"><li name="524d" id="524d" class="graf graf--li graf-after--p">99% latency.</li><li name="fcd7" id="fcd7" class="graf graf--li graf-after--li">Time per request.</li><li name="9526" id="9526" class="graf graf--li graf-after--li">No. of failed requests.</li><li name="a5e0" id="a5e0" class="graf graf--li graf-after--li">Requests per second.</li></ul><p name="13eb" id="13eb" class="graf graf--p graf-after--li">The biggest problem of <code class="markup--code markup--p-code">ab</code> is that it does not provide a parameter to control the number of requests per second. We had to tweak the concurrency level to get our desired requests per second and this lead to a lot of trail and errors.</p><h3 name="f813" id="f813" class="graf graf--h3 graf-after--p">The Almighty Graph</h3><p name="a1a7" id="a1a7" class="graf graf--p graf-after--h3">We could not randomly go about doing multiple load runs and keep getting results because that would not give us any meaningful information. We had to perform these tests in some specific way so as to get meaningful results out of it. So we followed this graph</p><figure name="5409" id="5409" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 650px; max-height: 500px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 76.9%;"></div><img class="graf-image" data-image-id="1*5NfdO-F4C_1qV4XMKJqNMw.png" data-width="650" data-height="500" src="https://cdn-images-1.medium.com/max/800/1*5NfdO-F4C_1qV4XMKJqNMw.png"></div></figure><p name="b20b" id="b20b" class="graf graf--p graf-after--figure">This graph states that up until a certain point, if we keep increasing the number of requests, the latency will remain almost the same. However, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">beyond a certain tipping point</em></strong>, the latency will start to increase exponentially. It is this tipping point for a machine or for a setup that we intended to measure.</p><h3 name="61e2" id="61e2" class="graf graf--h3 graf-after--p">Ganglia</h3><p name="3d92" id="3d92" class="graf graf--p graf-after--h3">Before providing some test results, I would like to mention <a href="http://ganglia.sourceforge.net/" data-href="http://ganglia.sourceforge.net/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Ganglia</a>.</p><blockquote name="1b06" id="1b06" class="graf graf--blockquote graf-after--p">Ganglia is a scalable distributed monitoring system for high-performance computing systems such as clusters and Grids.</blockquote><p name="199c" id="199c" class="graf graf--p graf-after--blockquote">Look at the following screenshot of one of our machines to get an idea about what ganglia is and what sort of information it provides about the underlying machine.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="e741" id="e741" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 397px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 39.7%;"></div><img class="graf-image" data-image-id="1*ABgypqN1-Cq2l4fRduxZYQ.png" data-width="2748" data-height="1090" src="https://cdn-images-1.medium.com/max/1000/1*ABgypqN1-Cq2l4fRduxZYQ.png"></div></figure><figure name="30eb" id="30eb" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 436px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 43.6%;"></div><img class="graf-image" data-image-id="1*J3uFhy4njFCsISzsJsgYUg.png" data-width="2144" data-height="934" src="https://cdn-images-1.medium.com/max/1000/1*J3uFhy4njFCsISzsJsgYUg.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="98f8" id="98f8" class="graf graf--p graf-after--figure">Pretty interesting, eh?</p><p name="0699" id="0699" class="graf graf--p graf-after--p">Moving on, we constantly monitored ganglia for our HAProxy machine to monitor some important things.</p><ol class="postList"><li name="fb69" id="fb69" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">TCP established</code> This tells us the total number of tcp connections established on the system. NOTE: this is the sum of inbound as well as outbound connections.</li><li name="a32d" id="a32d" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">packets sent and received</code> We wanted to see the total number of tcp packets being sent and received by our HAProxy machine.</li><li name="fb58" id="fb58" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">bytes sent and received</code> This shows us the total data that we sent and received by the machine.</li><li name="b93e" id="b93e" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">memory</code> The amount of RAM being used over time.</li><li name="4c5d" id="4c5d" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">network</code> The network bandwidth consumption because of the packets being sent over the wire.</li></ol><p name="ea96" id="ea96" class="graf graf--p graf-after--li">Following are the known limits found via previous tests/numbers that we wanted to achieve via our load test.</p><blockquote name="3bcd" id="3bcd" class="graf graf--blockquote graf--hasDropCapModel graf-after--p">700k TCP established connections,<br>50k packets sent, 60k packets received, <br>10–15MB bytes sent as well as received, <br>14–15Gig memory at peak, <br>7MB network. <br><code class="markup--code markup--blockquote-code">ALL these values are on a per second basis</code></blockquote><h3 name="9e36" id="9e36" class="graf graf--h3 graf-after--blockquote">HAProxy Nbproc</h3><p name="3423" id="3423" class="graf graf--p graf-after--h3">Initially when we began load testing HAProxy, we found out that with SSL the CPU was being hit pretty early on in the process but the requests per second were very low. On investigating the <a href="http://www.tecmint.com/12-top-command-examples-in-linux/" data-href="http://www.tecmint.com/12-top-command-examples-in-linux/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">top</a> command, we found that HAProxy was using only 1 core. Whereas we had 15 more cores to spare.</p><p name="66b4" id="66b4" class="graf graf--p graf-after--p">Googling for about 10 minutes led us to find this interesting setting in HAProxy that lets HAProxy use multiple cores.</p><p name="903a" id="903a" class="graf graf--p graf-after--p">It’s called <code class="markup--code markup--p-code">nbproc</code> and to get a better hang of what it is and how to set it, check out this article:</p><p name="2528" id="2528" class="graf graf--p graf-after--p"><a href="http://blog.onefellow.com/post/82478335338/haproxy-mapping-process-to-cpu-core-for-maximum" data-href="http://blog.onefellow.com/post/82478335338/haproxy-mapping-process-to-cpu-core-for-maximum" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">http://blog.onefellow.com/post/82478335338/haproxy-mapping-process-to-cpu-core-for-maximum</a></p><p name="d8f5" id="d8f5" class="graf graf--p graf-after--p">Tuning this setting was the base of our load testing strategy moving forward. Because the ability to use multiple cores by HAProxy gave us the power to form multiple combinations for our load testing suite.</p><h3 name="2753" id="2753" class="graf graf--h3 graf-after--p">Load Testing with AB</h3><p name="d63f" id="d63f" class="graf graf--p graf-after--h3">When we had started out with our load testing journey, we were not clear on the things we should be measuring and what we need to achieve.</p><p name="bc2e" id="bc2e" class="graf graf--p graf-after--p">Initially we had only one goal in mind and that was to find the tipping point only by variation of all the below mentioned parameters.</p><figure name="f7f8" id="f7f8" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 585px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 83.6%;"></div><img class="graf-image" data-image-id="1*_YxnDT0z5HOlY95pnbuFJw.png" data-width="1606" data-height="1342" src="https://cdn-images-1.medium.com/max/800/1*_YxnDT0z5HOlY95pnbuFJw.png"></div></figure><p name="426c" id="426c" class="graf graf--p graf-after--figure">I maintained a table of all the results for the various load tests that we gave. All in all I gave over 500 test runs to get to the ultimate result. As you can clearly see, there are a lot of moving parts to each and every test.</p><h4 name="a8e2" id="a8e2" class="graf graf--h4 graf-after--p">Single Client issues</h4><p name="7ddb" id="7ddb" class="graf graf--p graf-after--h4">We started seeing that the client was becoming bottleneck as we kept on increasing our requests per second. Apache bench uses a single core and from the documentation it is evident that it does not provide any feature for using multiple cores.</p><p name="f88f" id="f88f" class="graf graf--p graf-after--p">To run multiple clients efficiently we found an interesting linux utility called <a href="http://www.shakthimaan.com/posts/2014/11/27/gnu-parallel/news.html" data-href="http://www.shakthimaan.com/posts/2014/11/27/gnu-parallel/news.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Parallel</a>. As the name suggests, it helps you run multiple commands in parallel and utilises multiple cores. Exactly what we wanted.</p><p name="b09c" id="b09c" class="graf graf--p graf-after--p">Have a look at a sample command that runs multiple clients using parallel.</p><pre name="6d83" id="6d83" class="graf graf--pre graf-after--p">cat hosts.txt |  parallel  &#39;ab  -S -p post_smaller.txt -T application/json -n 100000 -c 3000 {}&#39;</pre><pre name="cb12" id="cb12" class="graf graf--pre graf-after--pre">sachinm@ip-192-168-0-124:~$ cat hosts.txt<br>http://test.haproxy.in:80/ping<br>http://test.haproxy.in:80/ping<br>http://test.haproxy.in:80/ping</pre><p name="28e5" id="28e5" class="graf graf--p graf-after--pre">The above command would run 3 ab clients hitting the same URL. This helped us remove the client side bottleneck.</p><h4 name="a97a" id="a97a" class="graf graf--h4 graf-after--p">The Sleep and Times parameter</h4><p name="90e5" id="90e5" class="graf graf--p graf-after--h4">We talked about some parameters in ganglia that we wanted to track. Lets discuss them once by one.</p><ol class="postList"><li name="f1e5" id="f1e5" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">packets sent and received</code> This can be simulated by sending some data as a part of the post request. This would also help us generate some <code class="markup--code markup--li-code">network as well as bytes sent and received portions in ganglia</code></li><li name="4a31" id="4a31" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">tcp_established</code> This is something which took us a long, long time to actually simulate in our scenario. Imagine if a single ping request takes about a second, that would take us about 700k requests per second to reach our tcp_established milestone. <br>Now this number might seem easier to achieve on production, but it was impossible to generate it in our scenario.</li></ol><p name="6aa7" id="6aa7" class="graf graf--p graf-after--li">What did we do you might ask? We introduced a sleep parameter in our POST call that specifies the number of milliseconds the server needs to sleep before sending out a response. This would simulate a long running request on production. So now say we have a sleep of about 20 minutes (Yep), that would take us around 583 requests per second to reach the 700k mark.</p><p name="f8bd" id="f8bd" class="graf graf--p graf-after--p">Additionally, we also introduced another parameter in our POST calls to the HAProxy and that was the <code class="markup--code markup--p-code">times</code> parameter. That specified number of times the server should write a response on the tcp connection before terminating it. This helped us simulated even more data transferred over the wire.</p><h4 name="9f9e" id="9f9e" class="graf graf--h4 graf-after--p">Issues with apache bench</h4><p name="6020" id="6020" class="graf graf--p graf-after--h4">Although we found out a lot of results with apache bench, we also faced a lot of issues along the way. I won’t be mentioning all of them here as they are not important for this post as I’ll be introducing another client shortly.</p><p name="6f5f" id="6f5f" class="graf graf--p graf-after--p">We were pretty content with the numbers we were getting out of apache bench, but at one point of time, generating the required tcp connections just became impossible. Somehow the apache bench was not handling the sleep parameter we had introduced, properly and was not scaling for us.</p><p name="7b9f" id="7b9f" class="graf graf--p graf-after--p">Although running multiple ab clients on a single machine was sorted out by using the parallel utility. Running this setup across multiple client machines was still a pain for us. I had not heard of the <a href="https://github.com/grondo/pdsh" data-href="https://github.com/grondo/pdsh" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">pdsh</a> utility by then and was practically stuck.</p><p name="b9a4" id="b9a4" class="graf graf--p graf-after--p">Also, we were not focussing on any timeouts as well. There are some default set of timeouts on the HAProxy, the ab client and the server and we had completely ignored these. We figured out a lot of things along the way and organized ourselves a lot on how to go about testing.</p><p name="3d2c" id="3d2c" class="graf graf--p graf-after--p">We used to talk about the tipping point graph but we deviated a lot from it as time went on. Meaningful results, however, could only be found by focusing on that.</p><p name="9dab" id="9dab" class="graf graf--p graf-after--p">With apache bench a point came where the number of TCP connections were not increasing. We had around 40–45 clients running on 5–6 different client boxes but were not able to achieve the scale we wanted. Theoretically, the number of TCP connections should have jumped as we went on increasing the sleep time, but it wasn’t working for us.</p><h3 name="0904" id="0904" class="graf graf--h3 graf-after--p">Enter Vegeta</h3></div><div class="section-inner sectionLayout--fullWidth"><figure name="da8b" id="da8b" class="graf graf--figure graf--layoutFillWidth graf-after--h3"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.3%;"></div><img class="graf-image" data-image-id="1*KGjslsE-jllJYpItVg84sA.png" data-width="1920" data-height="1080" src="https://cdn-images-1.medium.com/max/2000/1*KGjslsE-jllJYpItVg84sA.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="ee61" id="ee61" class="graf graf--p graf-after--figure">I was searching for some other load testing tools that might be more scalable and better functionality wise as compared to apache bench when I came across V<a href="https://github.com/tsenart/vegeta" data-href="https://github.com/tsenart/vegeta" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">egeta</a>.</p><p name="eafe" id="eafe" class="graf graf--p graf-after--p">From my personal experience, I have seen Vegeta to be extremely scalable and provides much better functionality as compared to apache bench. A single Vegeta client was able to produce the level of throughput equivalent to 15 apache bench clients in our load test.</p><p name="0cf6" id="0cf6" class="graf graf--p graf-after--p">Moving forward, I will be providing load test results that have been tested using Vegeta itself.</p><h3 name="c7fb" id="c7fb" class="graf graf--h3 graf-after--p">Load Testing with Vegeta</h3><p name="b938" id="b938" class="graf graf--p graf-after--h3">First, have a look at the command that we used to run a single Vegeta client. Interestingly, the command to put load on the backend servers is called <code class="markup--code markup--p-code">attack</code> :p</p><pre name="8713" id="8713" class="graf graf--pre graf-after--p">echo &quot;POST https://test.haproxy.in:443/ping&quot; | vegeta -cpus=32 attack -duration=10m  -header=&quot;sleep:30000&quot;  -body=post_smaller.txt -rate=2000 -workers=500  | tee reports.bin | vegeta report</pre><p name="0c02" id="0c02" class="graf graf--p graf-after--pre">Just love the parameters provided by Vegeta. Let’s have a look at some of these below.</p><ol class="postList"><li name="8a04" id="8a04" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">-cpus=32</code> Specifies the number of cores to be used by this client. We had to expand our client machines to 32core, 64Gig because of the amount of load to be generated. If you look closely above, the rate isn’t much. But it becomes difficult to sustain such a load when a lot of connections are in sleep state from the server end.</li><li name="fdda" id="fdda" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">-duration=10m</code> I guess this is self explanatory. If you don’t specify any duration, the test will run forever.</li><li name="5691" id="5691" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">-rate=2000</code> The number of requests per second.</li></ol><figure name="7ea2" id="7ea2" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 323px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 46.2%;"></div><img class="graf-image" data-image-id="1*8QYxI2VKQFtcsNk858eg0A.png" data-width="1542" data-height="712" src="https://cdn-images-1.medium.com/max/800/1*8QYxI2VKQFtcsNk858eg0A.png"></div></figure><p name="1660" id="1660" class="graf graf--p graf-after--figure">So as you can see above, we reached a hefty 32k requests per second on a mere 4 core machine. If you remember the tipping point graph, you will be able to notice it clearly enough above. So the tipping point in this case is 31.5k Non SSL requests.</p><p name="004d" id="004d" class="graf graf--p graf-after--p">Have a look at some more results from the load test.</p><figure name="31a8" id="31a8" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 271px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 38.7%;"></div><img class="graf-image" data-image-id="1*v3m7_R_c89id10wD8pxWrQ.png" data-width="1546" data-height="598" src="https://cdn-images-1.medium.com/max/800/1*v3m7_R_c89id10wD8pxWrQ.png"></div></figure><p name="54f1" id="54f1" class="graf graf--p graf-after--figure">16k SSL connections is also not bad at all. Please note that at this point in our load testing journey, we had to start from scratch because we had adopted a new client and it was giving us way better results than ab. So we had to do a lot of stuff again.</p><figure name="dd6b" id="dd6b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 237px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 33.900000000000006%;"></div><img class="graf-image" data-image-id="1*aDkvgQvP3TNcIuUfEYa_fQ.png" data-width="1540" data-height="522" src="https://cdn-images-1.medium.com/max/800/1*aDkvgQvP3TNcIuUfEYa_fQ.png"></div></figure><p name="94ca" id="94ca" class="graf graf--p graf-after--figure">An increase in the number of cores led to an increase in the number of requests per second that the machine can take before the CPU limit is hit.</p><p name="71f9" id="71f9" class="graf graf--p graf-after--p">We found that there wasn’t a substantial increase in the number of requests per second if we increased the number of cores from 8 to 16. Also, if we finally decided to go with a 8 core machine in production, we would never allocate all of the cores to HAProxy or be it a any other process for that matter. So we decided to perform some tests with 6 cores as well to see if we had acceptable numbers.</p><figure name="be7d" id="be7d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 171px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 24.4%;"></div><img class="graf-image" data-image-id="1*Ba_wms7xaxRi5EzOfPsvTQ.png" data-width="1542" data-height="376" src="https://cdn-images-1.medium.com/max/800/1*Ba_wms7xaxRi5EzOfPsvTQ.png"></div></figure><p name="6b6e" id="6b6e" class="graf graf--p graf-after--figure">Not bad.</p><h3 name="b9d0" id="b9d0" class="graf graf--h3 graf-after--p">Introducing the sleep</h3><p name="934a" id="934a" class="graf graf--p graf-after--h3">We were pretty satisfied with our load test results till now. However, this did not simulate the real production scenario. That happened when we introduced a sleep time as well which was absent till now in our tests.</p><pre name="a7d0" id="a7d0" class="graf graf--pre graf-after--p">echo &quot;POST https://test.haproxy.in:443/ping&quot; | vegeta -cpus=32 attack -duration=10m  -header=&quot;sleep:1000&quot;  -body=post_smaller.txt-rate=2000 -workers=500  | tee reports.bin | vegeta report</pre><p name="ead8" id="ead8" class="graf graf--p graf-after--pre">So a sleep time of 1000 milliseconds would lead to server sleeping for <code class="markup--code markup--p-code">x</code> amount of time where <code class="markup--code markup--p-code">0&lt; x &lt; 1000</code> and is selected randomly. So on an average the above load test will give a latency of ≥ 500ms</p><figure name="82c6" id="82c6" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 187px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.700000000000003%;"></div><img class="graf-image" data-image-id="1*BfBf2pUQDVc7D6dSWoxOmw.png" data-width="1550" data-height="414" src="https://cdn-images-1.medium.com/max/800/1*BfBf2pUQDVc7D6dSWoxOmw.png"></div></figure><p name="c09b" id="c09b" class="graf graf--p graf-after--figure">The numbers in the last cell represent</p><pre name="7bdd" id="7bdd" class="graf graf--pre graf-after--p">TCP established, Packets Rec, Packets Sent</pre><p name="8bd4" id="8bd4" class="graf graf--p graf-after--pre">respectively. As you can clearly see the max requests per second that the 6 core machine can support has decreased to 8k from 20k. Clearly, the sleep has its impact and that impact is the increase in the number of TCP connections established. This is however nowhere near to the 700k mark that we set out to achieve.</p><h3 name="2ed3" id="2ed3" class="graf graf--h3 graf-after--p">Milestone #1</h3><p name="d20a" id="d20a" class="graf graf--p graf-after--h3">How do we increase the number of TCP connections? Simple, we keep on increasing the sleep time and they should rise. We kept playing around with the sleep time and we stopped at the 60 seconds sleep time. That would mean an average latency of around 30 sec.</p><p name="7f00" id="7f00" class="graf graf--p graf-after--p">There is an interesting result parameter that Vegeta provides and that is % of requests successful. We saw that with the above sleep time, only 50% of the calls were succeeding. See the results below.</p><figure name="8b80" id="8b80" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 127px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 18.2%;"></div><img class="graf-image" data-image-id="1*-YAcVyKfw40Q84s3brTRdw.png" data-width="1542" data-height="280" src="https://cdn-images-1.medium.com/max/800/1*-YAcVyKfw40Q84s3brTRdw.png"></div></figure><p name="f721" id="f721" class="graf graf--p graf-after--figure">We achieved a whooping 400k TCP established connections with 8k requests per second and 60000 ms sleep time. The R in 60000R means Random.</p><p name="50cf" id="50cf" class="graf graf--p graf-after--p">The first real discovery we made was that there is a default call timeout in Vegeta which is of 30 seconds and that explained why 50% of our calls were failing. So we increased that to about 70s for our further tests and kept on varying it as and when the need arose.</p><figure name="fa37" id="fa37" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 377px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 53.900000000000006%;"></div><img class="graf-image" data-image-id="1*0RM6sCjY53Lr9gL-nMOWeQ.png" data-width="1544" data-height="832" src="https://cdn-images-1.medium.com/max/800/1*0RM6sCjY53Lr9gL-nMOWeQ.png"></div></figure><p name="d586" id="d586" class="graf graf--p graf-after--figure">We hit the 700k mark easily after tweaking the timeout value from the client end. The only problem with this was that these were not consistent. These were just peaks. So the system hit a peak of 600k or 700k but did not stay there for very long.</p><p name="6b6a" id="6b6a" class="graf graf--p graf-after--p">We however wanted something similar to this</p><figure name="42d0" id="42d0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 304px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 43.4%;"></div><img class="graf-image" data-image-id="1*RJqRVlM6uspwaFytYVTUtw.png" data-width="1152" data-height="500" src="https://cdn-images-1.medium.com/max/800/1*RJqRVlM6uspwaFytYVTUtw.png"></div></figure><p name="348d" id="348d" class="graf graf--p graf-after--figure">This shows a stable state where 780k connections are maintained. If you look closely at the stats above, the number of requests per second are very high. On production however, we have much less number of requests (somewhere around 300) on a single HAProxy machine.</p><p name="a1df" id="a1df" class="graf graf--p graf-after--p">We were sure that if we drastically reduced the number of HAProxies we have on production (somewhere around 30, which means 30*300 ~ 9k connects per second) we will hit the machine limits w.r.t. the number of TCP connections first and not the CPU.</p><blockquote name="d256" id="d256" class="graf graf--blockquote graf-after--p">So we decided to achieve 900 requests per second and 30MB/s Network and 2.1Million TCP established connections. We agreed upon these numbers as these would be 3 times our production load on a single HAProxy.</blockquote><p name="58c3" id="58c3" class="graf graf--p graf-after--blockquote">Plus, till now we had settled on 6 cores being used by HAProxy. We wanted to test out 3 cores only because this is what would be easiest for us to roll out on our production machines (Our production machines, as mentioned before are 4 core 30 Gig. So for rolling out changes with <code class="markup--code markup--p-code">nbproc = 3</code> would be easiest for us.</p><pre name="5856" id="5856" class="graf graf--pre graf-after--p">REMEMBER the machine we had at this point in time was 16 core 30 Gig machine with 3 cores being allocated to HAProxy.</pre><h3 name="79bc" id="79bc" class="graf graf--h3 graf-after--pre">Milestone #2</h3><p name="9eaa" id="9eaa" class="graf graf--p graf-after--h3">Now that we had max limits on requests per second that different variations in machine configuration could support, we only had one task left as mentioned above.</p><p name="207d" id="207d" class="graf graf--p graf-after--p">Achieve 3X the production load which is</p><ul class="postList"><li name="6352" id="6352" class="graf graf--li graf-after--p">900 requests per second</li><li name="808b" id="808b" class="graf graf--li graf-after--li">2.1 million TCP established and</li><li name="318d" id="318d" class="graf graf--li graf-after--li">30 MB/s network.</li></ul><p name="54a4" id="54a4" class="graf graf--p graf-after--li">We got stuck yet again as the TCP established were taking a hard hit at 220k. No matter what the number of client machines or what the sleep time was, number of TCP connections seemed to have stuck there.</p><p name="df14" id="df14" class="graf graf--p graf-after--p">Let’s look at some calculations. 220k TCP established connections and 900 requests per second = 110,000 / 900 ~= 120 seconds .I took 110k because 220k connections include both incoming and outgoing. So it’s two way.</p><p name="9189" id="9189" class="graf graf--p graf-after--p">Our doubt about 2 minutes being a limit somewhere in the system was verified when we introduced logs on the HAProxy side. We could see 120000 ms as total time for a lot of connections in the logs.</p><pre name="216b" id="216b" class="graf graf--pre graf-after--p">Mar 23 13:24:24 localhost haproxy[53750]: 172.168.0.232:48380 [23/Mar/2017:13:22:22.686] api~ api-backend/http31 39/0/2062/-1/122101 -1 0 - - SD-- 1714/1714/1678/35/0 0/0 {0,&quot;&quot;,&quot;&quot;} &quot;POST /ping HTTP/1.1&quot;</pre><pre name="764b" id="764b" class="graf graf--pre graf-after--pre">122101 is the timeout value. See HAProxy documentation on meanings of all these values. </pre><p name="0302" id="0302" class="graf graf--p graf-after--pre">On investigating further we found out that NodeJs has a default request timeout of 2 minutes. Voila !</p><figure name="1b21" id="1b21" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 912px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 130.29999999999998%;"></div><img class="graf-image" data-image-id="1*GnRdySeu4J-6UnkHGOiuuA.png" data-width="1228" data-height="1600" src="https://cdn-images-1.medium.com/max/800/1*GnRdySeu4J-6UnkHGOiuuA.png"></div></figure><div name="7f4a" id="7f4a" class="graf graf--mixtapeEmbed graf-after--figure"><a href="http://stackoverflow.com/questions/23925284/how-to-modify-the-nodejs-request-default-timeout-time" data-href="http://stackoverflow.com/questions/23925284/how-to-modify-the-nodejs-request-default-timeout-time" class="markup--anchor markup--mixtapeEmbed-anchor" title="http://stackoverflow.com/questions/23925284/how-to-modify-the-nodejs-request-default-timeout-time"><strong class="markup--strong markup--mixtapeEmbed-strong">how to modify the nodejs request default timeout time?</strong><br><em class="markup--em markup--mixtapeEmbed-em">I was using nodejs request, the default timeout of nodejs http is 120000 ms, but it is not enough for me, while my…</em>stackoverflow.com</a><a href="http://stackoverflow.com/questions/23925284/how-to-modify-the-nodejs-request-default-timeout-time" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="69a6ce6ceca32709e65c1298a96eba04" data-thumbnail-img-id="0*I9pljOqQJDl9Hfxv." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*I9pljOqQJDl9Hfxv.);"></a></div><div name="f347" id="f347" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://nodejs.org/api/http.html#http_server_settimeout_msecs_callback" data-href="https://nodejs.org/api/http.html#http_server_settimeout_msecs_callback" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://nodejs.org/api/http.html#http_server_settimeout_msecs_callback"><strong class="markup--strong markup--mixtapeEmbed-strong">HTTP | Node.js v7.8.0 Documentation</strong><br><em class="markup--em markup--mixtapeEmbed-em">The HTTP interfaces in Node.js are designed to support many features of the protocol which have been traditionally…</em>nodejs.org</a><a href="https://nodejs.org/api/http.html#http_server_settimeout_msecs_callback" class="js-mixtapeImage mixtapeImage mixtapeImage--empty u-ignoreBlock" data-media-id="569a99a63445102def730e4cde08785e"></a></div><p name="4321" id="4321" class="graf graf--p graf-after--mixtapeEmbed">But our happiness was apparently short lived. At 1.3 million, the HAProxy connections suddenly dropped to 0 and started increasing again. We soon checked the <a href="http://www.linfo.org/dmesg.html" data-href="http://www.linfo.org/dmesg.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">dmesg</a> command that provided us some useful kernel level information for our HAProxy process.</p><p name="8c5e" id="8c5e" class="graf graf--p graf-after--p">Basically, the HAProxy process had gone out of memory. So we decided to increase the machine RAM and we shifted to 16 core 64 Gig machine with <code class="markup--code markup--p-code">nbproc = 3</code> and because of this change we were able to reach 2.4 million connections.</p><h3 name="8d9e" id="8d9e" class="graf graf--h3 graf-after--p">Backend Code</h3><p name="c225" id="c225" class="graf graf--p graf-after--h3">Following is the backend server code that was being used. We had also used statsd in the server code to get consolidated data on requests per second that were being received by the client.</p><pre name="ae99" id="ae99" class="graf graf--pre graf-after--p">var http = require(&#39;http&#39;);<br>var createStatsd = require(&#39;uber-statsd-client&#39;);<br>qs = require(&#39;querystring&#39;);</pre><pre name="f6eb" id="f6eb" class="graf graf--pre graf-after--pre">var sdc = createStatsd({<br>host: &#39;172.168.0.134&#39;,<br>port: 8125<br>});</pre><pre name="9a27" id="9a27" class="graf graf--pre graf-after--pre">var argv = process.argv;<br>var port = argv[2];</pre><pre name="1159" id="1159" class="graf graf--pre graf-after--pre">function randomIntInc (low, high)<br>{<br>    return Math.floor(Math.random() * (high - low + 1) + low);<br>}</pre><pre name="3677" id="3677" class="graf graf--pre graf-after--pre">function sendResponse(res,times, old_sleep)<br>{<br>    res.write(&#39;pong&#39;);<br>    if(times==0)<br>    {<br>        res.end();<br>    }<br>    else<br>    { <br>        sleep = randomIntInc(0, old_sleep+1);<br>        setTimeout(sendResponse, sleep, res,times-1, old_sleep);<br>    }<br>}</pre><pre name="6125" id="6125" class="graf graf--pre graf-after--pre">var server = http.createServer(function(req, res)<br>{<br>   headers = req.headers;<br>   old_sleep = parseInt(headers[&quot;sleep&quot;]);<br>   times = headers[&quot;times&quot;] || 0;<br>   sleep = randomIntInc(0, old_sleep+1);<br>   console.log(sleep);<br>   sdc.increment(&quot;ssl.server.http&quot;);<br>   res.writeHead(200);<br>   setTimeout(sendResponse, sleep, res, times, old_sleep)</pre><pre name="8e8a" id="8e8a" class="graf graf--pre graf-after--pre">});</pre><pre name="f149" id="f149" class="graf graf--pre graf-after--pre">server.timeout = 3600000;<br>server.listen(port);</pre><p name="0181" id="0181" class="graf graf--p graf-after--pre">We also had a small script to run multiple backend servers. We had 8 machines with 10 backend servers EACH (yeah !). We literally took the idea of clients and backend servers being infinite for the load test, seriously.</p><pre name="500e" id="500e" class="graf graf--pre graf-after--p">counter=0<br>while [ $counter -le 9 ]<br>do<br>   port=$((8282+$counter))<br>   nodejs /opt/local/share/test-tools/HikeCLI/nodeclient/httpserver.js $port &amp;<br>   echo &quot;Server created on port &quot;  $port</pre><pre name="a17c" id="a17c" class="graf graf--pre graf-after--pre">   ((counter++))<br>done</pre><pre name="bbc4" id="bbc4" class="graf graf--pre graf-after--pre">echo &quot;Created all servers&quot;</pre><h3 name="3b40" id="3b40" class="graf graf--h3 graf-after--pre">Client Code</h3><p name="219b" id="219b" class="graf graf--p graf-after--h3">As for the client, there was a limitation of 63k TCP connections per IP. If you are not sure about this concept, please refer my <a href="https://medium.com/@sachinmalhotra/load-testing-haproxy-part-2-4c8677780df6" data-href="https://medium.com/@sachinmalhotra/load-testing-haproxy-part-2-4c8677780df6" class="markup--anchor markup--p-anchor" target="_blank">previous article</a> in this series.</p><p name="7eb5" id="7eb5" class="graf graf--p graf-after--p">So in order to achieve 2.4 million connections (two sided which is 1.2 million from the client machines), we needed somewhere around 20 machines. Its a pain really to run the Vegeta command on all 20 machines one by one and even of you found a way to do that using something like <a href="https://github.com/brockgr/csshx" data-href="https://github.com/brockgr/csshx" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">csshx</a>, you still would need something to combine all the results from all the Vegeta clients.</p><p name="7e51" id="7e51" class="graf graf--p graf-after--p">Check out the script below.</p><pre name="12fe" id="12fe" class="graf graf--pre graf-after--p">result_file=$1</pre><pre name="3004" id="3004" class="graf graf--pre graf-after--pre">declare -a machines=(&quot;172.168.0.138&quot; &quot;172.168.0.141&quot; &quot;172.168.0.142&quot; &quot;172.168.0.18&quot; &quot;172.168.0.5&quot; &quot;172.168.0.122&quot; &quot;172.168.0.123&quot; &quot;172.168.0.124&quot; &quot;172.168.0.232&quot; &quot; 172.168.0.244&quot; &quot;172.168.0.170&quot; &quot;172.168.0.179&quot; &quot;172.168.0.59&quot; &quot;172.168.0.68&quot; &quot;172.168.0.137&quot; &quot;172.168.0.155&quot; &quot;172.168.0.154&quot; &quot;172.168.0.45&quot; &quot;172.168.0.136&quot; &quot;172.168.0.143&quot;)</pre><pre name="aa85" id="aa85" class="graf graf--pre graf-after--pre">bins=&quot;&quot;<br>commas=&quot;&quot;</pre><pre name="e6ee" id="e6ee" class="graf graf--pre graf-after--pre">for i in &quot;${machines[@]}&quot;; do bins=$bins&quot;,&quot;$i&quot;.bin&quot;; commas=$commas&quot;,&quot;$i;  done;</pre><pre name="96e3" id="96e3" class="graf graf--pre graf-after--pre">bins=${bins:1}<br>commas=${commas:1}</pre><pre name="6e77" id="6e77" class="graf graf--pre graf-after--pre">pdsh -b -w &quot;$commas&quot; &#39;echo &quot;POST http://test.haproxy.in:80/ping&quot; | /home/sachinm/.linuxbrew/bin/vegeta -cpus=32 attack -connections=1000000 -header=&quot;sleep:20&quot; -header=&quot;times:2&quot; -body=post_smaller.txt -timeout=2h -rate=3000 -workers=500 &gt; &#39; $result_file</pre><pre name="ccf9" id="ccf9" class="graf graf--pre graf-after--pre">for i in &quot;${machines[@]}&quot;; do  scp sachinm@$i:/home/sachinm/$result_file $i.bin ; done;</pre><pre name="f5fb" id="f5fb" class="graf graf--pre graf-after--pre">vegeta report -inputs=&quot;$bins&quot;</pre><p name="6ece" id="6ece" class="graf graf--p graf-after--pre">Apparently, Vegeta provides information on this utility called <a href="https://github.com/grondo/pdsh" data-href="https://github.com/grondo/pdsh" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">pdsh</a> that lets you run a command concurrently on multiple machines remotely . Additionally, the Vegeta allows us to combine multiple results into one and that’s really all we wanted.</p><h3 name="5d37" id="5d37" class="graf graf--h3 graf-after--p">HAProxy Configuration</h3><p name="0f79" id="0f79" class="graf graf--p graf-after--h3">This is probably what you came here looking for, below is the HAProxy config that we used in our load test runs. The most important part being that of the <code class="markup--code markup--p-code">nbproc</code> setting and the <code class="markup--code markup--p-code">maxconn</code> setting. The maxconn setting allows us to provide the maximum number of TCP connections that the HAProxy can support overall (one way).</p><p name="7d0b" id="7d0b" class="graf graf--p graf-after--p">Changes to <code class="markup--code markup--p-code">maxconn</code> setting leads to increase in HAProxy process’ ulimit. Take a look below</p><figure name="ffc2" id="ffc2" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 304px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 43.5%;"></div><img class="graf-image" data-image-id="1*At2DHNGCMm9hUPbKRvtX4g.png" data-width="1568" data-height="682" src="https://cdn-images-1.medium.com/max/800/1*At2DHNGCMm9hUPbKRvtX4g.png"></div></figure><p name="44ca" id="44ca" class="graf graf--p graf-after--figure">The max open files has increased to 4 million because of the max connections for HAProxy being set at 2 million. Neat !</p><p name="24da" id="24da" class="graf graf--p graf-after--p">Check the article below for a whole lot of HAProxy optimisations that you can and should do to achieve the kind of stats we achieved.</p><div name="eba5" id="eba5" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://www.linangran.com/?p=547" data-href="https://www.linangran.com/?p=547" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://www.linangran.com/?p=547"><strong class="markup--strong markup--mixtapeEmbed-strong">Use HAProxy to load balance 300k concurrent tcp socket connections: Port Exhaustion, Keep-alive and…</strong><br><em class="markup--em markup--mixtapeEmbed-em">I&#39;m trying to build up a push system recently. To increase the scalability of the system, the best practice is to make…</em>www.linangran.com</a><a href="https://www.linangran.com/?p=547" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="8d00329a02e93224a861ca93e4c60a64" data-thumbnail-img-id="0*NoW97lRlDGmHnsPJ." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*NoW97lRlDGmHnsPJ.);"></a></div><figure name="55a9" id="55a9" class="graf graf--figure graf-after--mixtapeEmbed"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 687px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 98.1%;"></div><img class="graf-image" data-image-id="1*sd0hWfAkh-iwzo4AKrCSuA.png" data-width="1250" data-height="1226" src="https://cdn-images-1.medium.com/max/800/1*sd0hWfAkh-iwzo4AKrCSuA.png"></div></figure><figure name="90a5" id="90a5" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 737px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 105.3%;"></div><img class="graf-image" data-image-id="1*pGyKr5KepjpzKIy--QcDPg.png" data-width="1600" data-height="1684" src="https://cdn-images-1.medium.com/max/800/1*pGyKr5KepjpzKIy--QcDPg.png"></div></figure><p name="2fcc" id="2fcc" class="graf graf--p graf-after--figure">The http30 goes on to http83 :p</p><p name="f9f5" id="f9f5" class="graf graf--p graf-after--p">That’s all for now folks. If you’ve it so far, I’m truly amazed :)</p><p name="11f1" id="11f1" class="graf graf--p graf-after--p">A special shout out to <a href="https://medium.com/u/7a4e80f5af06" data-href="https://medium.com/u/7a4e80f5af06" data-anchor-type="2" data-user-id="7a4e80f5af06" data-action-value="7a4e80f5af06" data-action="show-user-card" data-action-type="hover" class="markup--user markup--p-user" target="_blank">Dheeraj Kumar Sidana</a> who helped us all the way through this and without whose help we would not have been able to reach any meaningful results. :)</p><p name="a75b" id="a75b" class="graf graf--p graf-after--p graf--trailing">Do let me know how this blog post helped you. Also, please recommend (❤) and spread the love as much as possible for this post if you think this might be useful for someone.</p></div></div></section>
</section>
</article></body></html>
</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2018/07/12/Recursion-Demystified-99a2105cb871/">
            Recursion Demystified
            <small>12 Jul 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2018/07/12/Explore-the-applications-and-limits-of-Breadth-First-Search-to-the-shortest-paths-in-a-weighted-1e7b28b3307/">
            Breadth First Search and its application to shortest paths in weighted graphs
            <small>12 Jul 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2018/06/08/A-deep-dive-into-part-of-speech-tagging-using-the-Viterbi-algorithm-17c8de32e8bc/">
            A deep dive into part-of-speech tagging using the Viterbi algorithm
            <small>08 Jun 2018</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
