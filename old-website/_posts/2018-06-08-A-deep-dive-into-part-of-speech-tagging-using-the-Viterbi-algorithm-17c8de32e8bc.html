---
layout: post
title: "A deep dive into part-of-speech tagging using the Viterbi algorithm"
comments: True
---

<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>A deep dive into part-of-speech tagging using the Viterbi algorithm</title><style>
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 100%;
        margin: auto;
      }
      section {
        width: 100%;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 100%;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">A deep dive into part-of-speech tagging using the Viterbi algorithm</h1>
</header>
<section data-field="subtitle" class="p-summary">
by Sachin Malhotra and Divya Godayal
</section>
<section data-field="body" class="e-content">
<section name="fd85" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="74a2" id="74a2" class="graf graf--h3 graf--leading graf--title"></h3><div name="cc82" id="cc82" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">by </em><a href="https://medium.com/@sachinmalhotra" data-href="https://medium.com/@sachinmalhotra" class="markup--anchor markup--p-anchor" target="_blank"><em class="markup--em markup--p-em">Sachin Malhotra</em></a><em class="markup--em markup--p-em"> and </em><a href="https://medium.com/@divyagodayal" data-href="https://medium.com/@divyagodayal" class="markup--anchor markup--p-anchor" target="_blank"><em class="markup--em markup--p-em">Divya Godayal</em></a></div></div><div class="section-inner sectionLayout--fullWidth"><figure name="6d36" id="6d36" class="graf graf--figure graf--layoutFillWidth graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 59.8%;"></div><img class="graf-image" data-image-id="1*x-5ZBtUvlD78BOMuMnMAbg.png" data-width="2894" data-height="1730" data-is-featured="true" src="https://cdn-images-1.medium.com/max/2000/1*x-5ZBtUvlD78BOMuMnMAbg.png"></div><figcaption class="imageCaption">Source: <a href="https://www.vocal.com/echo-cancellation/viterbi-algorithm-in-speech-enhancement-and-hmm/" data-href="https://www.vocal.com/echo-cancellation/viterbi-algorithm-in-speech-enhancement-and-hmm/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://www.vocal.com/echo-cancellation/viterbi-algorithm-in-speech-enhancement-and-hmm/</a></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="230d" id="230d" class="graf graf--p graf-after--figure">Welcome back, Caretaker!</p><p name="c33b" id="c33b" class="graf graf--p graf-after--p">In case you’ve forgotten the <a href="https://medium.freecodecamp.org/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24" data-href="https://medium.freecodecamp.org/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">problem</a> we were trying to tackle in the previous article, let us revise it for you.</p><p name="921f" id="921f" class="graf graf--p graf-after--p">So there’s this naughty kid Peter and he’s going to pester his new caretaker, you!</p><p name="c485" id="c485" class="graf graf--p graf-after--p">As a caretaker, one of the most important tasks for you is to tuck Peter in bed and make sure he is sound asleep. Once you’ve tucked him in, you want to make sure that he’s actually asleep and not up to some mischief.</p><p name="238d" id="238d" class="graf graf--p graf-after--p">You cannot, however, enter the room again, as that would surely wake Peter up. All you can hear are the noises that might come from the room.</p><p name="b336" id="b336" class="graf graf--p graf-after--p">Either the room is <strong class="markup--strong markup--p-strong">quiet</strong> or there is <strong class="markup--strong markup--p-strong">noise</strong> coming from the room. These are your states.</p><p name="2422" id="2422" class="graf graf--p graf-after--p">All you have as the caretaker are:</p><ul class="postList"><li name="d043" id="d043" class="graf graf--li graf-after--p">a set of observations, which is basically a sequence containing <strong class="markup--strong markup--li-strong">noise<em class="markup--em markup--li-em"> </em></strong>or <strong class="markup--strong markup--li-strong">quiet</strong> over time, and</li><li name="c16c" id="c16c" class="graf graf--li graf-after--li">A state diagram provided by Peter’s mom — who happens to be a neurological scientist — that contains all the different sets of probabilities that you can use to solve the problem defined below.</li></ul><h3 name="5e58" id="5e58" class="graf graf--h3 graf-after--li">The problem</h3><p name="da1b" id="da1b" class="graf graf--p graf-after--h3">Given the state diagram and a sequence of N observations over time, we need to tell the state of the baby at the current point in time. Mathematically, we have N observations over times <code class="markup--code markup--p-code">t0, t1, t2 .... tN</code> . We want to find out if Peter would be awake or asleep, or rather which state is more probable at time <code class="markup--code markup--p-code">tN+1</code> .</p><p name="0d5a" id="0d5a" class="graf graf--p graf-after--p">In case any of this seems like Greek to you, go read the <a href="https://medium.com/@divyagodayal/part-of-speech-tagging-hmms-part-1-953d45338f24" data-href="https://medium.com/@divyagodayal/part-of-speech-tagging-hmms-part-1-953d45338f24" class="markup--anchor markup--p-anchor" target="_blank">previous article</a> to brush up on the Markov Chain Model, Hidden Markov Models, and Part of Speech Tagging.</p><figure name="ea8f" id="ea8f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 331px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 47.3%;"></div><img class="graf-image" data-image-id="0*_i4JqwAZ9DdjO5Kt." data-width="748" data-height="354" src="https://cdn-images-1.medium.com/max/800/0*_i4JqwAZ9DdjO5Kt."></div><figcaption class="imageCaption">The state diagram that Peter’s mom gave you before leaving.</figcaption></figure><p name="ab7f" id="ab7f" class="graf graf--p graf-after--figure">In that <a href="https://medium.com/@divyagodayal/part-of-speech-tagging-hmms-part-1-953d45338f24" data-href="https://medium.com/@divyagodayal/part-of-speech-tagging-hmms-part-1-953d45338f24" class="markup--anchor markup--p-anchor" target="_blank">previous article</a>, we had briefly modeled the problem of Part of Speech tagging using the Hidden Markov Model.</p><p name="031c" id="031c" class="graf graf--p graf-after--p">The problem of Peter being asleep or not is just an example problem taken up for a better understanding of some of the core concepts involved in these two articles. At the core, the articles deal with solving the Part of Speech tagging problem using the Hidden Markov Models.</p><p name="1b34" id="1b34" class="graf graf--p graf-after--p">So, before moving on to the <strong class="markup--strong markup--p-strong">Viterbi Algorithm</strong>, let’s first look at a much more detailed explanation of how the tagging problem can be modeled using HMMs.</p><h3 name="19a7" id="19a7" class="graf graf--h3 graf-after--p">Generative Models and the Noisy Channel Model</h3><p name="55cb" id="55cb" class="graf graf--p graf-after--h3">A lot of problems in Natural Language Processing are solved using a supervised learning approach.</p><p name="0b53" id="0b53" class="graf graf--p graf-after--p">Supervised problems in machine learning are defined as follows. We assume training examples <code class="markup--code markup--p-code">(x(1), y(1))</code>. . .<code class="markup--code markup--p-code">(x(m) , y(m))</code>, where each example consists of an input x(i) paired with a label y(i) . We use X to refer to the set of possible inputs, and Y to refer to the set of possible labels. Our task is to learn a function f : X → Y that maps any input x to a label f(x).</p><p name="cb85" id="cb85" class="graf graf--p graf-after--p">In tagging problems, each x(i) would be a sequence of words <code class="markup--code markup--p-code">X1 X2 X3 …. Xn(i)</code>, and each y(i) would be a sequence of tags <code class="markup--code markup--p-code">Y1 Y2 Y3 … Yn(i)</code>(we use n(i)to refer to the length of the i’th training example). X would refer to the set of all sequences x1 . . . xn, and Y would be the set of all tag sequences y1 . . . yn. Our task would be to learn a function f : X → Y that maps sentences to tag sequences.</p><p name="19dd" id="19dd" class="graf graf--p graf-after--p">An intuitive approach to get an estimate for this problem is to use conditional probabilities. <code class="markup--code markup--p-code">p(y | x)</code> which is the probability of the output y given an input x. The parameters of the model would be estimated using the training samples. Finally, given an unknown input <code class="markup--code markup--p-code">x</code> we would like to find</p><p name="78b5" id="78b5" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">f(x) = arg max(p(y | x)) ∀y ∊ Y</code></p><p name="f448" id="f448" class="graf graf--p graf-after--p">This here is the conditional model to solve this generic problem given the training data. Another approach that is mostly adopted in machine learning and natural language processing is to use a <strong class="markup--strong markup--p-strong">generative model<em class="markup--em markup--p-em">.</em></strong></p><p name="b341" id="b341" class="graf graf--p graf-after--p">Rather than directly estimating the conditional distribution <code class="markup--code markup--p-code">p(y|x)</code>, in generative models we instead model the joint probability <code class="markup--code markup--p-code">p(x, y)</code> over all the (x, y) pairs.</p><p name="3565" id="3565" class="graf graf--p graf-after--p">We can further decompose the joint probability into simpler values using Bayes’ rule:</p><figure name="b560" id="b560" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 158px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 22.6%;"></div><img class="graf-image" data-image-id="1*vqr5JCpC4lkhva03gtTLIg.png" data-width="778" data-height="176" src="https://cdn-images-1.medium.com/max/800/1*vqr5JCpC4lkhva03gtTLIg.png"></div></figure><ul class="postList"><li name="c03e" id="c03e" class="graf graf--li graf-after--figure"><code class="markup--code markup--li-code">p(y)</code> is the prior probability of any input belonging to the label y.</li><li name="9b27" id="9b27" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">p(x | y)</code> is the conditional probability of input x given the label y.</li></ul><p name="7b91" id="7b91" class="graf graf--p graf-after--li">We can use this decomposition and the Bayes rule to determine the conditional probability.</p><figure name="c2ee" id="c2ee" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 205px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 29.299999999999997%;"></div><img class="graf-image" data-image-id="1*omTbvXw1CsMe6EywVZ9TUg.png" data-width="1002" data-height="294" src="https://cdn-images-1.medium.com/max/800/1*omTbvXw1CsMe6EywVZ9TUg.png"></div></figure><p name="4787" id="4787" class="graf graf--p graf-after--figure">Remember, we wanted to estimate the function</p><pre name="4fb3" id="4fb3" class="graf graf--pre graf-after--p">f(x) = arg max( p(y|x) ) <code class="markup--code markup--pre-code">∀y ∊ Y</code></pre><pre name="94cc" id="94cc" class="graf graf--pre graf-after--pre">f(x) = arg max( p(y) * p(x | y) )</pre><p name="0f0c" id="0f0c" class="graf graf--p graf-after--pre">The reason we skipped the denominator here is because the probability <code class="markup--code markup--p-code">p(x)</code> remains the same no matter what the output label being considered. And so, from a computational perspective, it is treated as a <strong class="markup--strong markup--p-strong">normalization constant and is normally ignored.</strong></p><p name="3731" id="3731" class="graf graf--p graf-after--p">Models that decompose a joint probability into terms <code class="markup--code markup--p-code">p(y)</code> and <code class="markup--code markup--p-code">p(x|y) </code>are often called <strong class="markup--strong markup--p-strong">noisy-channel models</strong>. Intuitively, when we see a test example x, we assume that it has been generated in two steps:</p><ol class="postList"><li name="3398" id="3398" class="graf graf--li graf-after--p">first, a label y has been chosen with probability p(y)</li><li name="9a04" id="9a04" class="graf graf--li graf-after--li">second, the example x has been generated from the distribution p(x|y). The model p(x|y) can be interpreted as a <strong class="markup--strong markup--li-strong">“channel” </strong>which takes a label y as its input, and corrupts it to produce x as its output.</li></ol><h3 name="56be" id="56be" class="graf graf--h3 graf-after--li">Generative Part of Speech Tagging Model</h3><p name="601d" id="601d" class="graf graf--p graf-after--h3">Let us assume a finite set of words V and a finite sequence of tags K. Then the set S will be the set of all sequence, tags pairs <code class="markup--code markup--p-code">&lt;x1, x2, x3 ... xn, y1, y2, y3, ..., yn&gt;</code> such that n &gt; 0 <code class="markup--code markup--p-code">∀x ∊ V</code> and <code class="markup--code markup--p-code">∀y ∊ K</code> .</p><p name="edd4" id="edd4" class="graf graf--p graf-after--p">A generative tagging model is then the one where</p><figure name="c5ef" id="c5ef" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 140px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 20%;"></div><img class="graf-image" data-image-id="1*uYnEH6xCNZN2tumL9gx10Q.png" data-width="1460" data-height="292" src="https://cdn-images-1.medium.com/max/800/1*uYnEH6xCNZN2tumL9gx10Q.png"></div></figure><p name="3f28" id="3f28" class="graf graf--p graf-after--figure">2.</p><figure name="a670" id="a670" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 181px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 25.900000000000002%;"></div><img class="graf-image" data-image-id="1*AO6n9RB0drLb9jbrHPIKDQ.png" data-width="1408" data-height="364" src="https://cdn-images-1.medium.com/max/800/1*AO6n9RB0drLb9jbrHPIKDQ.png"></div></figure><p name="56f5" id="56f5" class="graf graf--p graf-after--figure">Given a generative tagging model, the function that we talked about earlier from input to output becomes</p><figure name="c69e" id="c69e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 89px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 12.7%;"></div><img class="graf-image" data-image-id="1*vlACvd9qk5twIw6NORx10A.png" data-width="1554" data-height="198" src="https://cdn-images-1.medium.com/max/800/1*vlACvd9qk5twIw6NORx10A.png"></div></figure><p name="51e2" id="51e2" class="graf graf--p graf-after--figure">Thus for any given input sequence of words, the output is the highest probability tag sequence from the model. Having defined the generative model, we need to figure out three different things:</p><ol class="postList"><li name="42a7" id="42a7" class="graf graf--li graf-after--p">How exactly do we define the generative model probability <code class="markup--code markup--li-code">p(&lt;x1, x2, x3 ... xn, y1, y2, y3, ..., yn&gt;)</code></li><li name="81fe" id="81fe" class="graf graf--li graf-after--li">How do we estimate the parameters of the model, and</li><li name="8979" id="8979" class="graf graf--li graf-after--li">How do we efficiently calculate</li></ol><figure name="fc69" id="fc69" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 89px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 12.7%;"></div><img class="graf-image" data-image-id="1*vlACvd9qk5twIw6NORx10A.png" data-width="1554" data-height="198" src="https://cdn-images-1.medium.com/max/800/1*vlACvd9qk5twIw6NORx10A.png"></div></figure><p name="b92d" id="b92d" class="graf graf--p graf-after--figure">Let us look at how we can answer these three questions side by side, once for our example problem and then for the actual problem at hand: part of speech tagging.</p><h3 name="e5c2" id="e5c2" class="graf graf--h3 graf-after--p">Defining the Generative Model</h3><p name="8aa0" id="8aa0" class="graf graf--p graf-after--h3">Let us first look at how we can estimate the probability <code class="markup--code markup--p-code">p(x1 .. xn, y1 .. yn)</code> using the HMM.</p><p name="6395" id="6395" class="graf graf--p graf-after--p">We can have any N-gram HMM which considers events in the previous window of size N.</p><p name="4c6e" id="4c6e" class="graf graf--p graf-after--p">The formulas provided hereafter are corresponding to a <strong class="markup--strong markup--p-strong">Trigram </strong>Hidden Markov Model.</p><h4 name="5ebc" id="5ebc" class="graf graf--h4 graf-after--p">Trigram Hidden Markov Model</h4><p name="b6b6" id="b6b6" class="graf graf--p graf-after--h4">A trigram Hidden Markov Model can be defined using</p><ul class="postList"><li name="29ee" id="29ee" class="graf graf--li graf-after--p">A finite set of states.</li><li name="82bd" id="82bd" class="graf graf--li graf-after--li">A sequence of observations.</li><li name="370a" id="370a" class="graf graf--li graf-after--li">q(s|u, v)<br><strong class="markup--strong markup--li-strong">Transition probability</strong> defined as the probability of a state “s” appearing right after observing “u” and “v” in the sequence of observations.</li><li name="03e5" id="03e5" class="graf graf--li graf-after--li">e(x|s)<br><strong class="markup--strong markup--li-strong">Emission probability</strong> defined as the probability of making an observation x given that the state was s.</li></ul><p name="e9a6" id="e9a6" class="graf graf--p graf-after--li">Then, the generative model probability would be estimated as</p><figure name="5ef3" id="5ef3" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 98px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 14.000000000000002%;"></div><img class="graf-image" data-image-id="1*T_g39RuOYYway6_mFvp70Q.png" data-width="1998" data-height="280" src="https://cdn-images-1.medium.com/max/800/1*T_g39RuOYYway6_mFvp70Q.png"></div></figure><p name="0583" id="0583" class="graf graf--p graf-after--figure">As for the baby sleeping problem that we are considering, we will have only two possible states: that the baby is either awake or he is asleep. The caretaker can make only two observations over time. Either there is noise coming in from the room or the room is absolutely quiet. The sequence of observations and states can be represented as follows:</p><figure name="0730" id="0730" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 365px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 52.2%;"></div><img class="graf-image" data-image-id="1*BRQINXla9wqwjh8KyIpgOg.png" data-width="1576" data-height="822" src="https://cdn-images-1.medium.com/max/800/1*BRQINXla9wqwjh8KyIpgOg.png"></div><figcaption class="imageCaption">Observations and States over time for the baby sleeping problem</figcaption></figure><p name="4f70" id="4f70" class="graf graf--p graf-after--figure">Coming on to the part of speech tagging problem, the states would be represented by the actual tags assigned to the words. The words would be our observations. The reason we say that the tags are our states is because in a Hidden Markov Model, the states are always hidden and all we have are the set of observations that are visible to us. Along similar lines, the sequence of states and observations for the part of speech tagging problem would be</p><figure name="10ea" id="10ea" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 299px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 42.699999999999996%;"></div><img class="graf-image" data-image-id="1*GrNSK8Jtgh2_ABHYS9lbdA.png" data-width="1574" data-height="672" src="https://cdn-images-1.medium.com/max/800/1*GrNSK8Jtgh2_ABHYS9lbdA.png"></div><figcaption class="imageCaption">Observations and States over time for the POS tagging problem</figcaption></figure><h3 name="cad2" id="cad2" class="graf graf--h3 graf-after--figure">Estimating the model’s parameters</h3><p name="9759" id="9759" class="graf graf--p graf-after--h3">We will assume that we have access to some training data. The training data consists of a set of examples where each example is a sequence consisting of the observations, every observation being associated with a state. Given this data, how do we estimate the parameters of the model?</p><p name="2860" id="2860" class="graf graf--p graf-after--p">Estimating the model’s parameters is done by reading various counts off of the training corpus we have, and then computing maximum likelihood estimates:</p><figure name="69ce" id="69ce" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 306px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 43.7%;"></div><img class="graf-image" data-image-id="1*VgexfYbchSAZJ5fCwQqFOQ.png" data-width="1226" data-height="536" src="https://cdn-images-1.medium.com/max/800/1*VgexfYbchSAZJ5fCwQqFOQ.png"></div><figcaption class="imageCaption">Transition probability and Emission probability for a Trigram HMM</figcaption></figure><p name="26d1" id="26d1" class="graf graf--p graf-after--figure">We already know that the first term represents transition probability and the second term represents the emission probability. Let us look at what the four different counts mean in the terms above.</p><ol class="postList"><li name="0c53" id="0c53" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">c(u, v, s)</strong> represents the trigram count of states u, v and s. Meaning it represents the number of times the three states u, v and s occurred together in that order in the training corpus.</li><li name="c095" id="c095" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">c(u, v)</strong> following along similar lines as that of the trigram count, this is the bigram count of states u and v given the training corpus.</li><li name="9ed4" id="9ed4" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">c(s → x)</strong> is the number of times in the training set that the state s and observation x are paired with each other. And finally,</li><li name="12ef" id="12ef" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">c(s)</strong> is the prior probability of an observation being labelled as the state s.</li></ol><p name="7f73" id="7f73" class="graf graf--p graf-after--li">Let us look at a sample training set for the toy problem first and see the calculations for transition and emission probabilities using the same.</p><p name="c34b" id="c34b" class="graf graf--p graf-after--p">The BLUE markings represent the transition probability, and RED is for emission probability calculations.</p><p name="96e2" id="96e2" class="graf graf--p graf-after--p">Note that since the example problem only has two distinct states and two distinct observations, and given that the training set is very small, the calculations shown below for the example problem are using a <strong class="markup--strong markup--p-strong">bigram<em class="markup--em markup--p-em"> </em></strong>HMM<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em"> </em></strong>instead of a trigram HMM.</p><p name="a836" id="a836" class="graf graf--p graf-after--p">Peter’s mother was maintaining a record of observations and states. And thus she even provided you with a training corpus to help you get the transition and emission probabilities.</p><h4 name="361a" id="361a" class="graf graf--h4 graf-after--p">Transition Probability Example:</h4><figure name="94f5" id="94f5" class="graf graf--figure graf-after--h4"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 60px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 8.6%;"></div><img class="graf-image" data-image-id="1*1KKb9HiSws-068YBfWjhgw.png" data-width="1518" data-height="130" src="https://cdn-images-1.medium.com/max/800/1*1KKb9HiSws-068YBfWjhgw.png"></div><figcaption class="imageCaption">Training Corpus</figcaption></figure><figure name="3dc7" id="3dc7" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 336px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 48%;"></div><img class="graf-image" data-image-id="1*VemATVV5YgmBZZZGxjzEQA.png" data-width="1216" data-height="584" src="https://cdn-images-1.medium.com/max/800/1*VemATVV5YgmBZZZGxjzEQA.png"></div><figcaption class="imageCaption">Calculations for Awake appearing after Awake</figcaption></figure><h4 name="dafd" id="dafd" class="graf graf--h4 graf-after--figure">Emission Probability Example:</h4><figure name="6834" id="6834" class="graf graf--figure graf-after--h4"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 60px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 8.6%;"></div><img class="graf-image" data-image-id="1*aCPlU-ApNX0umWin7U3PBQ.png" data-width="1518" data-height="130" src="https://cdn-images-1.medium.com/max/800/1*aCPlU-ApNX0umWin7U3PBQ.png"></div><figcaption class="imageCaption">Training corpus</figcaption></figure><figure name="2f00" id="2f00" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 298px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 42.6%;"></div><img class="graf-image" data-image-id="1*0umDSsoG676H66LkyHiM_Q.png" data-width="1602" data-height="682" src="https://cdn-images-1.medium.com/max/800/1*0umDSsoG676H66LkyHiM_Q.png"></div><figcaption class="imageCaption">Calculations for observing ‘Quiet’ when the state is ‘Awake’</figcaption></figure><p name="264a" id="264a" class="graf graf--p graf-after--figure">That was quite simple, since the training set was very small. Let us look at a sample training set for our actual problem of part of speech tagging. Here we can consider a trigram HMM, and we will show the calculations accordingly.</p><p name="8b49" id="8b49" class="graf graf--p graf-after--p">We will use the following sentences as a corpus of training data (the notation word/TAG means word tagged with a specific part-of-speech tag).</p><figure name="d335" id="d335" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 479px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 68.4%;"></div><img class="graf-image" data-image-id="1*E95LGOKw2YVumuZqk_uAJw.png" data-width="1296" data-height="886" src="https://cdn-images-1.medium.com/max/800/1*E95LGOKw2YVumuZqk_uAJw.png"></div></figure><p name="c1f9" id="c1f9" class="graf graf--p graf-after--figure">The training set that we have is a tagged corpus of sentences. Every sentence consists of words tagged with their corresponding part of speech tags. eg:- eat/VB means that the word is “eat” and the part of speech tag in this sentence in this very context is “VB” i.e. Verb Phrase. Let us look at a sample calculation for transition probability and emission probability just like we saw for the baby sleeping problem.</p><h4 name="a01c" id="a01c" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Transition Probability</strong></h4><p name="18ea" id="18ea" class="graf graf--p graf-after--h4">Let’s say we want to calculate the transition probability q(IN | VB, NN). For this, we see how many times we see a trigram (VB,NN,IN) in the training corpus in that specific order. We then divide it by the total number of times we see the bigram (VB,NN) in the corpus.</p><h4 name="12bc" id="12bc" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Emission Probability</strong></h4><p name="1dcb" id="1dcb" class="graf graf--p graf-after--h4">Let’s say we want to find out the emission probability e(an | DT). For this, we see how many times the word “an” is tagged as “DT” in the corpus and divide it by the total number of times we see the tag “DT” in the corpus.</p><figure name="9096" id="9096" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 494px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 70.6%;"></div><img class="graf-image" data-image-id="1*QLaaoZ1WPdWgWsPb7CFoSA.png" data-width="1422" data-height="1004" src="https://cdn-images-1.medium.com/max/800/1*QLaaoZ1WPdWgWsPb7CFoSA.png"></div></figure><p name="d9a6" id="d9a6" class="graf graf--p graf-after--figure">So if you look at these calculations, it shows that calculating the model’s parameters is not computationally expensive. That is, we don’t have to do multiple passes over the training data to calculate these parameters. All we need are a bunch of different counts, and a single pass over the training corpus should provide us with that.</p><p name="c159" id="c159" class="graf graf--p graf-after--p">Let’s move on and look at the final step that we need to look at given a generative model. That step is efficiently calculating</p><figure name="1385" id="1385" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 89px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 12.7%;"></div><img class="graf-image" data-image-id="1*vlACvd9qk5twIw6NORx10A.png" data-width="1554" data-height="198" src="https://cdn-images-1.medium.com/max/800/1*vlACvd9qk5twIw6NORx10A.png"></div></figure><p name="cbbc" id="cbbc" class="graf graf--p graf-after--figure">We will be looking at the famous Viterbi Algorithm for this calculation.</p><h3 name="fc2a" id="fc2a" class="graf graf--h3 graf-after--p">Finding the most probable sequence — Viterbi Algorithm</h3><p name="e7a7" id="e7a7" class="graf graf--p graf-after--h3">Finally, we are going to solve the problem of finding the most likely sequence of labels given a set of observations x1 … xn. That is, we are to find out</p><figure name="3068" id="3068" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 110px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 15.7%;"></div><img class="graf-image" data-image-id="1*YjdFBbrDymwoCdf_wgfOFA.png" data-width="700" data-height="110" src="https://cdn-images-1.medium.com/max/800/1*YjdFBbrDymwoCdf_wgfOFA.png"></div></figure><p name="9705" id="9705" class="graf graf--p graf-after--figure">The probability here is expressed in terms of the transition and emission probabilities that we learned how to calculate in the previous section of the article. Just to remind you, the formula for the probability of a sequence of labels given a sequence of observations over “n” time steps is</p><figure name="9636" id="9636" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 93px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 13.3%;"></div><img class="graf-image" data-image-id="1*k3LxXJs2vmw5YT1dGlvMLw.png" data-width="1160" data-height="154" src="https://cdn-images-1.medium.com/max/800/1*k3LxXJs2vmw5YT1dGlvMLw.png"></div></figure><p name="65da" id="65da" class="graf graf--p graf-after--figure">Before looking at an optimized algorithm to solve this problem, let us first look at a simple brute force approach to this problem. Basically, we need to find out the most probable label sequence given a set of observations out of a finite set of possible sequences of labels. Let’s look at the total possible number of sequences for a small example for our example problem and also for a part of speech tagging problem.</p><p name="1480" id="1480" class="graf graf--p graf-after--p">Say we have the following set of observations for the example problem.</p><pre name="ce00" id="ce00" class="graf graf--pre graf-after--p">Noise     Quiet     Noise</pre><p name="9608" id="9608" class="graf graf--p graf-after--pre">We have two possible labels {Asleep and Awake}. Some of the possible sequence of labels for the observations above are:</p><pre name="1866" id="1866" class="graf graf--pre graf-after--p">Awake      Awake     Awake</pre><pre name="f50f" id="f50f" class="graf graf--pre graf-after--pre">Awake      Awake     Asleep</pre><pre name="32c4" id="32c4" class="graf graf--pre graf-after--pre">Awake      Asleep    Awake</pre><pre name="580f" id="580f" class="graf graf--pre graf-after--pre">Awake      Asleep    Asleep</pre><p name="bacd" id="bacd" class="graf graf--p graf-after--pre">In all we can have ²³ = 8 possible sequences. This might not seem like very many, but if we increase the number of observations over time, the number of sequences would increase exponentially. This is the case when we only had two possible labels. What if we have more? As is the case with part of speech tagging.</p><p name="a3b5" id="a3b5" class="graf graf--p graf-after--p">For example, consider the sentence</p><pre name="bf95" id="bf95" class="graf graf--pre graf-after--p">the dog barks</pre><p name="a6c4" id="a6c4" class="graf graf--p graf-after--pre">and assuming that the set of possible tags are {D, N, V}, let us look at some of the possible tag sequences:</p><pre name="f977" id="f977" class="graf graf--pre graf-after--p">D     D     D<br>D     D     N<br>D     D     V<br>D     N     D<br>D     N     N<br>D     N     V ... etc</pre><p name="c800" id="c800" class="graf graf--p graf-after--pre">Here, we would have ³³ = 27 possible tag sequences. And as you can see, the sentence was extremely short and the number of tags weren’t very many. In practice, we can have sentences that might be much larger than just three words. Then the number of unique labels at our disposal would also be too high to follow this enumeration approach and find the best possible tag sequence this way.</p><p name="d340" id="d340" class="graf graf--p graf-after--p">So the exponential growth in the number of sequences implies that for any reasonable length sentence, the brute force approach would not work out as it would take too much time to execute.</p><p name="08d9" id="08d9" class="graf graf--p graf-after--p">Instead of this brute force approach, we will see that we can find the highest probable tag sequence efficiently using a dynamic programming algorithm known as the <strong class="markup--strong markup--p-strong">Viterbi Algorithm.</strong></p><p name="c823" id="c823" class="graf graf--p graf-after--p">Let us first define some terms that would be useful in defining the algorithm itself. We already know that the probability of a label sequence given a set of observations can be defined in terms of the transition probability and the emission probability. Mathematically, it is</p><figure name="8a48" id="8a48" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 91px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 13%;"></div><img class="graf-image" data-image-id="1*JIqZ2wCdNB4FeTgd1Uz6yw.png" data-width="1142" data-height="148" src="https://cdn-images-1.medium.com/max/800/1*JIqZ2wCdNB4FeTgd1Uz6yw.png"></div></figure><p name="5327" id="5327" class="graf graf--p graf-after--figure">Let us look at a truncated version of this which is</p><figure name="3864" id="3864" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 113px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.2%;"></div><img class="graf-image" data-image-id="1*i8qSInkAqmK9deXrLTKw8g.png" data-width="928" data-height="150" src="https://cdn-images-1.medium.com/max/800/1*i8qSInkAqmK9deXrLTKw8g.png"></div></figure><p name="89ac" id="89ac" class="graf graf--p graf-after--figure">and let us call this the cost of a sequence of length k.</p><p name="3787" id="3787" class="graf graf--p graf-after--p">So the definition of “r” is simply considering the first k terms off of the definition of probability where k ∊ {1..n} and for any label sequence y1…yk.</p><p name="8af1" id="8af1" class="graf graf--p graf-after--p">Next we have the set S(k, u, v) which is basically the set of all label sequences of length k that end with the bigram (u, v) i.e.</p><figure name="e4c9" id="e4c9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 43px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 6.1%;"></div><img class="graf-image" data-image-id="1*VnH7pFyjnoBLPMc34RWD7Q.png" data-width="1010" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*VnH7pFyjnoBLPMc34RWD7Q.png"></div></figure><p name="b841" id="b841" class="graf graf--p graf-after--figure">Finally, we define the term π(k, u, v) which is basically the sequence with the maximum cost.</p><figure name="68a0" id="68a0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 100px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 14.299999999999999%;"></div><img class="graf-image" data-image-id="1*Mq4QjRUI5lVkVBdVjzDcGQ.png" data-width="810" data-height="116" src="https://cdn-images-1.medium.com/max/800/1*Mq4QjRUI5lVkVBdVjzDcGQ.png"></div></figure><p name="412c" id="412c" class="graf graf--p graf-after--figure">The main idea behind the Viterbi Algorithm is that we can calculate the values of the term π(k, u, v) efficiently in a recursive, memoized fashion. In order to define the algorithm recursively, let us look at the base cases for the recursion.</p><pre name="2a54" id="2a54" class="graf graf--pre graf-after--p">π(0, *, *) = 1</pre><pre name="0f7d" id="0f7d" class="graf graf--pre graf-after--pre">π(0, u, v) = 0</pre><p name="5145" id="5145" class="graf graf--p graf-after--pre">Since we are considering a trigram HMM, we would be considering all of the trigrams as a part of the execution of the Viterbi Algorithm.</p><p name="309c" id="309c" class="graf graf--p graf-after--p">Now, we can start the first trigram window from the first three words of the sentence but then the model would miss out on those trigrams where the first word or the first two words occurred independently. For that reason, we consider two special start symbols as <code class="markup--code markup--p-code">*</code> and so our sentence becomes</p><pre name="3d6a" id="3d6a" class="graf graf--pre graf-after--p">*    *    x1   x2   x3   ......         xn</pre><p name="f009" id="f009" class="graf graf--p graf-after--pre">And the first trigram we consider then would be (*, *, x1) and the second one would be (*, x1, x2).</p><p name="d26c" id="d26c" class="graf graf--p graf-after--p">Now that we have all our terms in place, we can finally look at the recursive definition of the algorithm which is basically the heart of the algorithm.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="81ec" id="81ec" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 514px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 51.4%;"></div><img class="graf-image" data-image-id="1*OcNkv1g-V0s_YZR76D79sw.png" data-width="2224" data-height="1144" src="https://cdn-images-1.medium.com/max/1000/1*OcNkv1g-V0s_YZR76D79sw.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="460f" id="460f" class="graf graf--p graf-after--figure">This definition is clearly recursive, because we are trying to calculate one π term and we are using another one with a lower value of k in the recurrence relation for it.</p><figure name="f4bd" id="f4bd" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 55px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 7.8%;"></div><img class="graf-image" data-image-id="1*_AbNeEabGTVYBq4VbnbAQg.png" data-width="1766" data-height="138" src="https://cdn-images-1.medium.com/max/800/1*_AbNeEabGTVYBq4VbnbAQg.png"></div></figure><p name="5999" id="5999" class="graf graf--p graf-after--figure">Every sequence would end with a special STOP symbol. For the trigram model, we would also have two special start symbols “*” in the beginning.</p><p name="edf6" id="edf6" class="graf graf--p graf-after--p">Have a look at the pseudo-code for the entire algorithm.</p><figure name="2f93" id="2f93" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 330px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 47.199999999999996%;"></div><img class="graf-image" data-image-id="1*Vex29pOqTVPdtc7Wdz9tlw.png" data-width="1852" data-height="874" src="https://cdn-images-1.medium.com/max/800/1*Vex29pOqTVPdtc7Wdz9tlw.png"></div></figure><p name="d500" id="d500" class="graf graf--p graf-after--figure">The algorithm first fills in the π(k, u, v) values in using the recursive<br>definition. It then uses the identity described before to calculate the highest probability for any sequence.</p><p name="3362" id="3362" class="graf graf--p graf-after--p">The running time for the algorithm is O(n|K|³), hence it is linear in the length of the sequence, and cubic in the number of tags.</p><p name="91d7" id="91d7" class="graf graf--p graf-after--p">NOTE: We would be showing calculations for the baby sleeping problem and the part of speech tagging problem based off a <strong class="markup--strong markup--p-strong">bigram HMM only. </strong>The calculations for the trigram are left to the reader to do themselves. But the code that is attached at the end of this article is based on a trigram HMM. It’s just that the calculations are easier to explain and portray for the Viterbi algorithm when considering a bigram HMM instead of a trigram HMM.</p><p name="d00f" id="d00f" class="graf graf--p graf-after--p">Therefore, before showing the calculations for the Viterbi Algorithm, let us look at the recursive formula based on a bigram HMM.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="636c" id="636c" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 721px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 72.1%;"></div><img class="graf-image" data-image-id="1*XmRwVbO0yHq8bsXO7jgCGA.png" data-width="1996" data-height="1440" src="https://cdn-images-1.medium.com/max/1000/1*XmRwVbO0yHq8bsXO7jgCGA.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="21bd" id="21bd" class="graf graf--p graf-after--figure">This one is extremely similar to the one we saw before for the trigram model, except that now we are only concerning ourselves with the current label and the one before, instead of two before. The complexity of the algorithm now becomes O(n|K|²).</p><h4 name="aa4f" id="aa4f" class="graf graf--h4 graf-after--p">Calculations for Baby Sleeping Problem</h4><p name="5604" id="5604" class="graf graf--p graf-after--h4">Now that we have the recursive formula ready for the Viterbi Algorithm, let us see a sample calculation of the same firstly for the example problem that we had, that is, the baby sleeping problem, and then for the part of speech tagging version.</p><p name="e3ba" id="e3ba" class="graf graf--p graf-after--p">Note that when we are at this step, that is, the calculations for the Viterbi Algorithm to find the most likely tag sequence given a set of observations over a series of time steps, we assume that transition and emission probabilities have already been calculated from the given corpus. Let’s have a look at a sample of transition and emission probabilities for the baby sleeping problem that we would use for our calculations of the algorithm.</p><figure name="ab6a" id="ab6a" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 314px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 44.800000000000004%;"></div><img class="graf-image" data-image-id="1*I8XENfbYguOXxWgeHwFKJw.png" data-width="1076" data-height="482" src="https://cdn-images-1.medium.com/max/800/1*I8XENfbYguOXxWgeHwFKJw.png"></div></figure><p name="a8dc" id="a8dc" class="graf graf--p graf-after--figure">The baby starts by being awake, and remains in the room for three time points, t1 . . . t3 (three iterations of the Markov chain). The observations are: quiet, quiet, noise. Have a look at the following diagram that shows the calculations for up to two time-steps. The complete diagram with all the final set of values will be shown afterwards.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="76f1" id="76f1" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 827px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 82.69999999999999%;"></div><img class="graf-image" data-image-id="1*65f7qx2216umw5c4J-EByA.png" data-width="1588" data-height="1314" src="https://cdn-images-1.medium.com/max/1000/1*65f7qx2216umw5c4J-EByA.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="d0d1" id="d0d1" class="graf graf--p graf-after--figure">We have not shown the calculations for the state of “asleep” at k = 2 and the calculations for k = 3 in the above diagram to keep things simple.</p><p name="1d62" id="1d62" class="graf graf--p graf-after--p">Now that we have all these calculations in place, we want to calculate the most likely sequence of states that the baby can be in over the different given time steps. So, for k = 2 and the state of Awake, we want to know the most likely state at k = 1 that transitioned to Awake at k = 2. (k = 2 represents a sequence of states of length 3 starting off from 0 and t = 2 would mean the state at time-step 2. We are given the state at t = 0 i.e. Awake).</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="643a" id="643a" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 782px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 78.2%;"></div><img class="graf-image" data-image-id="1*Qs1Y0xm5aQDKWTO_783qCA.png" data-width="1780" data-height="1392" src="https://cdn-images-1.medium.com/max/1000/1*Qs1Y0xm5aQDKWTO_783qCA.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="0706" id="0706" class="graf graf--p graf-after--figure">Clearly, if the state at time-step 2 was AWAKE, then the state at time-step 1 would have been AWAKE as well, as the calculations point out. So, the Viterbi Algorithm not only helps us find the π(k) values, that is the cost values for all the sequences using the concept of dynamic programming, but it also helps us to find the most likely tag sequence given a start state and a sequence of observations. The algorithm, along with the pseudo-code for storing the <strong class="markup--strong markup--p-strong">back-pointers<em class="markup--em markup--p-em"> </em></strong>is given below.</p><figure name="15a0" id="15a0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 523px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 74.7%;"></div><img class="graf-image" data-image-id="1*FJkk8r9KxAZsOZXzFznnzQ.png" data-width="1082" data-height="808" src="https://cdn-images-1.medium.com/max/800/1*FJkk8r9KxAZsOZXzFznnzQ.png"></div></figure><h4 name="dda8" id="dda8" class="graf graf--h4 graf-after--figure">Calculations for the Part of Speech Tagging Problem</h4><p name="2052" id="2052" class="graf graf--p graf-after--h4">Let us look at a slightly bigger corpus for the part of speech tagging and the corresponding Viterbi graph showing the calculations and back-pointers for the Viterbi Algorithm.</p><p name="e355" id="e355" class="graf graf--p graf-after--p">Here is the corpus that we will consider:</p><figure name="b75b" id="b75b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 412px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 58.8%;"></div><img class="graf-image" data-image-id="1*Kk1RzSbVGDW4aJ8zmiSliA.png" data-width="938" data-height="552" src="https://cdn-images-1.medium.com/max/800/1*Kk1RzSbVGDW4aJ8zmiSliA.png"></div></figure><p name="f10a" id="f10a" class="graf graf--p graf-after--figure">Now take a look at the transition probabilities calculated from this corpus.</p><figure name="2bac" id="2bac" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 682px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 97.39999999999999%;"></div><img class="graf-image" data-image-id="1*PIMy_sW7sMz7RXgewMgrkA.png" data-width="1366" data-height="1330" src="https://cdn-images-1.medium.com/max/800/1*PIMy_sW7sMz7RXgewMgrkA.png"></div></figure><p name="a08f" id="a08f" class="graf graf--p graf-after--figure">Here, q0 → VB represents the probability of a sentence starting off with the tag VB, that is the first word of a sentence being tagged as VB. Similarly, q0 → NN represents the probability of a sentence starting with the tag NN. Notice that out of 10 sentences in the corpus, 8 start with NN and 2 with VB and hence the corresponding transition probabilities.</p><p name="d906" id="d906" class="graf graf--p graf-after--p">As for the emission probabilities, ideally we should be looking at all the combinations of tags and words in the corpus. Since that would be too much, we will only consider emission probabilities for the sentence that would be used in the calculations for the Viterbi Algorithm.</p><pre name="c38f" id="c38f" class="graf graf--pre graf-after--p">Time flies like an arrow</pre><p name="59ea" id="59ea" class="graf graf--p graf-after--pre">The emission probabilities for the sentence above are:</p><figure name="59ae" id="59ae" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 184px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.3%;"></div><img class="graf-image" data-image-id="1*QeTkU4PaCo4yon8LfBbtDg.png" data-width="1590" data-height="418" src="https://cdn-images-1.medium.com/max/800/1*QeTkU4PaCo4yon8LfBbtDg.png"></div></figure><p name="083e" id="083e" class="graf graf--p graf-after--figure">Finally, we are ready to see the calculations for the given sentence, transition probabilities, emission probabilities, and the given corpus.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="9646" id="9646" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 501px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 50.1%;"></div><img class="graf-image" data-image-id="1*rpRFFEmVLE1PBUc4OwpWgg.png" data-width="1990" data-height="996" src="https://cdn-images-1.medium.com/max/1000/1*rpRFFEmVLE1PBUc4OwpWgg.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="c32d" id="c32d" class="graf graf--p graf-after--figure">So, is that all there is to the Viterbi Algorithm ?</p><p name="6c6a" id="6c6a" class="graf graf--p graf-after--p">Take a look at the example below.</p><p name="1738" id="1738" class="graf graf--p graf-after--p">The bucket below each word is filled with the possible tags seen next to the word in the training corpus. The given sentence can have the combinations of tags depending on which path we take. But there is a catch. Can you figure out what that is?</p><figure name="f550" id="f550" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 287px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 41%;"></div><img class="graf-image" data-image-id="1*7DVGrmTkEW7wS2DxrS0B5g.png" data-width="2590" data-height="1062" src="https://cdn-images-1.medium.com/max/800/1*7DVGrmTkEW7wS2DxrS0B5g.png"></div><figcaption class="imageCaption">All combinations of sequence paths</figcaption></figure><p name="6573" id="6573" class="graf graf--p graf-after--figure">Were you able to figure it out?</p><p name="7fb6" id="7fb6" class="graf graf--p graf-after--p">No??</p><p name="e511" id="e511" class="graf graf--p graf-after--p">Let me tell you what it is.</p><p name="edee" id="edee" class="graf graf--p graf-after--p">There might be some path in the computation graph for which we do not have a transition probability. So our algorithm can just discard that path and take the other path.</p><figure name="eca8" id="eca8" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 266px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 38%;"></div><img class="graf-image" data-image-id="1*lXoyPV31HXNROE8wrvk_PQ.png" data-width="1562" data-height="594" src="https://cdn-images-1.medium.com/max/800/1*lXoyPV31HXNROE8wrvk_PQ.png"></div></figure><p name="aa68" id="aa68" class="graf graf--p graf-after--figure">In the above diagram, we discard the path marked in red since we do not have q(VB|VB). The training corpus never has a <strong class="markup--strong markup--p-strong">VB</strong> followed by <strong class="markup--strong markup--p-strong">VB</strong>. So in the Viterbi calculations, we end up taking q(VB|VB) = 0. And if you’ve been following the algorithm along closely, you would find that a single 0 in the calculations would make the entire probability or the maximum cost for a sequence of tags / labels to be 0.</p><p name="4874" id="4874" class="graf graf--p graf-after--p">This however means that we are ignoring the combinations which are not seen in the training corpus.</p><p name="cb69" id="cb69" class="graf graf--p graf-after--p">Is that the right way to approach the real world examples?</p><p name="8680" id="8680" class="graf graf--p graf-after--p">Consider a small tweak in the above sentence.</p><figure name="d621" id="d621" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 292px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 41.8%;"></div><img class="graf-image" data-image-id="1*_WJT4hUD6VIbDwwaeC0bYA.png" data-width="2620" data-height="1094" src="https://cdn-images-1.medium.com/max/800/1*_WJT4hUD6VIbDwwaeC0bYA.png"></div><figcaption class="imageCaption">Time flies like <strong class="markup--strong markup--figure-strong">take</strong> arrow</figcaption></figure><p name="066c" id="066c" class="graf graf--p graf-after--figure">In this sentence we do not have any alternative path. Even if we have Viterbi probability until we reach the word “like”, we cannot proceed further. Since both q(VB|VB) = 0 and q(VB|IN) = 0. What do we do now?</p><p name="232d" id="232d" class="graf graf--p graf-after--p">The corpus that we considered here was very small. Consider any reasonably sized corpus with a lot of words and we have a major problem of sparsity of data. Take a look below.</p><figure name="fc2b" id="fc2b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 593px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 84.7%;"></div><img class="graf-image" data-image-id="1*TbMxaow_H3ZUbkHuSorhqQ.png" data-width="1084" data-height="918" src="https://cdn-images-1.medium.com/max/800/1*TbMxaow_H3ZUbkHuSorhqQ.png"></div><figcaption class="imageCaption">Source: <a href="http://www.cs.pomona.edu/~kim/CSC181S08/lectures/Lec6/Lec6.pdf" data-href="http://www.cs.pomona.edu/~kim/CSC181S08/lectures/Lec6/Lec6.pdf" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener" target="_blank">http://www.cs.pomona.edu/~kim/CSC181S08/lectures/Lec6/Lec6.pdf</a></figcaption></figure><p name="b900" id="b900" class="graf graf--p graf-after--figure">That means that we can have a potential 68 billion bigrams but the number of words in the corpus are just under a billion. That is a huge number of zero transition probabilities to fill up. The problem of sparsity of data is even more elaborate in case we are considering trigrams.</p><p name="ce9e" id="ce9e" class="graf graf--p graf-after--p">To solve this problem of data sparsity, we resort to a solution called Smoothing.</p><h3 name="383d" id="383d" class="graf graf--h3 graf-after--p">Smoothing</h3><p name="a514" id="a514" class="graf graf--p graf-after--h3">The idea behind Smoothing is just this:</p><ol class="postList"><li name="b3a4" id="b3a4" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Discount </strong>— the existing probability values somewhat and</li><li name="6ea4" id="6ea4" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Reallocate </strong>— this probability to the zeroes</li></ol><p name="91a8" id="91a8" class="graf graf--p graf-after--li">In this way, we redistribute the non zero probability values to compensate for the unseen transition combinations. Let us consider a very simple type of smoothing technique known as Laplace Smoothing.</p><p name="4fa7" id="4fa7" class="graf graf--p graf-after--p">Laplace smoothing is also known as one count smoothing. You will understand exactly why it goes by that name in a moment. Let’s revise how the parameters for a trigram HMM model are calculated given a training corpus.</p><figure name="38b4" id="38b4" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 473px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 67.60000000000001%;"></div><img class="graf-image" data-image-id="1*xQpIu7eeclDRbdxM-mcAiA.png" data-width="1352" data-height="914" src="https://cdn-images-1.medium.com/max/800/1*xQpIu7eeclDRbdxM-mcAiA.png"></div></figure><p name="37fa" id="37fa" class="graf graf--p graf-after--figure">The possible values that can go wrong here are</p><ol class="postList"><li name="b5d6" id="b5d6" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">c(u, v, s)</code> is 0</li><li name="6dd7" id="6dd7" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">c(u, v)</code> is 0</li><li name="ff51" id="ff51" class="graf graf--li graf-after--li">We get an unknown word in the test sentence, and we don’t have any training tags associated with it.</li></ol><p name="2695" id="2695" class="graf graf--p graf-after--li">All these can be solved via smoothing. So the Laplace smoothing counts would become</p><figure name="d068" id="d068" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 423px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 60.5%;"></div><img class="graf-image" data-image-id="1*H8f_RnB22MxVlIjr_edSJg.png" data-width="1432" data-height="866" src="https://cdn-images-1.medium.com/max/800/1*H8f_RnB22MxVlIjr_edSJg.png"></div></figure><p name="d7db" id="d7db" class="graf graf--p graf-after--figure">Here V is the total number of tags in our corpus and λ is basically a real value between 0 and 1. It acts like a discounting factor. A λ = 1 value would give us <strong class="markup--strong markup--p-strong">too much of a redistribution of values of probabilities. </strong>For example:</p><figure name="d503" id="d503" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 282px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 40.300000000000004%;"></div><img class="graf-image" data-image-id="1*YRPm8NS-Fk5d6MB_3VHsqg.png" data-width="1166" data-height="470" src="https://cdn-images-1.medium.com/max/800/1*YRPm8NS-Fk5d6MB_3VHsqg.png"></div></figure><p name="669b" id="669b" class="graf graf--p graf-after--figure">Too much of a weight is given to unseen trigrams for λ = 1 and that is why the above mentioned modified version of Laplace Smoothing is considered for all practical applications. The value of the discounting factor is to be varied from one application to another.</p><p name="17df" id="17df" class="graf graf--p graf-after--p">Note that λ = 1 would only create a problem if the vocabulary size is too large. For a smaller corpus, λ = 1 would give us a good performance to start off with.</p><p name="3f0e" id="3f0e" class="graf graf--p graf-after--p">A thing to note about Laplace Smoothing is that it is a uniform redistribution, that is, all the trigrams that were previously unseen would have equal probabilities. So, suppose we are given some data and we observe that</p><ul class="postList"><li name="a1d1" id="a1d1" class="graf graf--li graf-after--p">Frequency of trigram &lt;gave, the, thing&gt; is zero</li><li name="ecb2" id="ecb2" class="graf graf--li graf-after--li">Frequency of trigram &lt;gave, the, think&gt; is also zero</li><li name="f240" id="f240" class="graf graf--li graf-after--li">Uniform distribution over unseen events means: <br>P(thing|gave, the) = P(think|gave, the)</li></ul><p name="6c6b" id="6c6b" class="graf graf--p graf-after--li">Does that reflect our knowledge about English use?<br>P(thing|gave, the) &gt; P(think|gave, the) ideally, but uniform distribution using Laplace smoothing will not consider this.</p><p name="bf16" id="bf16" class="graf graf--p graf-after--p">This means that millions of unseen trigrams in a huge corpus would have equal probabilities when they are being considered in our calculations. That is probably not the right thing to do. However, it is better than to consider the 0 probabilities which would lead to these trigrams and eventually some paths in the Viterbi graph getting completely ignored. But this still needs to be worked upon and made better.</p><p name="1e8d" id="1e8d" class="graf graf--p graf-after--p">There are, however, a lot of different types of smoothing techniques that improve upon the basic Laplace Smoothing technique and help overcome this problem of uniform distribution of probabilities. Some of these techniques are:</p><ul class="postList"><li name="f121" id="f121" class="graf graf--li graf-after--p">Good-Turing estimate</li><li name="63ce" id="63ce" class="graf graf--li graf-after--li">Jelinek-Mercer smoothing (interpolation)</li><li name="ab88" id="ab88" class="graf graf--li graf-after--li">Katz smoothing (backoff)</li><li name="e300" id="e300" class="graf graf--li graf-after--li">Witten-Bell smoothing</li><li name="c00a" id="c00a" class="graf graf--li graf-after--li">Absolute discounting</li><li name="0b2c" id="0b2c" class="graf graf--li graf-after--li">Kneser-Ney smoothing</li></ul><p name="15a9" id="15a9" class="graf graf--p graf-after--li">To read more on these different types of smoothing techniques in more detail, refer to <a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf" data-href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this</a> tutorial. Which smoothing technique to choose highly depends upon the type of application at hand, the type of data being considered, and also on the size of the data set.</p><p name="538c" id="538c" class="graf graf--p graf-after--p">If you have been following along this lengthy article, then I must say</p><figure name="97f2" id="97f2" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 906px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 129.4%;"></div><img class="graf-image" data-image-id="1*R1pwNm3-Ju-9E4LdvYak1A.jpeg" data-width="900" data-height="1165" src="https://cdn-images-1.medium.com/max/800/1*R1pwNm3-Ju-9E4LdvYak1A.jpeg"></div><figcaption class="imageCaption">Source: <a href="https://sebreg.deviantart.com/art/You-re-Kind-of-Awesome-289166787" data-href="https://sebreg.deviantart.com/art/You-re-Kind-of-Awesome-289166787" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener" target="_blank">https://sebreg.deviantart.com/art/You-re-Kind-of-Awesome-289166787</a></figcaption></figure><p name="5ced" id="5ced" class="graf graf--p graf-after--figure">Let’s move on and look at a slight optimization that we can do to the Viterbi algorithm that can reduce the number of computations and that also makes sense for a lot of data sets out there.</p><p name="d97d" id="d97d" class="graf graf--p graf-after--p">Before that, however, look at the pseudo-code for the algorithm once again.</p><figure name="a057" id="a057" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 330px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 47.199999999999996%;"></div><img class="graf-image" data-image-id="1*Vex29pOqTVPdtc7Wdz9tlw.png" data-width="1852" data-height="874" src="https://cdn-images-1.medium.com/max/800/1*Vex29pOqTVPdtc7Wdz9tlw.png"></div></figure><p name="8d29" id="8d29" class="graf graf--p graf-after--figure">If we look closely, we can see that <strong class="markup--strong markup--p-strong">for every trigram of words, we are considering all possible set of tags. </strong>That is, if the number of tags are V, then we are considering |V|³ number of combinations for every trigram of the test sentence.</p><p name="ce1f" id="ce1f" class="graf graf--p graf-after--p">Ignore the trigram for now and just consider a single word. We would be considering all of the unique tags for a given word in the above mentioned algorithm. Consider a corpus where we have the word “kick” which is associated with only two tags, say {NN, VB} and the total number of unique tags in the training corpus are around 500 (it’s a huge corpus).</p><figure name="01c6" id="01c6" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 601px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 85.9%;"></div><img class="graf-image" data-image-id="1*P1fkUUTMSTgBneCNqV4sgw.png" data-width="1654" data-height="1420" src="https://cdn-images-1.medium.com/max/800/1*P1fkUUTMSTgBneCNqV4sgw.png"></div></figure><p name="c4f4" id="c4f4" class="graf graf--p graf-after--figure">Now the problem here is apparent. We might end up assigning a tag that doesn’t make sense with the word under consideration, simply because the transition probability of the trigram ending at the tag was very high, like in the example shown above. Also, it would be computationally inefficient to consider all 500 tags for the word “kick” if it only ever occurs with two unique tags in the entire corpus.</p><p name="9f5e" id="9f5e" class="graf graf--p graf-after--p">So, the optimization we do is that for every word, instead of considering all the unique tags in the corpus, <strong class="markup--strong markup--p-strong">we just consider the tags that it occurred with in the corpus</strong>.</p><p name="c3bf" id="c3bf" class="graf graf--p graf-after--p">This would work because, for a reasonably large corpus, a given word would ideally occur with all the various set of tags with which it can occur (most of them at-least). Then it would be reasonable to simply consider just those tags for the Viterbi algorithm.</p><p name="9466" id="9466" class="graf graf--p graf-after--p">As far as the Viterbi decoding algorithm is concerned, the complexity still remains the same because we are always concerned with the worst case complexity. In the worst case, every word occurs with every unique tag in the corpus, and so the complexity remains at O(n|V|³) for the trigram model and O(n|V|²) for the bigram model.</p><p name="bd23" id="bd23" class="graf graf--p graf-after--p">For the recursive implementation of the code, please refer to</p><div name="a7d2" id="a7d2" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/DivyaGodayal/HMM-POS-Tagger" data-href="https://github.com/DivyaGodayal/HMM-POS-Tagger" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/DivyaGodayal/HMM-POS-Tagger"><strong class="markup--strong markup--mixtapeEmbed-strong">DivyaGodayal/HMM-POS-Tagger</strong><br><em class="markup--em markup--mixtapeEmbed-em">HMM-POS-Tagger — An HMM based Part of Speech Tagger implementation using Laplace Smoothing and Trigram HMMs</em>github.com</a><a href="https://github.com/DivyaGodayal/HMM-POS-Tagger" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="6eae171144a70b8b97d1899f9d7c65ec" data-thumbnail-img-id="0*5agih_ZSjxgNTkve." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*5agih_ZSjxgNTkve.);"></a></div><p name="13ad" id="13ad" class="graf graf--p graf-after--mixtapeEmbed">The recursive implementation is done along with Laplace Smoothing.</p><p name="8c6c" id="8c6c" class="graf graf--p graf-after--p">For the iterative implementation, refer to</p><div name="1154" id="1154" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/edorado93/HMM-Part-of-Speech-Tagger" data-href="https://github.com/edorado93/HMM-Part-of-Speech-Tagger" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/edorado93/HMM-Part-of-Speech-Tagger"><strong class="markup--strong markup--mixtapeEmbed-strong">edorado93/HMM-Part-of-Speech-Tagger</strong><br><em class="markup--em markup--mixtapeEmbed-em">HMM-Part-of-Speech-Tagger — An HMM based Part of Speech Tagger</em>github.com</a><a href="https://github.com/edorado93/HMM-Part-of-Speech-Tagger" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="067b589fcee6fada5537444c30b62ca1" data-thumbnail-img-id="0*trBIckExX_4bdurZ." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*trBIckExX_4bdurZ.);"></a></div><p name="cba1" id="cba1" class="graf graf--p graf-after--mixtapeEmbed">This implementation is done with One-Count Smoothing technique which leads to better accuracy as compared to the Laplace Smoothing.</p><p name="80d4" id="80d4" class="graf graf--p graf-after--p">A lot of snapshots of formulas and calculations in the two articles are derived from <a href="http://1.%20http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/hmms.pdf" data-href="http://1. http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/hmms.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>.</p><p name="5851" id="5851" class="graf graf--p graf-after--p graf--trailing">Do let us know how this blog post helped you, and point out the mistakes if you find some while reading the article in the comments section below. Also, please recommend (by clapping) and spread the love as much as possible for this post if you think this might be useful for someone.</p></div></div></section>
</section>
</article></body></html>
