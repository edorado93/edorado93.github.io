---
layout: post
title: "Logistic Regression with a Neural Networks Mindset"
comments: True
---

<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Logistic Regression with a Neural Networks Mindset</title><style>
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 100%;
        margin: auto;
      }
      section {
        width: 100%;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 100%;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Logistic Regression with a Neural Networks Mindset</h1>
</header>
<section data-field="subtitle" class="p-summary">
Build it, train it, test it, Makes it denser, deeper, faster, smarter! — Siraj Raval
</section>
<section data-field="body" class="e-content">
<section name="8b5c" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="8edd" id="8edd" class="graf graf--h3 graf--leading graf--title"></h3></div><div class="section-inner sectionLayout--fullWidth"><figure name="44a3" id="44a3" class="graf graf--figure graf--layoutFillWidth graf-after--h3"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.3%;"></div><img class="graf-image" data-image-id="1*3CIrmxmNJnzNg8J2KFL2AQ.gif" data-width="1920" data-height="1080" data-is-featured="true" src="https://cdn-images-1.medium.com/max/2000/1*3CIrmxmNJnzNg8J2KFL2AQ.gif"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><blockquote name="26f5" id="26f5" class="graf graf--pullquote graf-after--figure">Build it, train it, test it, Makes it denser, deeper, faster, smarter! — <a href="https://medium.com/u/54526471f9bf" data-href="https://medium.com/u/54526471f9bf" data-anchor-type="2" data-user-id="54526471f9bf" data-action-value="54526471f9bf" data-action="show-user-card" data-action-type="hover" class="markup--user markup--pullquote-user" target="_blank">Siraj Raval</a></blockquote><p name="94fa" id="94fa" class="graf graf--p graf-after--pullquote">What’s all the hype about Deep Learning and what is a Neural Network anyway?</p><p name="f235" id="f235" class="graf graf--p graf-after--p">Essentially,</p><ul class="postList"><li name="83b8" id="83b8" class="graf graf--li graf-after--p">you have an architecture</li><li name="17b7" id="17b7" class="graf graf--li graf-after--li">with millions of parameters shared amongst thousands of neurons</li><li name="6853" id="6853" class="graf graf--li graf-after--li">stacked up in multiple layers</li><li name="c630" id="c630" class="graf graf--li graf-after--li">with various activation functions applied to the logits or outputs of the layers</li><li name="9b6e" id="9b6e" class="graf graf--li graf-after--li">and normalized inputs fed with random weight initializations.</li><li name="524f" id="524f" class="graf graf--li graf-after--li">The loss on the training batch defines the gradients for the back-propagation step through the network</li><li name="2af0" id="2af0" class="graf graf--li graf-after--li">and stochastic gradient descent doing its magic to train the model and minimize the loss until convergence.</li></ul><p name="060d" id="060d" class="graf graf--p graf-after--li">If you liked the article, do spread some love and share it as much as possible. That’s all for today folks. Hope you had a fun time reading it. 😃.</p><figure name="ddc2" id="ddc2" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://giphy.com/embed/ZqlvCTNHpqrio/twitter/iframe" width="100%" height="300" frameborder="0" scrolling="no"></iframe><figcaption class="imageCaption">Source: <a href="https://giphy.com/gifs/laughing-despicable-me-minions-ZqlvCTNHpqrio" data-href="https://giphy.com/gifs/laughing-despicable-me-minions-ZqlvCTNHpqrio" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://giphy.com/gifs/laughing-despicable-me-minions-ZqlvCTNHpqrio</a></figcaption></figure><p name="e06f" id="e06f" class="graf graf--p graf-after--figure">You didn’t actually think I’d actually end the article amidst so much confusion about all this technical jargon, did you ? 🤔</p><p name="aefb" id="aefb" class="graf graf--p graf-after--p">If you’re someone with absolutely zero knowledge about any of this stuff and all the terms mentioned before seem Greek to you, don’t fret. Because by the end of this article, I’m sure you will be in a position to train it, test it, make it denser and hence, smarter.</p><p name="e3bf" id="e3bf" class="graf graf--p graf-after--p">Let’s get started now, shall we ?</p><figure name="f6a3" id="f6a3" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://giphy.com/embed/EOpZ7XsVfTN2E/twitter/iframe" width="100%" height="300" frameborder="0" scrolling="no"></iframe><figcaption class="imageCaption">Source: <a href="https://giphy.com/gifs/minions-EOpZ7XsVfTN2E" data-href="https://giphy.com/gifs/minions-EOpZ7XsVfTN2E" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://giphy.com/gifs/minions-EOpZ7XsVfTN2E</a></figcaption></figure><h3 name="f0d4" id="f0d4" class="graf graf--h3 graf-after--figure">The Age old Machine Learning Algorithm</h3><p name="60bc" id="60bc" class="graf graf--p graf-after--h3">Let’s start of with a very brief (well, too brief) an introduction to what one of the oldest algorithms in Machine Learning essentially does.</p><p name="8d4d" id="8d4d" class="graf graf--p graf-after--p">Take some points on a 2D graph, and draw a line that fits them as well as possible. What you have just done is generalized from a few example of pairs of input values (x) and output values (y) to a general function that can map any input value to an output value.</p><p name="e1cb" id="e1cb" class="graf graf--p graf-after--p">This is known as <strong class="markup--strong markup--p-strong">linear regression</strong>, and it is a wonderful technique for extrapolating a general function from some set of input-output pairs.</p><p name="a32b" id="a32b" class="graf graf--p graf-after--p">And here’s why having such a technique is wonderful: there are uncountable number of functions in the real world for which finding the equations is a very difficult task but collecting input-output pairs is relatively easier task to do— for instance, the function mapping an input of recorded audio of a spoken word to an output of what that spoken word is. <a href="http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning/" data-href="http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[Source]</a></p><figure name="0b03" id="0b03" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 457px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 65.3%;"></div><img class="graf-image" data-image-id="1*DsWyE2NY7AxX-RSzHk5mzQ.png" data-width="1440" data-height="940" src="https://cdn-images-1.medium.com/max/800/1*DsWyE2NY7AxX-RSzHk5mzQ.png"></div><figcaption class="imageCaption">Source: <a href="https://www.oreilly.com/" data-href="https://www.oreilly.com/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://www.oreilly.com/</a></figcaption></figure><p name="59ea" id="59ea" class="graf graf--p graf-after--figure">However, as we all know, we can have data that is not linearly separable. The data points might be too scattered that we cannot have a linear function to approximately map the given X values to the given Y values. A linear regression algorithm is very suitable for something like house price prediction given a set of hand crafted features. But it won’t be able to fit data that can only be approximated by a non linear function.</p><p name="56b1" id="56b1" class="graf graf--p graf-after--p">What if the problem statement is that of <strong class="markup--strong markup--p-strong">image classification</strong>? Say we are given an image as an input and we want our model to output if the given image is that of a cat or a dog.</p><p name="6b43" id="6b43" class="graf graf--p graf-after--p">How do we go about solving this problem?</p><p name="2bb8" id="2bb8" class="graf graf--p graf-after--p">Let us start off with the most basic step which is that of <strong class="markup--strong markup--p-strong">preparing the dataset for our model</strong>. We will explain the working of the model later on in the article.</p><p name="cf7a" id="cf7a" class="graf graf--p graf-after--p">One of the most important aspects of building any Machine Learning model is to prepare the dataset and bring it in a suitable format that the model will be able to process and draw meaningful conclusions from.</p><h3 name="3f91" id="3f91" class="graf graf--h3 graf-after--p">Data Preparation</h3><p name="6377" id="6377" class="graf graf--p graf-after--h3">The dataset that we will be looking at in this task is the Cats v/s Dogs binary classification task available on <a href="http://kaggle.com/c/dogs-vs-cats" data-href="http://kaggle.com/c/dogs-vs-cats" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Kaggle</a>. Assuming that you have downloaded the dataset, let us load the dataset in memory and look at a few of the images available in the dataset.</p><figure name="beeb" id="beeb" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 394px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.3%;"></div><img class="graf-image" data-image-id="1*W9z8zHGafvz98XQDyI1_Ww.png" data-width="2522" data-height="1420" src="https://cdn-images-1.medium.com/max/800/1*W9z8zHGafvz98XQDyI1_Ww.png"></div></figure><p name="765c" id="765c" class="graf graf--p graf-after--figure">The data is available as zip files and after unzipping, you should have two different folders. One for <code class="markup--code markup--p-code">train</code> and the other one for <code class="markup--code markup--p-code">test</code> . The <code class="markup--code markup--p-code">test</code> data will not be used throughout the article because we made do with the train dataset itself. The <code class="markup--code markup--p-code">train</code> folder has around 25000 images and we split the them into a smaller <code class="markup--code markup--p-code">train</code> dataset with 2000 images and another one which would server as our validation set containing 5000 images.</p><figure name="c03b" id="c03b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 174px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 24.9%;"></div><img class="graf-image" data-image-id="1*flmbyyalr-gjvrSx2pVSAA.png" data-width="1480" data-height="368" src="https://cdn-images-1.medium.com/max/800/1*flmbyyalr-gjvrSx2pVSAA.png"></div></figure><p name="3efb" id="3efb" class="graf graf--p graf-after--figure">We just printed the name of a random file in the train dataset. The file names are of the type</p><pre name="25b3" id="25b3" class="graf graf--pre graf-after--p">cat.&lt;some number&gt;.jpg or <br>dog.&lt;some number&gt;.jpg</pre><p name="8c78" id="8c78" class="graf graf--p graf-after--pre">Our model will not be able to simply process jpg files. There is some amount of work that has to be done on these images to bring the data in a certain format before our model can process it and make predictions.</p><h4 name="e6cd" id="e6cd" class="graf graf--h4 graf-after--p">Images as Matrix Representations</h4><p name="8439" id="8439" class="graf graf--p graf-after--h4">As discussed earlier, when training our model, we will have a certain image fed into the model and the model will give us a prediction as to whether it thinks the image is that of a cat or a dog. The model may or may not be correct and if it is not, then we have to “train” it so that it gets better at classifying cat and dog images.</p><p name="38a0" id="38a0" class="graf graf--p graf-after--p">A computer stores image data in the form of an <code class="markup--code markup--p-code">M-by-N-by-3</code> data array that defines red, green, and blue color components for each individual pixel. So, if we look at the image data in the form of a multidimensional matrix, we have a 3D matrix with dimensions <code class="markup--code markup--p-code">(M, N, 3)</code> and each value will be an integer value in the range 0–255 where 0 stands for black and 1 stands for white and the remaining values make up different shades of the color components.</p><figure name="2dd5" id="2dd5" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 183px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.1%;"></div><img class="graf-image" data-image-id="1*LTtuPIc1N3_luff9LDR3jA.png" data-width="2292" data-height="598" src="https://cdn-images-1.medium.com/max/800/1*LTtuPIc1N3_luff9LDR3jA.png"></div></figure><p name="a18e" id="a18e" class="graf graf--p graf-after--figure"><a href="https://imageio.github.io/" data-href="https://imageio.github.io/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Imageio</a> is a Python library that provides an easy interface to read and write a wide range of image data, including animated images, video, volumetric data, and scientific formats.</p><p name="f6e1" id="f6e1" class="graf graf--p graf-after--p"><a href="http://www.numpy.org/" data-href="http://www.numpy.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Numpy</a> is a scientific computing package in Python and is one of the most fundamental libraries in Python to manipulate and work with high dimensional arrays efficiently. For a detailed primer on Numpy and how we manipulate image data, read <a href="http://cs231n.github.io/python-numpy-tutorial/#numpy" data-href="http://cs231n.github.io/python-numpy-tutorial/#numpy" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this</a>.</p><p name="cde9" id="cde9" class="graf graf--p graf-after--p">I would strongly recommend going through the Numpy tutorial mentioned above. Programming the model that we will discuss ahead is done primarily in Numpy, and a basic understanding of the operations in Numpy is extremely important.</p><h4 name="9e15" id="9e15" class="graf graf--h4 graf-after--p">Resizing Images</h4><p name="4da3" id="4da3" class="graf graf--p graf-after--h4">Moving on, let us look at what the numpy array for some of these images look like i.e. what is the dimensionality of some of these arrays.</p><figure name="00b8" id="00b8" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 338px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 48.3%;"></div><img class="graf-image" data-image-id="1*xy5iD-h9-B3BVrvcST_OWg.png" data-width="1550" data-height="748" src="https://cdn-images-1.medium.com/max/800/1*xy5iD-h9-B3BVrvcST_OWg.png"></div><figcaption class="imageCaption">Printing out 10 random image sizes from the data we have.</figcaption></figure><p name="3bf2" id="3bf2" class="graf graf--p graf-after--figure">We randomly select some of the images from the list <code class="markup--code markup--p-code">train_data</code> that we created and we print the shape i.e. dimensions of the numpy arrays representing that image.</p><p name="f069" id="f069" class="graf graf--p graf-after--p">As we can see, the image sizes are quite varied. At the most basic level, each pixel will be an input to our image classification model and if the number of pixels are different for each image, then the model won’t be able to process them.</p><p name="e97b" id="e97b" class="graf graf--p graf-after--p">We need the images to be of the same size before we feed them into our model.</p><p name="915f" id="915f" class="graf graf--p graf-after--p">If you are vaguely aware of any of the Machine Learning or Deep Learning models, you must have heard of something known as the <code class="markup--code markup--p-code">parameters</code> of the model.</p><p name="e765" id="e765" class="graf graf--p graf-after--p">The parameters are what carry the crux of the information learnt by our models and the number of parameters for a model has to be fixed before we start training the model.</p><p name="1fa5" id="1fa5" class="graf graf--p graf-after--p">For this very reason, we cannot feed in dynamically sized images. We need to fix the size of the images and hence the number of pixels in each image so that we can define the number of inputs to our model per example and also fix the total learnable parameters for our model.</p><p name="0210" id="0210" class="graf graf--p graf-after--p">Don’t worry if you don’t have any idea about what these <code class="markup--code markup--p-code">parameters</code> actually are. We will get to them soon enough.</p><p name="47ac" id="47ac" class="graf graf--p graf-after--p">For now, the important thing is that <strong class="markup--strong markup--p-strong">we need all the images to be of the same size</strong> for our model to process them.</p><p name="6fba" id="6fba" class="graf graf--p graf-after--p">Let us see how many images have sizes more than <code class="markup--code markup--p-code">64-by-64</code> height and width.</p><figure name="2ac7" id="2ac7" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 177px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 25.2%;"></div><img class="graf-image" data-image-id="1*bXVEyOWbkU4u9p-OkAsDrQ.png" data-width="1482" data-height="374" src="https://cdn-images-1.medium.com/max/800/1*bXVEyOWbkU4u9p-OkAsDrQ.png"></div><figcaption class="imageCaption">99% of the images can be scaled down to 64-by-64–3 images.</figcaption></figure><p name="ba5d" id="ba5d" class="graf graf--p graf-after--figure">As we can clearly see, 99% of the images are have dimensions more than <code class="markup--code markup--p-code">64-by-64</code> and hence, we can downscale them to the size <code class="markup--code markup--p-code">64-by-64-by-3</code> .</p><p name="7423" id="7423" class="graf graf--p graf-after--p">This approach is just a naïve approach that I adopted to make all of the images of the same size and I felt that downscaling would not degrade the quality of the images as much as upscaling would because upscaling a very small image to a large one mostly leads to pixelated effects and would make learning for the the model tougher. Also, this dimension<code class="markup--code markup--p-code">64-by-64-by-3</code> is not a magic number, just something I went with.</p><p name="3ca8" id="3ca8" class="graf graf--p graf-after--p">There would be much better approaches to data preprocessing as far as image data is concerned, but this will suffice for the current article.</p><p name="5e97" id="5e97" class="graf graf--p graf-after--p">Let’s move on and look at the code that would resize all of these images and also split the data into <code class="markup--code markup--p-code">train</code> and <code class="markup--code markup--p-code">test</code> sets. We split the given data using a 80/20 split, i.e. 80% of the data would be used for training our data and the remaining 20% would be used for testing out out model to see the final performance on the unseen data.</p><figure name="8121" id="8121" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 337px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 48.1%;"></div><img class="graf-image" data-image-id="1*brJegYUIgHEz1Rg07YyX5A.png" data-width="1950" data-height="938" src="https://cdn-images-1.medium.com/max/800/1*brJegYUIgHEz1Rg07YyX5A.png"></div><figcaption class="imageCaption">The train_files contain tuples of the form (image_matrix, 1 or 0 depending on if it’s a cat or dog)</figcaption></figure><p name="35ed" id="35ed" class="graf graf--p graf-after--figure">For resizing the images, we used <code class="markup--code markup--p-code"><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imresize.html" data-href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imresize.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">scipy.misc.imresize</a></code> method. This method will be deprecated soon, so better check out some other <a href="https://datascience.stackexchange.com/questions/5224/how-to-prepare-augment-images-for-neural-network" data-href="https://datascience.stackexchange.com/questions/5224/how-to-prepare-augment-images-for-neural-network" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">options</a> for resizing images as well instead of relying on this one for your future adventures.</p><p name="502b" id="502b" class="graf graf--p graf-after--p">The reason that the inner working of the <code class="markup--code markup--p-code">scipy.misc.imresize</code> is not provided here is because it is not relevant to the scope of this article.</p><p name="4e26" id="4e26" class="graf graf--p graf-after--p">Now that we have resized our images, let us look at what our training and test data look like now, i.e. what are their final dimensions.</p><figure name="8ccc" id="8ccc" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 76px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 10.9%;"></div><img class="graf-image" data-image-id="1*BgcM7hjRe9WDC-LRVbpXsw.png" data-width="1596" data-height="174" src="https://cdn-images-1.medium.com/max/800/1*BgcM7hjRe9WDC-LRVbpXsw.png"></div><figcaption class="imageCaption">Final dimensions of our training and test data that can directly be fed into our model.</figcaption></figure><h4 name="dfa0" id="dfa0" class="graf graf--h4 graf-after--figure">Saving the data</h4><p name="ebd7" id="ebd7" class="graf graf--p graf-after--h4">Now that we have processed our data and have it in the format that we need, we can finally save it into two separate files namely <code class="markup--code markup--p-code">train.npz</code> and <code class="markup--code markup--p-code">valid.npz</code> .</p><figure name="1ec4" id="1ec4" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 135px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 19.400000000000002%;"></div><img class="graf-image" data-image-id="1*U1fhWibqTLH8VMBsW3suYQ.png" data-width="1488" data-height="288" src="https://cdn-images-1.medium.com/max/800/1*U1fhWibqTLH8VMBsW3suYQ.png"></div><figcaption class="imageCaption">Saving the training and testing data to files.</figcaption></figure><p name="fb24" id="fb24" class="graf graf--p graf-after--figure">The file size of <code class="markup--code markup--p-code">train.npz</code> is 1.8G and that of <code class="markup--code markup--p-code">valid.npz</code> is 469M. These files contain the numpy arrays that we created earlier on for representing our training and test sets respectively.</p><p name="e759" id="e759" class="graf graf--p graf-after--p">Note, in this dataset we are not using any fancy deep learning architectures. We are only experimenting and showing the power of a single neuron.</p><figure name="7301" id="7301" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 876px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 125.1%;"></div><img class="graf-image" data-image-id="1*OGNO4PTnE60iGF4zQhJEZg.jpeg" data-width="1167" data-height="1460" src="https://cdn-images-1.medium.com/max/800/1*OGNO4PTnE60iGF4zQhJEZg.jpeg"></div><figcaption class="imageCaption">Source: <a href="https://i1.wp.com/blog.eyewire.org" data-href="https://i1.wp.com/blog.eyewire.org" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://i1.wp.com/blog.eyewire.org</a></figcaption></figure><p name="b243" id="b243" class="graf graf--p graf-after--figure">As a result, we don’t have any extensive list of hyper-parameters to tune, and hence we did not split the original data into train, dev, and test. We just sufficed with a training set and a dev set (or a validation set or a test set as far as this article is concerned.) So, we use the term validation data or test data interchangeably in this article.</p><p name="6048" id="6048" class="graf graf--p graf-after--p">Note: The purpose of training data, dev or validation data, and test data are very different from one another. All three of them are extremely important</p><ul class="postList"><li name="da6b" id="da6b" class="graf graf--li graf-after--p">whenever we solve any big problem with loads of data and complex architectures, and</li><li name="752c" id="752c" class="graf graf--li graf-after--li">where we are actually concerned with the final results being on a separate held-out unseen test set.</li></ul><p name="1114" id="1114" class="graf graf--p graf-after--li">We have no such requirements here. And hence, no separate test set.</p><p name="5db8" id="5db8" class="graf graf--p graf-after--p">Have a look at the Jupyter Notebook that brings all of what we discussed above together for you to see.</p><figure name="e347" id="e347" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/edorado93/5a21e8f9d26d1d59a00e523b9b4ffc04.js"></script></figure><h3 name="76ab" id="76ab" class="graf graf--h3 graf-after--figure">Say Hi to the Neuron</h3><p name="623e" id="623e" class="graf graf--p graf-after--h3">Now that we have preprocessed our data and we have it in a format that our binary classification model would be able to understand, allow us to introduce the core component of our model: The Neuron!</p><figure name="d751" id="d751" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://giphy.com/embed/lbDWSy70KJVRe/twitter/iframe" width="100%" height="300" frameborder="0" scrolling="no"></iframe><figcaption class="imageCaption">Source: <a href="https://giphy.com/gifs/lbDWSy70KJVRe" data-href="https://giphy.com/gifs/lbDWSy70KJVRe" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://giphy.com/gifs/lbDWSy70KJVRe</a></figcaption></figure><p name="1543" id="1543" class="graf graf--p graf-after--figure">The neuron is the core computational element of our classification model. Essentially, the neuron performs something known as the <code class="markup--code markup--p-code">forward propagation</code> on the input data.</p><p name="f41f" id="f41f" class="graf graf--p graf-after--p">Let us see what this means.</p><h3 name="8c44" id="8c44" class="graf graf--h3 graf-after--p">Forward Propagation</h3><p name="f655" id="f655" class="graf graf--p graf-after--h3">Let us assume for now that our image is represented by a single real value. We will refer to this single real value as a feature representing our input image.</p><p name="bda9" id="bda9" class="graf graf--p graf-after--p">If you have been following along and have gone through the data preparation step, you would know that we have around <code class="markup--code markup--p-code">12288</code> features in all that represent a single image.</p><p name="e250" id="e250" class="graf graf--p graf-after--p">If you are not sure how we came at this number, don’t worry, we will come to this later.</p><p name="82b5" id="82b5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Hint:</strong> Its 64 * 64 * 3 😅</p><p name="4164" id="4164" class="graf graf--p graf-after--p">Before we proceed, let us look at the notations that we will be following throughout this article</p><ul class="postList"><li name="ac58" id="ac58" class="graf graf--li graf-after--p">small italic alphabet represents a scalar value</li><li name="221b" id="221b" class="graf graf--li graf-after--li">small bold non-italic alphabet represents a vector i.e. either a row matrix <code class="markup--code markup--li-code">1-by-m</code> or a column matrix <code class="markup--code markup--li-code">m-by-1</code>.</li><li name="9ae4" id="9ae4" class="graf graf--li graf-after--li">capital italic alphabets would represents matrices of a given dimension say <code class="markup--code markup--li-code">m-by-n</code> .</li></ul><p name="7489" id="7489" class="graf graf--p graf-after--li">Let <code class="markup--code markup--p-code"><em class="markup--em markup--p-em">x</em></code> denote the single feature that represents our input image.</p><p name="39c7" id="39c7" class="graf graf--p graf-after--p">Now, as a first step to this process called forward propagation,</p><blockquote name="5ecc" id="5ecc" class="graf graf--pullquote graf-after--p">the neuron applies a linear transformation to the input feature</blockquote><p name="6b5a" id="6b5a" class="graf graf--p graf-after--pullquote">Don’t get scared by what this <strong class="markup--strong markup--p-strong">linear transformation</strong> means. It essentially means that we have something of the form represented by the diagram below.</p><figure name="54d9" id="54d9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 241px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 34.4%;"></div><img class="graf-image" data-image-id="1*NOvvMzvrcWMZ9Ji_C02iOw.png" data-width="1406" data-height="484" src="https://cdn-images-1.medium.com/max/800/1*NOvvMzvrcWMZ9Ji_C02iOw.png"></div></figure><p name="bc0c" id="bc0c" class="graf graf--p graf-after--figure">So, given the input feature <em class="markup--em markup--p-em">x, </em>the neuron gives us the following output:</p><pre name="5154" id="5154" class="graf graf--pre graf-after--p"><em class="markup--em markup--pre-em">Wx </em>+ <strong class="markup--strong markup--pre-strong">b</strong></pre><p name="af95" id="af95" class="graf graf--p graf-after--pre">Note the use of notations in the diagram and in the code section above. So, <code class="markup--code markup--p-code"><em class="markup--em markup--p-em">x</em></code><em class="markup--em markup--p-em"> </em>represents a scalar value, <code class="markup--code markup--p-code"><em class="markup--em markup--p-em">W</em></code><em class="markup--em markup--p-em"> </em>represents a matrix, and <strong class="markup--strong markup--p-strong">b </strong>represents a vector.</p><p name="5ec3" id="5ec3" class="graf graf--p graf-after--p">The matrix <em class="markup--em markup--p-em">W </em>is called as the <code class="markup--code markup--p-code">weight</code> matrix and the vector <strong class="markup--strong markup--p-strong">b </strong>is known as the <code class="markup--code markup--p-code">bias</code> vector.</p><p name="349e" id="349e" class="graf graf--p graf-after--p">Remember when we were referring to the parameters of the model earlier on? Well, the weight matrix and the bias vector would represent the parameters of our model in this scenario.</p><p name="d1a3" id="d1a3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Note:</strong> that although we are referring to <code class="markup--code markup--p-code"><em class="markup--em markup--p-em">W</em></code><em class="markup--em markup--p-em"> </em>as our weight “matrix” and <code class="markup--code markup--p-code"><strong class="markup--strong markup--p-strong">b</strong></code><strong class="markup--strong markup--p-strong"> </strong>as our bias “vector”, in the above scenario since we only have one input feature, we only need a <code class="markup--code markup--p-code">1-by-1</code> matrix for <code class="markup--code markup--p-code"><em class="markup--em markup--p-em">W</em></code><em class="markup--em markup--p-em"> </em>and a scalar value for <code class="markup--code markup--p-code"><strong class="markup--strong markup--p-strong">b</strong></code><strong class="markup--strong markup--p-strong"> </strong>.</p><p name="b47e" id="b47e" class="graf graf--p graf-after--p">That’s all there is to the linear transformation of the input feature. We multiply it by a <code class="markup--code markup--p-code">weight</code> and add a <code class="markup--code markup--p-code">bias</code> to it to get a transformed value.</p><h3 name="a686" id="a686" class="graf graf--h3 graf-after--p"><em class="markup--em markup--h3-em">Wx </em>+ <strong class="markup--strong markup--h3-strong">b </strong>= 94.233, What now?</h3><p name="5fc2" id="5fc2" class="graf graf--p graf-after--h3">That’s just a random value off the top of my head. The point I’m trying to make here is what should we do with this transformed value now?</p><p name="15fa" id="15fa" class="graf graf--p graf-after--p">Remember, our ultimate goal is to train a model that will be able to differentiate between a dog and a cat.</p><p name="7114" id="7114" class="graf graf--p graf-after--p">Since this is a binary classification task (just 2 classes for the model to choose from), we need to have some kind of a threshold say <code class="markup--code markup--p-code">Θ</code> . When the neuron generates a value above <code class="markup--code markup--p-code">Θ</code> , it will output one of the classes, otherwise its output would be the second class.</p><p name="f242" id="f242" class="graf graf--p graf-after--p">It is really difficult to get the range of values that a linear transformation would give. The value can be anything ranging from <code class="markup--code markup--p-code">-inf</code> to <code class="markup--code markup--p-code">+inf</code> . It all depends the range of values the input feature(s) can take and how the weight and the bias have been initialized.</p><p name="99e2" id="99e2" class="graf graf--p graf-after--p">So, we definitely need to fix the range of output values by the neuron.</p><p name="d91b" id="d91b" class="graf graf--p graf-after--p">How do we do that?</p><p name="d15b" id="d15b" class="graf graf--p graf-after--p">By applying what is known as an <strong class="markup--strong markup--p-strong">activation</strong> function to the linear output of the neuron.</p><p name="1968" id="1968" class="graf graf--p graf-after--p">The activation function will simply fix the range of output values by the neuron so that we can decide on our threshold <code class="markup--code markup--p-code">Θ</code> for classification output by the neuron.</p><p name="7e44" id="7e44" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Note:</strong> The activation function does a lot more than simply fixing the output values of the neuron, but again, for the scope of this article, knowing this much is more than enough.</p><p name="6948" id="6948" class="graf graf--p graf-after--p">The activation function that we will consider here is known as the Sigmoid function.</p><figure name="ce1d" id="ce1d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 235px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 33.6%;"></div><img class="graf-image" data-image-id="1*Gf2Qf07cng6zsXhRcPimQw.png" data-width="906" data-height="304" src="https://cdn-images-1.medium.com/max/800/1*Gf2Qf07cng6zsXhRcPimQw.png"></div><figcaption class="imageCaption">The Sigmoid activation function</figcaption></figure><p name="05d8" id="05d8" class="graf graf--p graf-after--figure">And here is the graph of the sigmoid function.</p><figure name="3aaa" id="3aaa" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 486px; max-height: 426px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 87.7%;"></div><img class="graf-image" data-image-id="1*a5QwiyaSyvRa6n3VKYVEnQ.png" data-width="486" data-height="426" src="https://cdn-images-1.medium.com/max/800/1*a5QwiyaSyvRa6n3VKYVEnQ.png"></div><figcaption class="imageCaption">Source: <a href="https://www.vaetas.cz/img/machine-learning/sigmoid-function.png" data-href="https://www.vaetas.cz/img/machine-learning/sigmoid-function.png" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://www.vaetas.cz/img/machine-learning/sigmoid-function.png</a></figcaption></figure><p name="a892" id="a892" class="graf graf--p graf-after--figure">As we can see from the graph above, the sigmoid activation function applies what is known as a <code class="markup--code markup--p-code">non linear transformation</code> onto the input value and the range of the sigmoid function is a set of real values between [0, 1].</p><p name="96c1" id="96c1" class="graf graf--p graf-after--p">So, the neuron in its entirety essentially performs two operations as a part of the forward propagation process.</p><ol class="postList"><li name="a7f4" id="a7f4" class="graf graf--li graf-after--p">Apply a linear transformation on the input feature(s) and</li><li name="99a9" id="99a9" class="graf graf--li graf-after--li">Apply a non linear transformation (sigmoid in our case) on top of the previous output to give the final output.</li></ol><figure name="1e3c" id="1e3c" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 174px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 24.8%;"></div><img class="graf-image" data-image-id="1*NqQz69UgV083Id6Ex7pcvA.png" data-width="2096" data-height="520" src="https://cdn-images-1.medium.com/max/800/1*NqQz69UgV083Id6Ex7pcvA.png"></div><figcaption class="imageCaption">Complete set of transformations by a neuron.</figcaption></figure><h3 name="799e" id="799e" class="graf graf--h3 graf-after--figure">But wait. You said each image has 12288 features?</h3><p name="1d07" id="1d07" class="graf graf--p graf-after--h3">We explained the calculations above assuming that the input image would be represented by a single feature value.</p><p name="c70b" id="c70b" class="graf graf--p graf-after--p">That is, however, not the case we are dealing with. Remember we had gone through the entire data preparation step before we started off with forward propagation and we had rescaled our images to <code class="markup--code markup--p-code">64-by-64-by-3</code>?</p><p name="60c8" id="60c8" class="graf graf--p graf-after--p">This means that our processed image is essentially composed of <code class="markup--code markup--p-code">12288</code> pixels in all.</p><p name="59c6" id="59c6" class="graf graf--p graf-after--p">For our use case and the simplistic classification model that we are dealing with, we will simply consider each of the pixels as an input feature.</p><p name="3432" id="3432" class="graf graf--p graf-after--p">That means that we have <code class="markup--code markup--p-code">12288</code> input features per image for our model. Now let us see the changes this introduces into the dimensions of our model’s parameters, i.e. the weight and the bias.</p><figure name="924d" id="924d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 724px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 103.4%;"></div><img class="graf-image" data-image-id="1*fJmDBOaKrIuEKo2-INTP6g.png" data-width="1050" data-height="1086" src="https://cdn-images-1.medium.com/max/800/1*fJmDBOaKrIuEKo2-INTP6g.png"></div></figure><p name="5ec0" id="5ec0" class="graf graf--p graf-after--figure">Let us represent the number of input features of our image by <code class="markup--code markup--p-code">nx</code> . For the images we will be considering here, the number of input features would be 12288.</p><p name="a242" id="a242" class="graf graf--p graf-after--p">So, we feed in all these input features for a given image to our neuron, it does a linear transformation on each of the features, combines the result to give a scalar value, then applies the sigmoid transformation on the value to finally give us <code class="markup--code markup--p-code">y^</code> i.e. the class this image belongs to.</p><p name="23bc" id="23bc" class="graf graf--p graf-after--p">Note, the model simply outputs a real value between 0 and 1.</p><p name="be7d" id="be7d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The interpretation is that if the value is &gt; 0.5, it’s a dog/cat (whatever way you want to consider) and for all other values, its the other class.</strong></p><p name="aa0b" id="aa0b" class="graf graf--p graf-after--p">We can either represent the input features as a <code class="markup--code markup--p-code">column vector</code> or a <code class="markup--code markup--p-code">row vector</code> . So, we can either have a vector of shape <code class="markup--code markup--p-code">12288-by-1)</code> or <code class="markup--code markup--p-code">1-by-12288</code> . Let us consider the former i.e. we will have a column vector.</p><figure name="81c5" id="81c5" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 272px; max-height: 502px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 184.60000000000002%;"></div><img class="graf-image" data-image-id="1*uCvi3E7SeSbSHdBURchO9Q.png" data-width="272" data-height="502" src="https://cdn-images-1.medium.com/max/800/1*uCvi3E7SeSbSHdBURchO9Q.png"></div><figcaption class="imageCaption">Column vector representing the input features.</figcaption></figure><p name="1390" id="1390" class="graf graf--p graf-after--figure">We will get to the coding part in the next section. It’s not a huge task to convert the <code class="markup--code markup--p-code">64-by-64-by-3</code> image pixel values to <code class="markup--code markup--p-code">12288-by-1</code> .</p><p name="0457" id="0457" class="graf graf--p graf-after--p">Earlier we had explained the forward propagation process for just a single input feature. We had a weight value for that single input feature and then we also had a bias value for it that combined and gave us the linear transformation we were looking for.</p><p name="57dd" id="57dd" class="graf graf--p graf-after--p">In a similar fashion, we will be needing a weight value for <strong class="markup--strong markup--p-strong">each<em class="markup--em markup--p-em"> </em></strong>of the input features. We don’t need a separate bias value for each of the features. A single value i.e. a scalar value would suffice here.</p><p name="754f" id="754f" class="graf graf--p graf-after--p">The linear transformation now becomes:</p><figure name="1f99" id="1f99" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 254px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 36.3%;"></div><img class="graf-image" data-image-id="1*9i3llqOioPkiikiNf6Bcxw.png" data-width="1574" data-height="572" src="https://cdn-images-1.medium.com/max/800/1*9i3llqOioPkiikiNf6Bcxw.png"></div><figcaption class="imageCaption">Linear transformation of the image’s input features</figcaption></figure><p name="0fba" id="0fba" class="graf graf--p graf-after--figure">For every input feature, we multiply it with it’s corresponding weight value and then we add all these values together and finally add the bias term to get the linear transformation value.</p><p name="e10a" id="e10a" class="graf graf--p graf-after--p">The next step remains the same: we apply the sigmoid activation function on <code class="markup--code markup--p-code"><em class="markup--em markup--p-em">z</em></code><em class="markup--em markup--p-em"> </em>and we obtain a real value between 0 and 1 (remember, that’s the range of the sigmoid function).</p><h3 name="ccfc" id="ccfc" class="graf graf--h3 graf-after--p">Implementation!</h3><p name="9126" id="9126" class="graf graf--p graf-after--h3">This is the fun part. 😉</p><p name="4a91" id="4a91" class="graf graf--p graf-after--p">We will go about this in multiple steps. Just like our final Jupyter Notebook would be structured.</p><h4 name="b8fb" id="b8fb" class="graf graf--h4 graf-after--p">Retrieving the data</h4><figure name="2af4" id="2af4" class="graf graf--figure graf-after--h4"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 158px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 22.6%;"></div><img class="graf-image" data-image-id="1*P79CrxXGktNJmRxVsV62mA.png" data-width="1682" data-height="380" src="https://cdn-images-1.medium.com/max/800/1*P79CrxXGktNJmRxVsV62mA.png"></div><figcaption class="imageCaption">Obtaining the data from the saved files</figcaption></figure><figure name="56a4" id="56a4" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 96px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 13.700000000000001%;"></div><img class="graf-image" data-image-id="1*Lmqi2fbp51aSkLnNeP7qog.png" data-width="1766" data-height="242" src="https://cdn-images-1.medium.com/max/800/1*Lmqi2fbp51aSkLnNeP7qog.png"></div><figcaption class="imageCaption">Shapes of the individual numpy arrays</figcaption></figure><p name="9eb5" id="9eb5" class="graf graf--p graf-after--figure">Remember we saved our data after preprocessing in two files namely <code class="markup--code markup--p-code">train.npz</code> and <code class="markup--code markup--p-code">valid.npz</code> ? We will load our data from them and return 4 different numpy arrays.</p><ol class="postList"><li name="6748" id="6748" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">train_x_original</code> represents our training set images in their original dimensions.</li><li name="5129" id="5129" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">train_y</code> contains the corresponding labels for each of the image. Just to remind you, a <code class="markup--code markup--li-code">1</code> represents a cat and a <code class="markup--code markup--li-code">0</code> represents a dog.</li><li name="cd21" id="cd21" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">valid_x_original</code> is the same as <code class="markup--code markup--li-code">train_x_original</code> except it contains the validation dataset i.e. the dataset on which we will evaluate our model’s performance.</li><li name="4140" id="4140" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">valid_y</code> are the labels for the validation set images.</li></ol><p name="1947" id="1947" class="graf graf--p graf-after--li">Now we have our entire training and validation dataset loaded into the memory. As we discussed earlier, we want to convert all the features of the given image into either a column vector or a row vector. So, for every image, we want either a <code class="markup--code markup--p-code">12288-by-1</code> vector or a <code class="markup--code markup--p-code">1-by-12288</code> vector. Let us look at how we can do that from our original images of dimensions<code class="markup--code markup--p-code">64-by-64-by-3</code> .</p><h4 name="b04c" id="b04c" class="graf graf--h4 graf-after--p">Image Flattening</h4><p name="2aa5" id="2aa5" class="graf graf--p graf-after--h4">This is a pretty straightforward task in NumPy. Given an image of dimension <code class="markup--code markup--p-code">64-by-64-by-3</code>we simply want to change it’s shape to either <code class="markup--code markup--p-code">12288-by-1</code> or <code class="markup--code markup--p-code">1-by-12288</code> .</p><p name="44c6" id="44c6" class="graf graf--p graf-after--p">Since, we have a whole bunch of images, it would be either <code class="markup--code markup--p-code">12288-by-m</code> or <code class="markup--code markup--p-code">m-by-12288</code> where <code class="markup--code markup--p-code">m</code> represents the total number of images that we would feed our model i.e. the number of training examples.</p><p name="a789" id="a789" class="graf graf--p graf-after--p">Let us have a look at the code for this transformation.</p><figure name="002b" id="002b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 382px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 54.6%;"></div><img class="graf-image" data-image-id="1*-Fa3PKCRDS1GSRzNPVHlGw.png" data-width="2288" data-height="1250" src="https://cdn-images-1.medium.com/max/800/1*-Fa3PKCRDS1GSRzNPVHlGw.png"></div></figure><p name="33de" id="33de" class="graf graf--p graf-after--figure">So we defined a function called as <code class="markup--code markup--p-code">image2vec</code> which essentially takes in our entire dataset in its original dimension i.e. <code class="markup--code markup--p-code">20000-by-64-by-64-by-3</code> for the training set and <code class="markup--code markup--p-code">5000-by-64-by-64-by-3</code> for the validation set and returns the flattened matrices.</p><p name="901e" id="901e" class="graf graf--p graf-after--p">Did you notice these lines of code?</p><pre name="2e08" id="2e08" class="graf graf--pre graf-after--p"># Normalize our dataset<br>train_x /= 255.<br>valid_x /= 255.</pre><p name="3099" id="3099" class="graf graf--p graf-after--pre">Don’t worry, we will get to this in the next section when we discuss the activation function.</p><p name="b87d" id="b87d" class="graf graf--p graf-after--p">Coming back to our resulting matrix, its is a 2D one and the first index is now the number of features in each of our images i.e. <code class="markup--code markup--p-code">12288</code> and the second index represents the number of samples in that dataset which are <code class="markup--code markup--p-code">20000</code> in the training set and <code class="markup--code markup--p-code">5000</code> in the validation set.</p><p name="98e6" id="98e6" class="graf graf--p graf-after--p">You might ask why this specific way of arranging our data. Why didn’t we go for the transposed versions i.e. <code class="markup--code markup--p-code">m-by-12288</code> where <code class="markup--code markup--p-code">m</code> represents the number of samples in a dataset.</p><p name="0f9e" id="0f9e" class="graf graf--p graf-after--p">I’d say we could have done that. There is nothing wrong with that arrangement of image features. However, you will notice the advantage of arranging our data in this way in the upcoming sections when we get to forward propagation for the entire dataset.</p><p name="52eb" id="52eb" class="graf graf--p graf-after--p">Visually, the final flattened matrix looks like this</p><figure name="6f30" id="6f30" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 369px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 52.800000000000004%;"></div><img class="graf-image" data-image-id="1*V8OxIKYrVuuPUhkD5LTMXA.png" data-width="1884" data-height="994" src="https://cdn-images-1.medium.com/max/800/1*V8OxIKYrVuuPUhkD5LTMXA.png"></div><figcaption class="imageCaption">The flattened matrix representing our images.</figcaption></figure><h4 name="f084" id="f084" class="graf graf--h4 graf-after--figure">Sigmoid Activation Function</h4><p name="d33a" id="d33a" class="graf graf--p graf-after--h4">Next step is defining our activation function. We described in the previous sections that we use a non linear activation function which for this task is the sigmoid function.</p><figure name="f06f" id="f06f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 118px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.8%;"></div><img class="graf-image" data-image-id="1*SjRCsEHzX98O84YqBo1HDg.png" data-width="1284" data-height="216" src="https://cdn-images-1.medium.com/max/800/1*SjRCsEHzX98O84YqBo1HDg.png"></div></figure><p name="c940" id="c940" class="graf graf--p graf-after--figure">For your reference once again, here is how the sigmoid function looks like.</p><figure name="d933" id="d933" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 486px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 69.5%;"></div><img class="graf-image" data-image-id="1*CQyZ_xFQHa8uCJX8jMsq7A.png" data-width="1822" data-height="1266" src="https://cdn-images-1.medium.com/max/800/1*CQyZ_xFQHa8uCJX8jMsq7A.png"></div><figcaption class="imageCaption">Sigmoid function.</figcaption></figure><p name="7285" id="7285" class="graf graf--p graf-after--figure">As explained before and as can be seen in the figure above,</p><blockquote name="b4ae" id="b4ae" class="graf graf--pullquote graf-after--p">sigmoid gives a value of 0 for very high or very low values</blockquote><p name="0e55" id="0e55" class="graf graf--p graf-after--pullquote">Hence, we need to normalize our feature values so that we don’t have too large or too small values. Let me explain this with the help of an example.</p><p name="2956" id="2956" class="graf graf--p graf-after--p">Consider the following images of cats.</p><figure name="eb92" id="eb92" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 347px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 49.5%;"></div><img class="graf-image" data-image-id="1*TLXi3KTnDb7-_vS0ilbP4Q.png" data-width="2036" data-height="1008" src="https://cdn-images-1.medium.com/max/800/1*TLXi3KTnDb7-_vS0ilbP4Q.png"></div></figure><p name="d8ec" id="d8ec" class="graf graf--p graf-after--figure">As we saw before, we have to resize our images to bring them to a fixed size. So, let us look at both these images after resizing them to <code class="markup--code markup--p-code">64-by-64-by-3</code> .</p><figure name="a6e7" id="a6e7" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 332px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 47.4%;"></div><img class="graf-image" data-image-id="1*xdHmf4xP9NbO17_N6LXGVA.png" data-width="1606" data-height="762" src="https://cdn-images-1.medium.com/max/800/1*xdHmf4xP9NbO17_N6LXGVA.png"></div><figcaption class="imageCaption">Resized images.</figcaption></figure><p name="31af" id="31af" class="graf graf--p graf-after--figure">Each image is essentially is 3D matrix consisting of RGB values of different intensities and essentially they represent the colors for a given image.</p><p name="f317" id="f317" class="graf graf--p graf-after--p">The pixel values (be it for R, G or B) range from <code class="markup--code markup--p-code">0-255</code> where a <code class="markup--code markup--p-code">0</code> represents complete black and <code class="markup--code markup--p-code">255</code>is for white. In between there are millions of color combinations possible.</p><p name="43df" id="43df" class="graf graf--p graf-after--p">Let’s have a look at some of the pixel values for each of these images.</p><pre name="f04c" id="f04c" class="graf graf--pre graf-after--p">[137, 109, 70, 144, 117, 74, 154, 126, 79, 160, 132, 85, 165, 137, 90, 167, 139, 92, 178, 150, 103, 186, 159, 112, 194, 170, 126, 204, 182, 143, 210, 187, 153, 212, 188, 150, 213, 185, 138, 214, 178, 121, 213, 170, 100, 212, 164, 85, 213, 158, 73, 212, 155, 65, 215, 159, 72, 215, 161, 75, 217, 162, 80, 216, 165, 82, 214, 166, 84, 212, 166, 84, 221, 187, 124, 226, 205, 160, 232, 212, 175, 236, 216, 179, 242, 220, 183, 242, 219, 177, 243, 215, 167, 243, 213, 162, 242, 213, 163, 240]</pre><p name="76c0" id="76c0" class="graf graf--p graf-after--pre">These are 100 consecutive pixel values for the colored image. What matters here is the value of each of these pixels. The values are pretty high as one would expect from a colored image as that of the brown(ish) cat shown above.</p><pre name="3862" id="3862" class="graf graf--pre graf-after--p">[37, 37, 37, 37, 37, 37, 29, 29, 29, 35, 35, 35, 39, 39, 39, 36, 36, 36, 38, 38, 38, 88, 88, 88, 43, 43, 43, 34, 34, 34, 33, 33, 33, 52, 52, 52, 48, 48, 48, 40, 40, 40, 33, 33, 33, 33, 33, 33, 39, 39, 39, 52, 52, 52, 36, 36, 36, 34, 34, 34, 46, 46, 46, 31, 31, 31, 34, 34, 34, 33, 33, 33, 35, 35, 35, 34, 34, 34, 26, 26, 26, 32, 32, 32, 25, 25, 25, 29, 29, 29, 23, 23, 23, 44, 44, 44, 43, 43, 43, 20]</pre><p name="7f2f" id="7f2f" class="graf graf--p graf-after--pre">Same set of pixel values but for the black image are the ones shown above. As we can see clearly, these values are much smaller than the values corresponding to the colored image.</p><p name="d25f" id="d25f" class="graf graf--p graf-after--p">The reason is straightforward in that the black and white pixel values are the ones in the vicinity of 0 and naturally, they are smaller as compared to the colored values which are in the vicinity of 255 i.e. white.</p><p name="6478" id="6478" class="graf graf--p graf-after--p">We had talked about linear transformation on the input features of the given image. For revision purposes, here is the linear transformation formula that we had used for an image with multiple features.</p><figure name="f104" id="f104" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 254px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 36.3%;"></div><img class="graf-image" data-image-id="1*9i3llqOioPkiikiNf6Bcxw.png" data-width="1574" data-height="572" src="https://cdn-images-1.medium.com/max/800/1*9i3llqOioPkiikiNf6Bcxw.png"></div><figcaption class="imageCaption">Linear transformation of the image’s input features</figcaption></figure><p name="aa25" id="aa25" class="graf graf--p graf-after--figure">For the same weight matrix <code class="markup--code markup--p-code"><em class="markup--em markup--p-em">W</em></code><em class="markup--em markup--p-em"> </em>and bias vector <code class="markup--code markup--p-code"><strong class="markup--strong markup--p-strong">b</strong></code><strong class="markup--strong markup--p-strong"> </strong>, we will get extremely high values for the features of the colored image as compared to that of the black and white image, right ?</p><p name="6e57" id="6e57" class="graf graf--p graf-after--p">After this linear transformation we apply the sigmoidal activation function and we saw earlier that the sigmoid activation function gives an output of 0 for very high or very low values.</p><p name="1400" id="1400" class="graf graf--p graf-after--p">So, naturally, the sigmoid activation for the colored cat would almost always end up as 0.</p><p name="647e" id="647e" class="graf graf--p graf-after--p">How do we solve this problem one might ask?</p><p name="4b90" id="4b90" class="graf graf--p graf-after--p">We <strong class="markup--strong markup--p-strong">Normalize.</strong></p><pre name="6a9f" id="6a9f" class="graf graf--pre graf-after--p"># Normalize our dataset<br>train_x /= 255.<br>valid_x /= 255.</pre><p name="ae55" id="ae55" class="graf graf--p graf-after--pre">This will bring all the input features in the range <code class="markup--code markup--p-code">[0,1]</code> and hence all these values, be it colored images or black and white images, would have a common range. We call this process normalization of image features.</p><p name="2a7e" id="2a7e" class="graf graf--p graf-after--p">Now that we have all of our data processed and loaded in memory, the only thing that remains is our network i.e. the one powered by our single neuron. Now we will feed all these images to our model iteratively and the model will eventually learn (with some accuracy) to classify images as dogs or cats.</p><p name="b2ba" id="b2ba" class="graf graf--p graf-after--p">Just to summarize what all we we have done till now:</p><ul class="postList"><li name="fdb8" id="fdb8" class="graf graf--li graf-after--p">Process the image dataset available to us and we converted <code class="markup--code markup--li-code">.jpg</code> images to numpy arrays that our model will be able to process.</li><li name="8959" id="8959" class="graf graf--li graf-after--li">Then we loaded the two files <code class="markup--code markup--li-code">train.npz</code> and <code class="markup--code markup--li-code">valid.npz</code> in memory and we flattened the images so that we have <code class="markup--code markup--li-code">12288-by-1</code> instead of <code class="markup--code markup--li-code">64-by-64-by-3</code> images. We essentially brought all of the features for the image in a single column.</li><li name="3892" id="3892" class="graf graf--li graf-after--li">We defined our sigmoid activation function and finally,</li><li name="6c1e" id="6c1e" class="graf graf--li graf-after--li">We discussed why normalizing the data is a necessary step.</li></ul><p name="25a1" id="25a1" class="graf graf--p graf-after--li">Now, we are ready to move on to developing our model.</p><h3 name="e062" id="e062" class="graf graf--h3 graf-after--p">Feeding the Network 🥘</h3><p name="35fc" id="35fc" class="graf graf--p graf-after--h3">Finally we get to the point where we are ready to feed an entire image to our network and get some predictions out of it. Let’s see how we can do that firstly for a single image and then for our entire dataset all at once.</p><h4 name="f856" id="f856" class="graf graf--h4 graf-after--p">Single Image</h4><p name="8c84" id="8c84" class="graf graf--p graf-after--h4">As discussed before, every image now has a dimension of <code class="markup--code markup--p-code">12288-by-1</code> and when we refer to the word “image”, what we really mean are the features of that image that have been flattened out and have been normalized.</p><p name="3e81" id="3e81" class="graf graf--p graf-after--p">For the linear transformation, we need to have a weight matrix and a bias vector. We know that the model will eventually give us a single value between 0 and 1 i.e. after applying the sigmoid activation function and hence, the bias is simply a scalar.</p><p name="a855" id="a855" class="graf graf--p graf-after--p">Essentially, we need this operation:</p><figure name="7126" id="7126" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 398px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.89999999999999%;"></div><img class="graf-image" data-image-id="1*epe5lbATkfGAgrXDdGmZPA.png" data-width="1512" data-height="860" src="https://cdn-images-1.medium.com/max/800/1*epe5lbATkfGAgrXDdGmZPA.png"></div></figure><figure name="60f9" id="60f9" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 99px; max-height: 94px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 94.89999999999999%;"></div><img class="graf-image" data-image-id="0*3lOqit4y7kj08v4A" data-width="99" data-height="94" src="https://cdn-images-1.medium.com/max/800/0*3lOqit4y7kj08v4A"></div></figure><p name="e335" id="e335" class="graf graf--p graf-after--figure">This is what is known as the <strong class="markup--strong markup--p-strong">Hadamard product </strong>or the element wise product of two vectors and then we do the summation of all these values.</p><p name="f77c" id="f77c" class="graf graf--p graf-after--p">Instead of doing this, we can use <strong class="markup--strong markup--p-strong">a dot product of the weight matrix and the feature vector</strong>.</p><figure name="f488" id="f488" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 382px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 54.50000000000001%;"></div><img class="graf-image" data-image-id="1*ElKdT7LHxgrZFZgiI2r4lg.png" data-width="1482" data-height="808" src="https://cdn-images-1.medium.com/max/800/1*ElKdT7LHxgrZFZgiI2r4lg.png"></div></figure><p name="b465" id="b465" class="graf graf--p graf-after--figure">The dot product of the weight matrix and the vector representing the features of the input image would give us the summation value we are looking for.</p><p name="01a3" id="01a3" class="graf graf--p graf-after--p">We can either define the weight matrix as a row vector and use the equation</p><pre name="68ac" id="68ac" class="graf graf--pre graf-after--p"><em class="markup--em markup--pre-em">W . </em><strong class="markup--strong markup--pre-strong">x </strong>+ <strong class="markup--strong markup--pre-strong">b</strong></pre><p name="a478" id="a478" class="graf graf--p graf-after--pre">or we can take the weight matrix as a column vector and then do a transpose of it for the dot product.</p><pre name="2c22" id="2c22" class="graf graf--pre graf-after--p">transpose(<em class="markup--em markup--pre-em">W</em>) . <strong class="markup--strong markup--pre-strong">x</strong><em class="markup--em markup--pre-em"> </em>+ <strong class="markup--strong markup--pre-strong">b</strong></pre><p name="aeb5" id="aeb5" class="graf graf--p graf-after--pre">Here, we will go with the second option. We consider the weight matrix to be of the shape <code class="markup--code markup--p-code">12288-by-1</code> for a single image. For the purpose of linear transformation, we do the transpose of the weight matrix before doing a dot product with the feature vector.</p><p name="9070" id="9070" class="graf graf--p graf-after--p">For now, just know that we want the weight values per image to be arranged in the form of a single column rather than rows. This will make the calculations a whole lot easier moving forwards.</p><p name="d241" id="d241" class="graf graf--p graf-after--p">Same goes for the input features. We want them to be arranged them in columns.</p><h4 name="fe64" id="fe64" class="graf graf--h4 graf-after--p">Feeding in the Entire training set</h4><p name="dcda" id="dcda" class="graf graf--p graf-after--h4">We don’t really want to process one image at a time as that would be too slow.</p><p name="b696" id="b696" class="graf graf--p graf-after--p">Ideally, we want to make a single forward pass over our model (i.e. the single neuron here) and obtain predictions for the entire training set. All in one go!</p><p name="db66" id="db66" class="graf graf--p graf-after--p">We can achieve this using the equation we looked at earlier:</p><pre name="04da" id="04da" class="graf graf--pre graf-after--p">transpose(<em class="markup--em markup--pre-em">W</em>) . <em class="markup--em markup--pre-em">X </em>+ <strong class="markup--strong markup--pre-strong">b</strong></pre><p name="9bbe" id="9bbe" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Note: </strong><code class="markup--code markup--p-code"><em class="markup--em markup--p-em">X</em></code> represents a matrix containing all of our images and <code class="markup--code markup--p-code"><strong class="markup--strong markup--p-strong">x</strong></code><em class="markup--em markup--p-em"> </em>represents a single image vector.</p><figure name="98c5" id="98c5" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 294px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 42%;"></div><img class="graf-image" data-image-id="1*DwTuChaiToN9qnhPi4cDVg.png" data-width="1728" data-height="726" src="https://cdn-images-1.medium.com/max/800/1*DwTuChaiToN9qnhPi4cDVg.png"></div><figcaption class="imageCaption">Linear transformation on the entire dataset of m examples all in one go.</figcaption></figure><figure name="8c5c" id="8c5c" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 246px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 35.099999999999994%;"></div><img class="graf-image" data-image-id="1*GwF4WdSueEOLWOcsQ47WAw.png" data-width="1648" data-height="578" src="https://cdn-images-1.medium.com/max/800/1*GwF4WdSueEOLWOcsQ47WAw.png"></div><figcaption class="imageCaption">Dimensions of matrices involved in our calculations</figcaption></figure><p name="0a9d" id="0a9d" class="graf graf--p graf-after--figure">Notice the change in the notations in the equation. We were earlier using small <code class="markup--code markup--p-code"><strong class="markup--strong markup--p-strong">x</strong></code> to denote the features of a single image.</p><p name="2ec4" id="2ec4" class="graf graf--p graf-after--p">Since we are processing our entire dataset at once, we shifted to the capital italic<code class="markup--code markup--p-code"><em class="markup--em markup--p-em">X</em></code> notation that depicts the entire dataset and as mentioned in the figure above, it is of the dimension <code class="markup--code markup--p-code">12288-by-m</code> where each image consists of 12288 features and there are <code class="markup--code markup--p-code">m</code> examples in all.</p><p name="c172" id="c172" class="graf graf--p graf-after--p">Let’s look at the code for this.</p><figure name="5369" id="5369" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 399px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.99999999999999%;"></div><img class="graf-image" data-image-id="1*IBzoDHrXF-pt6oHjlguTow.png" data-width="1782" data-height="1016" src="https://cdn-images-1.medium.com/max/800/1*IBzoDHrXF-pt6oHjlguTow.png"></div></figure><p name="625f" id="625f" class="graf graf--p graf-after--figure">The function <code class="markup--code markup--p-code">forward_propagate</code> is the one doing all the heavy lifting for us. We first get the number of examples (this is not being used here, but I put it just to show that the second dimension represents number of examples).</p><p name="761d" id="761d" class="graf graf--p graf-after--p">According to the algorithm we have discussed till now, we first do a linear transformation on the input matrix <code class="markup--code markup--p-code"><em class="markup--em markup--p-em">X</em></code> , which represents our dataset of images.</p><p name="c20c" id="c20c" class="graf graf--p graf-after--p">Then we apply the sigmoid activation function on the resultant matrix (vector in this case) and obtain the non linearity applied activation values from the neuron.</p><p name="4a0f" id="4a0f" class="graf graf--p graf-after--p">We initialized a random dataset here and used it to show the running and output of the <code class="markup--code markup--p-code">forward_propagate</code> function above.</p><h3 name="7e0a" id="7e0a" class="graf graf--h3 graf-after--p">Let’s Predict 🤩</h3><p name="2942" id="2942" class="graf graf--p graf-after--h3">Now that we have defined our neuron’s structure and the computation it performs on the image features, we are ready to make some actual classifications with our model.</p><p name="5e18" id="5e18" class="graf graf--p graf-after--p">But, before we do that, we need to have some sort of measure to see how well our model is actually doing on the test set.</p><p name="444d" id="444d" class="graf graf--p graf-after--p">Let’s look at the code which calculates test set accuracy of our model’s predictions. This is the core metric that we will be using throughout the remainder of our article to assess how our model is performing.</p><figure name="c817" id="c817" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 420px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 60.099999999999994%;"></div><img class="graf-image" data-image-id="1*-5RToiNDgZCZqKH2JGQ8QQ.png" data-width="2098" data-height="1260" src="https://cdn-images-1.medium.com/max/800/1*-5RToiNDgZCZqKH2JGQ8QQ.png"></div></figure><p name="f473" id="f473" class="graf graf--p graf-after--figure">This is the code for measuring how accurate our model is in the cat vs dog classification task (test set). It turns out that an untrained model — we randomly initialized the weights and the bias values — achieves almost 50% accuracy. This is expected from a random sampler, because this is a 2 class classification task. If we pick a value from [0,1] randomly, theres a 50% probability that we will get the right value.</p><p name="5262" id="5262" class="graf graf--p graf-after--p">The question now arises, how do we improve our model?</p><p name="6bc7" id="6bc7" class="graf graf--p graf-after--p">We can improve our model by an algorithm called the <strong class="markup--strong markup--p-strong">Gradient Descent. </strong>Let’s move ahead and see what this algorithm is all about and how it can help us improve our model.</p><h3 name="f98c" id="f98c" class="graf graf--h3 graf-after--p">Let’s Go Down the Hill 🗻</h3><p name="e614" id="e614" class="graf graf--p graf-after--h3">The whole point of the gradient descent algorithm is to minimize the cost function so that our neuron-based model is able to learn.</p><figure name="963d" id="963d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 700px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><img class="graf-image" data-image-id="1*P0qx-JPybIs5UBE0J6Y59A.jpeg" data-width="720" data-height="720" src="https://cdn-images-1.medium.com/max/800/1*P0qx-JPybIs5UBE0J6Y59A.jpeg"></div><figcaption class="imageCaption">Source: <a href="https://www.pinterest.com/pin/409053578638780708/?lp=true" data-href="https://www.pinterest.com/pin/409053578638780708/?lp=true" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://www.pinterest.com/pin/409053578638780708/?lp=true</a></figcaption></figure><p name="633d" id="633d" class="graf graf--p graf-after--figure">What’s this <strong class="markup--strong markup--p-strong">cost function?</strong></p><p name="2a20" id="2a20" class="graf graf--p graf-after--p">Our model <strong class="markup--strong markup--p-strong">learns as well?</strong></p><p name="56e6" id="56e6" class="graf graf--p graf-after--p">I know. We need to take a step back and first go through these terms before getting to our gradient descent algorithm. So let us see exactly what is a cost function first.</p><h3 name="95dc" id="95dc" class="graf graf--h3 graf-after--p">Our model’s performance 💪</h3><p name="915d" id="915d" class="graf graf--p graf-after--h3">In order to quantify how well our model is doing at the classification task, we have the metric of accuracy. Our ultimate aim is for the model’s classification accuracy to increase.</p><p name="b3b4" id="b3b4" class="graf graf--p graf-after--p">The only set of parameters controlling how accurate our model is are the weights and the bias of our neuron based model.</p><p name="83a9" id="83a9" class="graf graf--p graf-after--p">The reason for this is because these are the values responsible for transforming the input image features and that help us get a prediction as to whether the image is that of a dog or a cat.</p><p name="b32b" id="b32b" class="graf graf--p graf-after--p">We saw earlier that a random set of weights and bias value can achieve 50% classification accuracy on the test set. That means there is a lot of scope for improvement of the model.</p><p name="6b9a" id="6b9a" class="graf graf--p graf-after--p">Mathematically, we should be able to modify the weights and bias values in such a way so that the model’s accuracy becomes the best. We want those perfect set of weights and bias so that the model classifies all of the images in our test set correctly.</p><figure name="f4da" id="f4da" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 259px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 36.9%;"></div><img class="graf-image" data-image-id="1*GzJ3i1wTMsKIOhEO9E_8yQ.png" data-width="1754" data-height="648" src="https://cdn-images-1.medium.com/max/800/1*GzJ3i1wTMsKIOhEO9E_8yQ.png"></div></figure><p name="823e" id="823e" class="graf graf--p graf-after--figure">We have to update the model’s parameters so that it gets the highest accuracy possible on the test data of our classification task.</p><figure name="1b7e" id="1b7e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 650px; max-height: 433px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.60000000000001%;"></div><img class="graf-image" data-image-id="1*KglMFPoQFHL_knOSpNxFUA.jpeg" data-width="650" data-height="433" src="https://cdn-images-1.medium.com/max/800/1*KglMFPoQFHL_knOSpNxFUA.jpeg"></div></figure><p name="8dd7" id="8dd7" class="graf graf--p graf-after--figure">Consider a mathematical function like the one below:</p><figure name="1788" id="1788" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 164px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 23.400000000000002%;"></div><img class="graf-image" data-image-id="1*e4Bpx81HzTzLY5cj8scGXQ.png" data-width="1596" data-height="374" src="https://cdn-images-1.medium.com/max/800/1*e4Bpx81HzTzLY5cj8scGXQ.png"></div></figure><p name="c1ef" id="c1ef" class="graf graf--p graf-after--figure">In calculus, the maxima (or minima) of any function can be found out by</p><ul class="postList"><li name="868c" id="868c" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">taking the first order differential of the function</strong> and equating it to 0. The point found in this way can be the point of maximum or minimum.</li><li name="350b" id="350b" class="graf graf--li graf-after--li">Then, we substitute these values (the point we just found) into the <strong class="markup--strong markup--li-strong">second order differential</strong> of the function and if the value is positive i.e. &gt; 0 then that point(s) represent the point(s) of local minima else local maxima.</li></ul><p name="1569" id="1569" class="graf graf--p graf-after--li">If we look at the computational graph of our neuron-based model for an image consisting of just 2 features, it looks like the following:</p><figure name="765e" id="765e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 206px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 29.4%;"></div><img class="graf-image" data-image-id="1*Tuyubpz2kR0axX3omNAkMQ.png" data-width="1968" data-height="578" src="https://cdn-images-1.medium.com/max/800/1*Tuyubpz2kR0axX3omNAkMQ.png"></div></figure><p name="dbed" id="dbed" class="graf graf--p graf-after--figure">The final value we obtain is a real value between 0 and 1 and we use that to make a prediction as to whether the image is that of a dog or a cat.</p><p name="ee6a" id="ee6a" class="graf graf--p graf-after--p">Consider the following output values by the model on the same image from time to time:</p><pre name="28df" id="28df" class="graf graf--pre graf-after--p">0.52<br>0.56<br>0.61<br>0.67<br>0.78<br>0.80<br>0.85<br>0.89<br>0.98</pre><p name="579c" id="579c" class="graf graf--p graf-after--pre">By looking at these values for the same image, you would say that the model is becoming more and more confident that the image does in-fact belong to that of a cat (values &gt; 0.5 are considered cat images in the current implementation. It’s really up-to you how you want to structure your data.)</p><p name="7a9b" id="7a9b" class="graf graf--p graf-after--p">Although the model is becoming more and more confident with its predictions, the actual prediction still remains the same i.e. a cat. For all of these values, the final prediction of the model is a cat.</p><p name="9529" id="9529" class="graf graf--p graf-after--p">This clearly highlights a <strong class="markup--strong markup--p-strong">big issue with using the accuracy as an optimization measure for obtaining the model’s optimal weights and biases</strong>.</p><p name="266d" id="266d" class="graf graf--p graf-after--p">We could have modeled a function centered around accuracy and maximizing it would have been our objective.</p><p name="7f66" id="7f66" class="graf graf--p graf-after--p">The number of images correctly classified is not a smooth function of the weights and biases in the network. For the most part, <strong class="markup--strong markup--p-strong">making small changes to the weights and biases won’t cause any change at all in the number of training images classified correctly</strong>.</p><p name="2bd9" id="2bd9" class="graf graf--p graf-after--p">That makes it difficult to figure out how to change the weights and biases to get improved performance. This is visible from the example that we just considered.</p><p name="f615" id="f615" class="graf graf--p graf-after--p">Although the model was getting more confident, the accuracy will never reflect this and hence, the model won’t make these sort of improvements.</p><p name="03f3" id="03f3" class="graf graf--p graf-after--p">What we need instead is <strong class="markup--strong markup--p-strong">a proxy measure</strong> that is somewhat related to the accuracy and is also a smooth function of the weights and the bias.</p><h3 name="e0b7" id="e0b7" class="graf graf--h3 graf-after--p">Enter: The loss function 📉</h3><p name="44e1" id="44e1" class="graf graf--p graf-after--h3">This is a binary classification problem. That means that we can have just two classes: 0 or 1.</p><p name="f098" id="f098" class="graf graf--p graf-after--p">In a perfect world, our model would output a 0 for a dog and a 1 for a cat and in that case it would achieve 100% accuracy. Outputting a 0 for a dog or a 1 for a cat would show the model’s 100% percent confidence in its predictions. This doesn’t really happen in the real world scenario (at least not yet!).</p><p name="5481" id="5481" class="graf graf--p graf-after--p">Our untrained model will sound confused at first. It won’t be too sure about its predictions. So it might output values like <code class="markup--code markup--p-code">0.51, 0.49, 0.514</code> etc. Just because the threshold is crossed and the prediction <strong class="markup--strong markup--p-strong">happens to be right</strong>, doesn’t mean that our model is well trained.</p><p name="e556" id="e556" class="graf graf--p graf-after--p">From the discussion above, one thing is clear. We need to close in on the gap between the model’s output and the actual output. Lesser the gap, the better our model is at its predictions and the more confidence it shows while predicting.</p><p name="a4fe" id="a4fe" class="graf graf--p graf-after--p">This means that for a dog image, we want our model to output values as close to 0 as possible and similarly, for cat images we want our model to output values as close to 1 as possible.</p><p name="b270" id="b270" class="graf graf--p graf-after--p">This is what leads us to what is known as the <strong class="markup--strong markup--p-strong">loss / error / cost </strong>term for our model. The loss function essentially models the difference between our model’s prediction and the actual output. Ideally, if these two values are far apart, the loss value or the error value should be higher. Similarly, if these two values are closer, the error value should be low.</p><p name="cbbf" id="cbbf" class="graf graf--p graf-after--p">Given this sort of error function as a proxy for our model’s performance, we would like to <strong class="markup--strong markup--p-strong">minimize the value of this loss function<em class="markup--em markup--p-em">.</em></strong></p><p name="79c6" id="79c6" class="graf graf--p graf-after--p">What do you think? Is the loss function simply the distance between <code class="markup--code markup--p-code">y_predicted</code> and <code class="markup--code markup--p-code">y_actual</code> ?</p><pre name="d9e8" id="d9e8" class="graf graf--pre graf-after--p">|y_actual - y_predicted|</pre><p name="828b" id="828b" class="graf graf--p graf-after--pre">Well, we can use this specific loss function for every image and average out the loss for the entire training set to get the loss for the entire epoch. But, this is not really suitable.</p><p name="977d" id="977d" class="graf graf--p graf-after--p">The loss function below is known as the <strong class="markup--strong markup--p-strong">absolute difference loss function</strong></p><figure name="36c0" id="36c0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 121px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 17.299999999999997%;"></div><img class="graf-image" data-image-id="1*d030zdG7e43z4xFCvxO4LQ.png" data-width="1738" data-height="300" src="https://cdn-images-1.medium.com/max/800/1*d030zdG7e43z4xFCvxO4LQ.png"></div></figure><p name="003d" id="003d" class="graf graf--p graf-after--figure">However, it makes sense to consider this loss function for minimizing, because at the end of the day, this is the exact proxy measure that we talked about earlier that lets us in on how well our model is performing.</p><p name="4533" id="4533" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">J </strong>is a common notation for the loss function and <strong class="markup--strong markup--p-strong">Θ</strong> represents out model’s parameters i.e. the weights and biases.</p><p name="4a30" id="4a30" class="graf graf--p graf-after--p">But, instead of taking this function as our loss function, we end up considering the following function.</p><figure name="e9b6" id="e9b6" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 100px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 14.299999999999999%;"></div><img class="graf-image" data-image-id="1*6Z1EmO0V--Um338O4gwhKw.png" data-width="1764" data-height="252" src="https://cdn-images-1.medium.com/max/800/1*6Z1EmO0V--Um338O4gwhKw.png"></div></figure><p name="d341" id="d341" class="graf graf--p graf-after--figure">This function is known as the <strong class="markup--strong markup--p-strong">squared error</strong>. We simply took the difference between the actual output <code class="markup--code markup--p-code">y </code>and the predicted output <code class="markup--code markup--p-code">y^</code> and we squared that value (hence the name) and divided it by 2.</p><p name="3001" id="3001" class="graf graf--p graf-after--p">One of the major reasons for preferring the squared error instead of the absolute error is that the squared error is <strong class="markup--strong markup--p-strong">everywhere differentiable</strong>, while the absolute error is not (its derivative is undefined at 0).</p><p name="c40c" id="c40c" class="graf graf--p graf-after--p">To optimize the squared error, you can just set its derivative equal to 0 and solve; to optimize the absolute error often requires more complex techniques.</p><p name="e6cb" id="e6cb" class="graf graf--p graf-after--p">Additionally, the benefits of squaring include:</p><ul class="postList"><li name="870c" id="870c" class="graf graf--li graf-after--p">Squaring <strong class="markup--strong markup--li-strong">always gives a positive value</strong>, so the sum will not be zero. We talk about sum here because we will add up the loss or the error values for every image in our training dataset and then we will average out to find the loss for the entire batch of training examples.</li><li name="bcd3" id="bcd3" class="graf graf--li graf-after--li">Squaring emphasizes larger differences — a feature that turns out to be both good and bad (think of the effect outliers have).</li></ul><p name="b51a" id="b51a" class="graf graf--p graf-after--li">Consider the graphs of the absolute error and squared errors respectively below.</p><figure name="f177" id="f177" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 509px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 72.7%;"></div><img class="graf-image" data-image-id="1*xUGYWm3HBpYqqjZKHE0akQ.png" data-width="2158" data-height="1568" src="https://cdn-images-1.medium.com/max/800/1*xUGYWm3HBpYqqjZKHE0akQ.png"></div><figcaption class="imageCaption">Absolute error. We can have y^ — 0 or y^ — 1 because the actual label can be 0 or 1. So, two lines are depicted above.</figcaption></figure><figure name="6fcb" id="6fcb" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 520px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 74.3%;"></div><img class="graf-image" data-image-id="1*msqMb-F25hH6_fz0jEKfqA.png" data-width="1834" data-height="1362" src="https://cdn-images-1.medium.com/max/800/1*msqMb-F25hH6_fz0jEKfqA.png"></div><figcaption class="imageCaption">Similarly, we have (y^ — 1)² or (y^)² as the squared error functions.</figcaption></figure><p name="98bf" id="98bf" class="graf graf--p graf-after--figure">For the time being, forget the fact that we apply a sigmoid activation function to the output of the neuron before making predictions using it. Then there would be no minimum or maximum value defined for the absolute error as is clear from the graph of this function.</p><p name="842d" id="842d" class="graf graf--p graf-after--p">But, if we look at the parabolic graph for the squared function, we can see the bottom tip which is the minima of this function and this occurs at <code class="markup--code markup--p-code">ŷ</code>= 0 or 1 depending upon which one we are talking about. But, there’s a well defined minima for this function and hence it is easier to optimize.</p><figure name="975d" id="975d" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://giphy.com/embed/8GoeF2PXOoF2w/twitter/iframe" width="100%" height="300" frameborder="0" scrolling="no"></iframe></figure><p name="93b8" id="93b8" class="graf graf--p graf-after--figure">That’s how I feel right now after working on this article for so long. I hope you were able to grasp all the important concepts we have discussed so far. There is still a long way to go before we wrap up.</p><p name="39f1" id="39f1" class="graf graf--p graf-after--p">You might wanna take a break and come back to the article, because we will start with the gradient descent algorithm now.</p><p name="f71b" id="f71b" class="graf graf--p graf-after--p">Ok then. Hope you are back and ready to go on!</p><p name="1f28" id="1f28" class="graf graf--p graf-after--p">Let’s officially define the error function that we will be using here. It is known as the mean squared error and the formula is as follows.</p><figure name="82de" id="82de" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 85px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 12.1%;"></div><img class="graf-image" data-image-id="1*xkb0tTBQu3_QodtbladJ-g.png" data-width="1848" data-height="224" src="https://cdn-images-1.medium.com/max/800/1*xkb0tTBQu3_QodtbladJ-g.png"></div></figure><p name="11b1" id="11b1" class="graf graf--p graf-after--figure">We calculate the squared error for each image in our training set and then we <strong class="markup--strong markup--p-strong">find the average</strong> of these values and this represents the overall error of the model on our training set.</p><h3 name="1cdb" id="1cdb" class="graf graf--h3 graf-after--p">Minimizing the loss function ⬇️</h3><p name="33f0" id="33f0" class="graf graf--p graf-after--h3">Consider the example of a single image with just 2 features from before. Two features means that we have 2 corresponding weight values and a bias value. In all, we have 3 parameters for our model.</p><figure name="24e4" id="24e4" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 214px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 30.5%;"></div><img class="graf-image" data-image-id="1*eTKSIHxNRfqIF5nptyUmXA.png" data-width="1932" data-height="590" src="https://cdn-images-1.medium.com/max/800/1*eTKSIHxNRfqIF5nptyUmXA.png"></div></figure><p name="134c" id="134c" class="graf graf--p graf-after--figure">We want to find values for our weights and the bias that minimize the value of our loss function. Since this is a multi-variable equation, that means we would have to deal with partial derivatives of the loss function corresponding to each of our variables <code class="markup--code markup--p-code">w1, w2 and b</code> .</p><figure name="fb7e" id="fb7e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 154px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 22%;"></div><img class="graf-image" data-image-id="1*z9b6gTqEKEL7cFv9Zs7ycA.png" data-width="1688" data-height="372" src="https://cdn-images-1.medium.com/max/800/1*z9b6gTqEKEL7cFv9Zs7ycA.png"></div></figure><p name="8bbc" id="8bbc" class="graf graf--p graf-after--figure">This might seem simple enough to do because we just have 3 different variables.</p><p name="8961" id="8961" class="graf graf--p graf-after--p">However, if we consider the task at hand i.e. cat v/s dog image classification using our single neuron-based model, we have <code class="markup--code markup--p-code">12288</code> weights.</p><p name="53a2" id="53a2" class="graf graf--p graf-after--p">Doing multivariate optimization with so many variables is computationally inefficient and is not tractable. Hence, we resort to alternatives and approximations.</p><p name="af76" id="af76" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Fun Fact:</strong> a typical deep neural network model has millions of weights and biases 🎃.</p><p name="00b1" id="00b1" class="graf graf--p graf-after--p">We are all set to learn about the gradient descent algorithm now.</p><figure name="1622" id="1622" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://giphy.com/embed/l2JHY6zkjwNRmNiGk/twitter/iframe" width="100%" height="435" frameborder="0" scrolling="no"></iframe></figure><p name="4b31" id="4b31" class="graf graf--p graf-after--figure">If there’s one algorithm that’s used in almost every Machine Learning model, it’s <strong class="markup--strong markup--p-strong">Gradient Descent</strong>.</p><p name="7425" id="7425" class="graf graf--p graf-after--p">This is the algorithm that helps our model <strong class="markup--strong markup--p-strong">learn</strong>. Without the learning capability, any machine learning model is essentially as good as a random guessing model.</p><p name="f684" id="f684" class="graf graf--p graf-after--p">It’s the learning capability granted by the gradient descent algorithm that makes machine learning and deep learning models so cool.</p><p name="ece4" id="ece4" class="graf graf--p graf-after--p">The aim of this algorithm is to minimize the value of our loss function (surprise surprise!). And we want to do this in an efficient manner.</p><p name="18b6" id="18b6" class="graf graf--p graf-after--p">As discussed before, the fastest way would be to find out second order derivatives of loss function with respect to the model’s parameters. But, that is computationally expensive.</p><p name="f5a5" id="f5a5" class="graf graf--p graf-after--p">The meat of the gradient descent algorithm is the process of getting to the lowest error value. <a href="https://en.wikipedia.org/wiki/Gradient_descent#An_analogy_for_understanding_gradient_descent" data-href="https://en.wikipedia.org/wiki/Gradient_descent#An_analogy_for_understanding_gradient_descent" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Wikipedia</a> has a great analogy for the gradient descent algorithm:</p><p name="542f" id="542f" class="graf graf--p graf-after--p">The basic intuition behind gradient descent can be illustrated by a hypothetical scenario.</p><p name="52c2" id="52c2" class="graf graf--p graf-after--p">A person is stuck in the mountains and is trying to get down (i.e. trying to find the minima). There is heavy fog such that visibility is extremely low. Therefore, the path down the mountain is not visible, so they must use local information to find the minima.</p><p name="68ff" id="68ff" class="graf graf--p graf-after--p">They can use the method of gradient descent, which involves looking at the steepness of the hill at his current position, then proceeding in the direction with the steepest descent (i.e. downhill).</p><p name="acf7" id="acf7" class="graf graf--p graf-after--p">If they were trying to find the top of the mountain (i.e. the maxima), then they would proceed in the direction with the steepest ascent (i.e. uphill). Using this method, they would eventually find their way.</p><p name="c263" id="c263" class="graf graf--p graf-after--p">However, assume also that the steepness of the hill is not immediately obvious with simple observation, but rather it requires a sophisticated instrument to measure, which the person happens to have at the moment.</p><p name="0fad" id="0fad" class="graf graf--p graf-after--p">It takes quite some time to measure the steepness of the hill with the instrument, thus they should minimize their use of the instrument if they want to get down the mountain before sunset.</p><p name="ca98" id="ca98" class="graf graf--p graf-after--p">The difficulty then is choosing the frequency at which they should measure the steepness of the hill so as not to go off track.</p><p name="7f32" id="7f32" class="graf graf--p graf-after--p">In this analogy,</p><ul class="postList"><li name="cbf0" id="cbf0" class="graf graf--li graf-after--p">the person represents our <strong class="markup--strong markup--li-strong">learning algorithm</strong>, and</li><li name="70fd" id="70fd" class="graf graf--li graf-after--li">the path taken down the mountain represents the <strong class="markup--strong markup--li-strong">sequence of parameter updates</strong> that our model will eventually explore.</li><li name="7cb5" id="7cb5" class="graf graf--li graf-after--li">The steepness of the hill represents the <strong class="markup--strong markup--li-strong">slope of the error surface</strong> at that point.</li><li name="4fc6" id="4fc6" class="graf graf--li graf-after--li">The instrument used to measure steepness is <strong class="markup--strong markup--li-strong">differentiation</strong> (the slope of the error surface can be calculated by taking the derivative of the squared error function at that point). This is the approximation that we do when we apply gradient descent. We don’t really know the minimum point, but <strong class="markup--strong markup--li-strong">we do know the direction</strong> that will lead us to the minima (local or global) and we take a step in that direction.</li><li name="2d97" id="2d97" class="graf graf--li graf-after--li">The direction the person chooses to travel in aligns with the gradient of the error surface at that point.</li><li name="4215" id="4215" class="graf graf--li graf-after--li">The amount of time they travel before taking another measurement is the <strong class="markup--strong markup--li-strong">learning rate of the algorithm</strong>. This is essentially how big a step our model (or the person going downhill) decides to take each time.</li></ul><p name="9a5d" id="9a5d" class="graf graf--p graf-after--li">For a deeper understanding and the mathematics behind the gradient descent algorithm, I would recommend going through:</p><div name="8c5f" id="8c5f" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://hackernoon.com/gradient-descent-aynk-7cbe95a778da" data-href="https://hackernoon.com/gradient-descent-aynk-7cbe95a778da" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://hackernoon.com/gradient-descent-aynk-7cbe95a778da"><strong class="markup--strong markup--mixtapeEmbed-strong">Gradient Descent: All You Need to Know</strong><br><em class="markup--em markup--mixtapeEmbed-em">Gradient Descent is THE most used learning algorithm in Machine Learning. This post shows you almost everything you…</em>hackernoon.com</a><a href="https://hackernoon.com/gradient-descent-aynk-7cbe95a778da" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="5add85e5340fde21d1af6b40b8d8bc74" data-thumbnail-img-id="1*f9a162GhpMbiTVTAua_lLQ.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*f9a162GhpMbiTVTAua_lLQ.png);"></a></div><h3 name="2893" id="2893" class="graf graf--h3 graf-after--mixtapeEmbed">Does Gradient Descent Always find the Global Minima?</h3><figure name="cda2" id="cda2" class="graf graf--figure graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 436px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 62.2%;"></div><img class="graf-image" data-image-id="1*uEGOBMCM78L7ah2Ahc3TeA.png" data-width="1440" data-height="896" src="https://cdn-images-1.medium.com/max/800/1*uEGOBMCM78L7ah2Ahc3TeA.png"></div><figcaption class="imageCaption">Source: <a href="https://www.oreilly.com/" data-href="https://www.oreilly.com/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://www.oreilly.com/</a></figcaption></figure><p name="89b9" id="89b9" class="graf graf--p graf-after--figure">That is one of the most common figures associated with gradient descent and it shows our error function to be a smooth convex function. It also shows that the gradient descent algorithm finds the global minimum.</p><p name="bdb7" id="bdb7" class="graf graf--p graf-after--p">The loss function that we have defined is known as the <strong class="markup--strong markup--p-strong">mean squared loss function</strong>. The function takes in two parameters: <code class="markup--code markup--p-code">ŷ</code> which is the model’s prediction for a given input <code class="markup--code markup--p-code">x</code> and then we have <code class="markup--code markup--p-code">y</code> which is the actual label corresponding to that input.</p><p name="5723" id="5723" class="graf graf--p graf-after--p">Clearly, our function is a convex function with respect to the prediction <code class="markup--code markup--p-code">ŷ</code>.</p><p name="cc8c" id="cc8c" class="graf graf--p graf-after--p">However, if you look at the expanded equation of the loss function that we wrote a few paragraphs before, you will see that the prediction value is not something that we can control directly.</p><p name="df26" id="df26" class="graf graf--p graf-after--p">We can simply control the model’s weights and the bias value i.e. the model’s parameters and they in turn control the prediction.</p><p name="99b3" id="99b3" class="graf graf--p graf-after--p">Although, the mean squared loss function is convex with respect to the the prediction of the model <code class="markup--code markup--p-code">ŷ</code> but the convexity property that we are really interested in is with respect to the model’s parameters.</p><p name="2876" id="2876" class="graf graf--p graf-after--p">We want our loss function to be a smooth convex function of our model’s weights.</p><p name="8fc9" id="8fc9" class="graf graf--p graf-after--p">Since a typical machine learning model has millions of weights (our model has 12288 of them and it’s a single neuron), our loss function may contain multiple local minima points and the gradient descent may not necessarily find the global minima.</p><p name="0552" id="0552" class="graf graf--p graf-after--p">It all depends on the <strong class="markup--strong markup--p-strong">step sizes that the model takes,<em class="markup--em markup--p-em"> </em></strong>i.e. the learning rate, how long the model is trained for, and the amount of training data our model has.</p><h3 name="5df3" id="5df3" class="graf graf--h3 graf-after--p">Let the gradients flow ✈️</h3><p name="8280" id="8280" class="graf graf--p graf-after--h3">Ok. So by now we know that these are the two equations by which our model will learn to get better at cats and dogs image classification.</p><figure name="1ece" id="1ece" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 184px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.3%;"></div><img class="graf-image" data-image-id="1*5BMHYemAg2fVAnQBbJtedg.png" data-width="1450" data-height="382" src="https://cdn-images-1.medium.com/max/800/1*5BMHYemAg2fVAnQBbJtedg.png"></div></figure><p name="4fb3" id="4fb3" class="graf graf--p graf-after--figure">The <code class="markup--code markup--p-code">α</code> represents the learning rate for our gradient descent algorithm i.e. the step size for going down the hill. The term(s) next to <code class="markup--code markup--p-code">α</code> represent the <strong class="markup--strong markup--p-strong">gradients </strong>of the loss function corresponding to the weights and the bias respectively.</p><p name="f054" id="f054" class="graf graf--p graf-after--p">The question here is, how do we actually calculate these gradients?</p><p name="f480" id="f480" class="graf graf--p graf-after--p">Let us look at our computation graph for the simple model we have been working with up until now.</p><figure name="a4a2" id="a4a2" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 477px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 68.10000000000001%;"></div><img class="graf-image" data-image-id="1*p9B9v21-GWh3h1VmZJ1xOQ.png" data-width="1970" data-height="1342" src="https://cdn-images-1.medium.com/max/800/1*p9B9v21-GWh3h1VmZJ1xOQ.png"></div></figure><p name="5d41" id="5d41" class="graf graf--p graf-after--figure">We already know how the activations flow in the forward direction. We take the input image’s features, transform them linearly, apply the sigmoid activation on the resulting value, and finally we have our activation which we then use to make a prediction.</p><p name="584e" id="584e" class="graf graf--p graf-after--p">What we will look at in this section is the flow of gradients along the red line in the diagram above by a process known as the <strong class="markup--strong markup--p-strong">backpropagation.</strong></p><figure name="7cae" id="7cae" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://giphy.com/embed/ph6ewybUlGbW8/twitter/iframe" width="100%" height="327" frameborder="0" scrolling="no"></iframe><figcaption class="imageCaption">Source: <a href="https://gph.is/19oJD8F" data-href="https://gph.is/19oJD8F" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://gph.is/19oJD8F</a></figcaption></figure><p name="26a5" id="26a5" class="graf graf--p graf-after--figure">Don’t fret!</p><p name="1005" id="1005" class="graf graf--p graf-after--p">By the end of this section, you will have a clear understanding of what back-propagation is doing for us and the math behind it (at least for our specific model).</p><blockquote name="67d9" id="67d9" class="graf graf--pullquote graf-after--p">Backpropagation, is essentially the chain rule of calculus applied to computational graphs.</blockquote><figure name="2b69" id="2b69" class="graf graf--figure graf-after--pullquote"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 243px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 34.599999999999994%;"></div><img class="graf-image" data-image-id="1*_KMMFvRP5X9kC59brI0ykw.png" data-width="1876" data-height="650" src="https://cdn-images-1.medium.com/max/800/1*_KMMFvRP5X9kC59brI0ykw.png"></div></figure><p name="f748" id="f748" class="graf graf--p graf-after--figure">Say we wanted to find the partial derivative of the variable <code class="markup--code markup--p-code">y</code> with respect to <code class="markup--code markup--p-code">x</code> in the figure above. We can’t find that out directly because there are 3 other variables involved in the computational graph.</p><pre name="c0eb" id="c0eb" class="graf graf--pre graf-after--p">x --&gt; (Some computation) --&gt; A<br>A --&gt; (Some computation) --&gt; B<br>B --&gt; (Some computation) --&gt; C<br>C --&gt; (Some computation) --&gt; y</pre><p name="28dd" id="28dd" class="graf graf--p graf-after--pre">So, we do this process iteratively going <strong class="markup--strong markup--p-strong">backwards<em class="markup--em markup--p-em"> </em></strong>in the computation graph.</p><p name="2a07" id="2a07" class="graf graf--p graf-after--p">We first find out the partial derivative of the output <code class="markup--code markup--p-code">y</code> with respect to the variable <code class="markup--code markup--p-code">C</code> . Then we use the <a href="https://en.wikipedia.org/wiki/Chain_rule" data-href="https://en.wikipedia.org/wiki/Chain_rule" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">chain rule</a> of calculus and we determine the partial derivative with respect to the variable <code class="markup--code markup--p-code">B</code> and so on until we get the partial derivative we are looking for.</p><p name="4598" id="4598" class="graf graf--p graf-after--p">That’s all there is to backpropagation.</p><p name="7cf0" id="7cf0" class="graf graf--p graf-after--p">Obviously, actually finding out the derivatives in a computation graph is something that is tricky and scares most people off. However, we have a relatively simply model here at our hands and it is pretty easy to do backpropagation here. So, without further adieu, let’s get on with the mathematics of backprop on our computation graph.</p><h4 name="0b0a" id="0b0a" class="graf graf--h4 graf-after--p">Step 1: dJ/dŷ</h4><p name="5198" id="5198" class="graf graf--p graf-after--h4">The final equation for our loss function is:</p><figure name="2b71" id="2b71" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 192px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 27.400000000000002%;"></div><img class="graf-image" data-image-id="1*lex7D1q4YLvVDytRBqLXJA.png" data-width="1840" data-height="504" src="https://cdn-images-1.medium.com/max/800/1*lex7D1q4YLvVDytRBqLXJA.png"></div></figure><p name="c133" id="c133" class="graf graf--p graf-after--figure">The partial derivative of the loss function with respect to the activation of our model <code class="markup--code markup--p-code">ŷ</code> is:</p><figure name="0248" id="0248" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 383px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 54.6%;"></div><img class="graf-image" data-image-id="1*uyQtipJwArqLBJhwD3u_GQ.png" data-width="1658" data-height="906" src="https://cdn-images-1.medium.com/max/800/1*uyQtipJwArqLBJhwD3u_GQ.png"></div></figure><p name="b559" id="b559" class="graf graf--p graf-after--figure">That was pretty easy!</p><p name="2dcb" id="2dcb" class="graf graf--p graf-after--p">Let’s move on one step <strong class="markup--strong markup--p-strong">backward and calculate our next partial derivative. </strong>This will take us one step closer to the actual gradients we want to calculate.</p><h4 name="841b" id="841b" class="graf graf--h4 graf-after--p">Step 2: dJ/dA</h4><p name="112c" id="112c" class="graf graf--p graf-after--h4">This is the point where we apply the chain rule we mentioned before. So, to calculate the partial derivative of the loss function with respect to the linear transformed output i.e. the output of our model <strong class="markup--strong markup--p-strong">before </strong>applying the sigmoid activation.:</p><figure name="895c" id="895c" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 133px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 19%;"></div><img class="graf-image" data-image-id="1*QkfTUtyhc0Hj75_uA0hIOQ.png" data-width="1582" data-height="300" src="https://cdn-images-1.medium.com/max/800/1*QkfTUtyhc0Hj75_uA0hIOQ.png"></div></figure><p name="ec9a" id="ec9a" class="graf graf--p graf-after--figure">The first part of this equation is the value we had calculated in step 1. We can simply use that here.</p><p name="4637" id="4637" class="graf graf--p graf-after--p">The essential thing to calculate here is the partial derivative of our model’s prediction with respect to the linearly transformed output, also known as the <strong class="markup--strong markup--p-strong">logit.</strong></p><p name="6cd3" id="6cd3" class="graf graf--p graf-after--p">Let’s look at the equation for our model’s prediction in terms of the logit.</p><figure name="8ff0" id="8ff0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 125px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 17.8%;"></div><img class="graf-image" data-image-id="1*56NMarxBAbwGyJEye2M-NQ.png" data-width="1558" data-height="278" src="https://cdn-images-1.medium.com/max/800/1*56NMarxBAbwGyJEye2M-NQ.png"></div></figure><p name="f517" id="f517" class="graf graf--p graf-after--figure">We have simply written the formula for the sigmoid activation function.</p><p name="fc27" id="fc27" class="graf graf--p graf-after--p">Derivative of our model’s final output w.r.t the logit simply means the partial derivative of the sigmoid function with respect to it’s input. Let us have a look at a derivation of the derivative 😎.</p><figure name="070a" id="070a" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 456px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 65.10000000000001%;"></div><img class="graf-image" data-image-id="1*oA-sBWiG8FmgNeEYpnAYFA.png" data-width="1910" data-height="1244" src="https://cdn-images-1.medium.com/max/800/1*oA-sBWiG8FmgNeEYpnAYFA.png"></div></figure><p name="8187" id="8187" class="graf graf--p graf-after--figure">That’s quite a bit of math. But it’s just basic differential calculus here.</p><p name="3d40" id="3d40" class="graf graf--p graf-after--p">If you are not interested in the math and the derivation of this, you can simply look at the final values of each of these partial derivatives. You will need those to build your model from scratch.</p><p name="bfda" id="bfda" class="graf graf--p graf-after--p">Moving on, we can further simplify this equation.</p><figure name="a6ea" id="a6ea" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 429px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 61.3%;"></div><img class="graf-image" data-image-id="1*5iQufIdGbgzcRGAT8wR3HA.png" data-width="1648" data-height="1010" src="https://cdn-images-1.medium.com/max/800/1*5iQufIdGbgzcRGAT8wR3HA.png"></div></figure><p name="70e1" id="70e1" class="graf graf--p graf-after--figure">Substituting this value in our earlier equation we get:</p><figure name="53e1" id="53e1" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 481px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 68.7%;"></div><img class="graf-image" data-image-id="1*ft1cdjtiR2tDsBbanFu1Ng.png" data-width="1540" data-height="1058" src="https://cdn-images-1.medium.com/max/800/1*ft1cdjtiR2tDsBbanFu1Ng.png"></div></figure><p name="d4ba" id="d4ba" class="graf graf--p graf-after--figure">Phew! That was a lot of math, right?</p><figure name="e246" id="e246" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://giphy.com/embed/EDt1m8p5hqXG8/twitter/iframe" width="100%" height="300" frameborder="0" scrolling="no"></iframe></figure><p name="a4a4" id="a4a4" class="graf graf--p graf-after--figure">But we are not done yet. There’s still one more step to go in this backpropagation algorithm.</p><p name="71b6" id="71b6" class="graf graf--p graf-after--p">We need to find the partial derivatives with respect to the weights and the bias yet.</p><p name="1e91" id="1e91" class="graf graf--p graf-after--p">Let’s move on and see how we can do that.</p><h4 name="a138" id="a138" class="graf graf--h4 graf-after--p">Step 3: dJ / dW and dJ / db</h4><p name="2c53" id="2c53" class="graf graf--p graf-after--h4">We need the partial derivative of the loss function corresponding to <strong class="markup--strong markup--p-strong">each of the weights.</strong> But since we are resorting to <strong class="markup--strong markup--p-strong">vectorization, </strong>we can find it all in one go. That is why we have been using the notation capital italic <code class="markup--code markup--p-code"><em class="markup--em markup--p-em">W</em></code> instead of smaller <code class="markup--code markup--p-code">w1 or w2 or any other individual weights.</code></p><figure name="8b38" id="8b38" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 199px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.4%;"></div><img class="graf-image" data-image-id="1*tV5DsdbJPAmqjbCD5cSlKQ.png" data-width="1646" data-height="468" src="https://cdn-images-1.medium.com/max/800/1*tV5DsdbJPAmqjbCD5cSlKQ.png"></div></figure><p name="0a14" id="0a14" class="graf graf--p graf-after--figure">We’ll show the derivation for the weights and leave the bias portion for you to do.</p><figure name="bd98" id="bd98" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 415px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 59.3%;"></div><img class="graf-image" data-image-id="1*wJQi2eDpfhaglkvbqEXcLA.png" data-width="1346" data-height="798" src="https://cdn-images-1.medium.com/max/800/1*wJQi2eDpfhaglkvbqEXcLA.png"></div></figure><h3 name="e389" id="e389" class="graf graf--h3 graf-after--figure">Let’s Code! 💻</h3><p name="2c8c" id="2c8c" class="graf graf--p graf-after--h3">Let us put all the math we learned in the last section into a simple function that takes in the activations vector<code class="markup--code markup--p-code">A</code> and the true output labels vector<code class="markup--code markup--p-code">Y</code> and computes the gradients of our loss with respect to the weights and the bias.</p><figure name="62d4" id="62d4" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 224px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 31.900000000000002%;"></div><img class="graf-image" data-image-id="1*YXA2wWNqRSIYvkFm5coFFQ.png" data-width="2286" data-height="730" src="https://cdn-images-1.medium.com/max/800/1*YXA2wWNqRSIYvkFm5coFFQ.png"></div></figure><ul class="postList"><li name="072e" id="072e" class="graf graf--li graf-after--figure">The term <code class="markup--code markup--li-code">dY_hat</code> represents the partial derivative of our loss function with respect to the final output value <code class="markup--code markup--li-code">ŷ</code> and as shown in the previous section, that value is <code class="markup--code markup--li-code">A — Y</code> .</li><li name="38eb" id="38eb" class="graf graf--li graf-after--li">Then we have <code class="markup--code markup--li-code">dA</code> which is the partial derivative with respect to the linearly transformed output, <code class="markup--code markup--li-code">A</code> .</li><li name="a8ff" id="a8ff" class="graf graf--li graf-after--li">The next term is <code class="markup--code markup--li-code">dW</code> and we see that we have done matrix multiplication here to get that value. We divide by <code class="markup--code markup--li-code">m</code> to get average gradient over the entire training set. That is standard. But why have we done matrix multiplication here?</li></ul><p name="5740" id="5740" class="graf graf--p graf-after--li">In the last figure of the previous section, we see that <code class="markup--code markup--p-code">dJ/dW = dJ/dA * X</code> .</p><p name="edc9" id="edc9" class="graf graf--p graf-after--p">Let’s take a closer look at the dimensions of the two quantities involved here. Our matrix <code class="markup--code markup--p-code">X</code> would be <code class="markup--code markup--p-code">12288-by-m</code> where each image has <code class="markup--code markup--p-code">12288</code> features and we have <code class="markup--code markup--p-code">m</code> training samples.</p><p name="238a" id="238a" class="graf graf--p graf-after--p">The dimensions of the quantity <code class="markup--code markup--p-code">dJ/dA</code> would be the same as <code class="markup--code markup--p-code">A</code> and that is nothing but a single real value per training sample i.e. <code class="markup--code markup--p-code">1-by-m</code> .</p><p name="1a3e" id="1a3e" class="graf graf--p graf-after--p">Also, the dimension of the quantity <code class="markup--code markup--p-code">dJ/dW</code> should be the same as <code class="markup--code markup--p-code">W</code> because ultimately, we have to subtract the gradients from the original weight values. So the two matrices must have the same dimensions.</p><p name="6101" id="6101" class="graf graf--p graf-after--p">The matrix <code class="markup--code markup--p-code">W</code> has a dimension of <code class="markup--code markup--p-code">12288-by-1</code> . So, we need <code class="markup--code markup--p-code">dJ/dW</code> to be <code class="markup--code markup--p-code">12288-by-1</code> as well. For that to happen:</p><figure name="624b" id="624b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 160px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 22.900000000000002%;"></div><img class="graf-image" data-image-id="1*L80MmSmMxvBaGd3zx1cFcg.png" data-width="1598" data-height="366" src="https://cdn-images-1.medium.com/max/800/1*L80MmSmMxvBaGd3zx1cFcg.png"></div></figure><p name="0dd2" id="0dd2" class="graf graf--p graf-after--figure">Hope this clears up why we have written the code as <code class="markup--code markup--p-code">np.matmul(X, dZ.T)</code> before taking the average.</p><p name="abae" id="abae" class="graf graf--p graf-after--p">Let us bring all of this together in a single comprehensive function that does the following things:</p><ul class="postList"><li name="8736" id="8736" class="graf graf--li graf-after--p">Takes the input X and model parameters W and b and applies forward propagation on the input.</li><li name="4504" id="4504" class="graf graf--li graf-after--li">Does backpropagation to compute the gradients.</li><li name="86ba" id="86ba" class="graf graf--li graf-after--li">Applies gradient descent on the model’s parameters.</li><li name="6bff" id="6bff" class="graf graf--li graf-after--li">Calculates loss on the validation dataset to measure the model’s performance and we use this to see if the model generalize well or not.</li></ul><figure name="06ac" id="06ac" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 332px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 47.4%;"></div><img class="graf-image" data-image-id="1*4UHcq51DrFL9l4T1iVi0OA.png" data-width="2270" data-height="1076" src="https://cdn-images-1.medium.com/max/800/1*4UHcq51DrFL9l4T1iVi0OA.png"></div></figure><p name="2b8d" id="2b8d" class="graf graf--p graf-after--figure">We have this combined function that does forward and backward propagation for us. So, a single call to this function and our model would have processed and learned once from our entire training set.</p><figure name="0ef3" id="0ef3" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 468px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.8%;"></div><img class="graf-image" data-image-id="1*-oIUF5ZRoQbsH7cAZQOqqw.png" data-width="2274" data-height="1520" src="https://cdn-images-1.medium.com/max/800/1*-oIUF5ZRoQbsH7cAZQOqqw.png"></div></figure><p name="7cec" id="7cec" class="graf graf--p graf-after--figure">That’s the main function that essentially does the same process but multiple times.</p><p name="d363" id="d363" class="graf graf--p graf-after--p">Exposing the model multiple times to the training dataset and the model learns something new every time.</p><p name="4037" id="4037" class="graf graf--p graf-after--p">So, these iterations are known as <strong class="markup--strong markup--p-strong">epochs</strong> and we have this <code class="markup--code markup--p-code">model</code> function that for every epoch goes over the set of steps provided earlier.</p><p name="5827" id="5827" class="graf graf--p graf-after--p">Finally, we are at the stage where we can train our model and see how much a single neuron can actually learn as far as our cats vs image classification task is concerned.</p><figure name="6e17" id="6e17" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://giphy.com/embed/RrVzUOXldFe8M/twitter/iframe" width="100%" height="326" frameborder="0" scrolling="no"></iframe><figcaption class="imageCaption">Source: <a href="https://gph.is/1hkTw7l" data-href="https://gph.is/1hkTw7l" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://gph.is/1hkTw7l</a></figcaption></figure><figure name="02a1" id="02a1" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 189px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 27%;"></div><img class="graf-image" data-image-id="1*K5HpDPgWyFPlOabVkEyVSg.png" data-width="2266" data-height="612" src="https://cdn-images-1.medium.com/max/800/1*K5HpDPgWyFPlOabVkEyVSg.png"></div></figure><figure name="2b14" id="2b14" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 367px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 52.400000000000006%;"></div><img class="graf-image" data-image-id="1*xn1f8_qdFHMtrR76n-TUDA.png" data-width="2256" data-height="1182" src="https://cdn-images-1.medium.com/max/800/1*xn1f8_qdFHMtrR76n-TUDA.png"></div><figcaption class="imageCaption">Sweet!</figcaption></figure><p name="74b5" id="74b5" class="graf graf--p graf-after--figure">That means that on the <strong class="markup--strong markup--p-strong">unseen validation/test set,</strong> our model is able to predict if the image is that of a cat or a dog with 61% accuracy. That’s awesome if you ask me, because we have been able to achieve a 10% boost over a random guesser simply by using a <strong class="markup--strong markup--p-strong">single neuron.</strong></p><p name="f4cd" id="f4cd" class="graf graf--p graf-after--p">Just for fun, I plotted the training and validation losses for the model’s training for all 5000 epochs.</p><figure name="6772" id="6772" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 469px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 67%;"></div><img class="graf-image" data-image-id="1*8wpjij06mca2fCKLFIL0SQ.png" data-width="2266" data-height="1518" src="https://cdn-images-1.medium.com/max/800/1*8wpjij06mca2fCKLFIL0SQ.png"></div></figure><p name="7500" id="7500" class="graf graf--p graf-after--figure">The training and validation losses decrease smoothly over time.</p><p name="0c1b" id="0c1b" class="graf graf--p graf-after--p">One final thing before I leave you to play around with the code. Let’s provide a custom image to our model, an image not a part of our dataset and see if it is able to predict correctly if the image is a cat or a dog.</p><figure name="d2db" id="d2db" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 473px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 67.5%;"></div><img class="graf-image" data-image-id="1*JEfTr5PybQUvsRXBoaysuw.png" data-width="2266" data-height="1530" src="https://cdn-images-1.medium.com/max/800/1*JEfTr5PybQUvsRXBoaysuw.png"></div></figure><p name="1f81" id="1f81" class="graf graf--p graf-after--figure">Voila! That’s correct. Indeed, that’s a cat.</p><figure name="580d" id="580d" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://giphy.com/embed/236NFDvNm82WPyBSC6/twitter/iframe" width="100%" height="300" frameborder="0" scrolling="no"></iframe></figure><p name="1077" id="1077" class="graf graf--p graf-after--figure">For the complete code, please check out:</p><div name="6c65" id="6c65" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/edorado93/Power-Of-A-Neuron" data-href="https://github.com/edorado93/Power-Of-A-Neuron" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/edorado93/Power-Of-A-Neuron"><strong class="markup--strong markup--mixtapeEmbed-strong">edorado93/Power-Of-A-Neuron</strong><br><em class="markup--em markup--mixtapeEmbed-em">Cats vs Dogs Image Classification using Logistic Regression - edorado93/Power-Of-A-Neuron</em>github.com</a><a href="https://github.com/edorado93/Power-Of-A-Neuron" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="6213ddc4b8119e306400c871a8d55055" data-thumbnail-img-id="0*aMoJ41sn-Q0QKuG-" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*aMoJ41sn-Q0QKuG-);"></a></div></div></div></section>
</section>
</article></body></html>