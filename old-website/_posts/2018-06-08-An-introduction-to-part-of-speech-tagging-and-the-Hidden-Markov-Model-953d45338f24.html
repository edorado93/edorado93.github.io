---
layout: post
title: "An introduction to part-of-speech tagging and the Hidden Markov Model"
comments: True
---

<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>An introduction to part-of-speech tagging and the Hidden Markov Model</title><style>
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 100%;
        margin: auto;
      }
      section {
        width: 100%;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 100%;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">An introduction to part-of-speech tagging and the Hidden Markov Model</h1>
</header>
<section data-field="subtitle" class="p-summary">
by Sachin Malhotra and Divya Godayal
</section>
<section data-field="body" class="e-content">
<section name="a852" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="2004" id="2004" class="graf graf--h3 graf--leading graf--title"></h3><div name="ca19" id="ca19" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">by </em><a href="https://medium.com/@sachinmalhotra" data-href="https://medium.com/@sachinmalhotra" class="markup--anchor markup--p-anchor" target="_blank"><em class="markup--em markup--p-em">Sachin Malhotra</em></a><em class="markup--em markup--p-em"> and </em><a href="https://medium.com/@divyagodayal" data-href="https://medium.com/@divyagodayal" class="markup--anchor markup--p-anchor" target="_blank"><em class="markup--em markup--p-em">Divya Godayal</em></a></div></div><div class="section-inner sectionLayout--fullWidth"><figure name="fc1d" id="fc1d" class="graf graf--figure graf--layoutFillWidth graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.699999999999996%;"></div><img class="graf-image" data-image-id="1*f6e0uf5PX17pTceYU4rbCA.jpeg" data-width="1526" data-height="865" data-is-featured="true" src="https://cdn-images-1.medium.com/max/2000/1*f6e0uf5PX17pTceYU4rbCA.jpeg"></div><figcaption class="imageCaption">Source: <a href="https://english.stackexchange.com/questions/218058/parts-of-speech-and-functions-bob-made-a-book-collector-happy-the-other-day" data-href="https://english.stackexchange.com/questions/218058/parts-of-speech-and-functions-bob-made-a-book-collector-happy-the-other-day" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener" target="_blank">https://english.stackexchange.com/questions/218058/parts-of-speech-and-functions-bob-made-a-book-collector-happy-the-other-day</a></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="1108" id="1108" class="graf graf--p graf-after--figure">Let‚Äôs go back into the times when we had no language to communicate. The only way we had was sign language. That‚Äôs how we usually communicate with our dog at home, right? When we tell him, ‚ÄúWe love you, Jimmy,‚Äù he responds by wagging his tail. This doesn‚Äôt mean he knows what we are actually saying. Instead, his response is simply because he understands the language of emotions and gestures more than words.</p><p name="ff39" id="ff39" class="graf graf--p graf-after--p">We as humans have developed an understanding of a lot of nuances of the natural language more than any animal on this planet. That is why when we say ‚ÄúI LOVE you, honey‚Äù vs when we say ‚ÄúLets make LOVE, honey‚Äù we mean different things. Since we understand the basic difference between the two phrases, our responses are very different. It is these very intricacies in natural language understanding that we want to teach to a machine.</p><p name="3683" id="3683" class="graf graf--p graf-after--p">What this could mean is when your future robot dog hears ‚ÄúI love you, Jimmy‚Äù, he would know LOVE is a Verb. He would also realize that it‚Äôs an emotion that we are expressing to which he would respond in a certain way. And maybe when you are telling your partner ‚ÄúLets make LOVE‚Äù, the dog would just stay out of your business üòõ.</p><p name="2a13" id="2a13" class="graf graf--p graf-after--p">This is just an example of how teaching a robot to communicate in a language known to us can make things easier.</p><p name="8ca4" id="8ca4" class="graf graf--p graf-after--p">The primary use case being highlighted in this example is how important it is to understand the difference in the usage of the word LOVE, in different contexts.</p><h3 name="1ef3" id="1ef3" class="graf graf--h3 graf-after--p">Part-of-Speech Tagging</h3><p name="4395" id="4395" class="graf graf--p graf-after--h3">From a very small age, we have been made accustomed to identifying part of speech tags. For example, reading a sentence and being able to identify what words act as nouns, pronouns, verbs, adverbs, and so on. All these are referred to as the part of speech tags.</p><p name="3419" id="3419" class="graf graf--p graf-after--p">Let‚Äôs look at the Wikipedia definition for them:</p><blockquote name="8576" id="8576" class="graf graf--pullquote graf-after--p">In corpus linguistics, <strong class="markup--strong markup--pullquote-strong">part-of-speech tagging</strong> (<strong class="markup--strong markup--pullquote-strong">POS tagging</strong> or <strong class="markup--strong markup--pullquote-strong">PoS tagging</strong> or <strong class="markup--strong markup--pullquote-strong">POST</strong>), also called <strong class="markup--strong markup--pullquote-strong">grammatical tagging</strong> or <strong class="markup--strong markup--pullquote-strong">word-category disambiguation</strong>, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context‚Ää‚Äî‚Ääi.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs,¬†etc.</blockquote><p name="43d8" id="43d8" class="graf graf--p graf-after--pullquote">Identifying part of speech tags is much more complicated than simply mapping words to their part of speech tags. This is because POS tagging is not something that is generic. It is quite possible for a single word to have a different part of speech tag in different sentences based on different contexts. That is why it is impossible to have a generic mapping for POS tags.</p><p name="8248" id="8248" class="graf graf--p graf-after--p">As you can see, it is not possible to manually find out different part-of-speech tags for a given corpus. New types of contexts and new words keep coming up in dictionaries in various languages, and manual POS tagging is not scalable in itself. That is why we rely on machine-based POS tagging.</p><p name="891d" id="891d" class="graf graf--p graf-after--p">Before proceeding further and looking at how part-of-speech tagging is done, we should look at why POS tagging is necessary and where it can be used.</p><h3 name="1ef5" id="1ef5" class="graf graf--h3 graf-after--p">Why Part-of-Speech tagging?</h3><p name="40eb" id="40eb" class="graf graf--p graf-after--h3">Part-of-Speech tagging in itself may not be the solution to any particular NLP problem. It is however something that is done as a pre-requisite to simplify a lot of different problems. Let us consider a few applications of POS tagging in various NLP tasks.</p><h4 name="de24" id="de24" class="graf graf--h4 graf-after--p">Text to Speech Conversion</h4><p name="7bf1" id="7bf1" class="graf graf--p graf-after--h4">Let us look at the following sentence:</p><pre name="e606" id="e606" class="graf graf--pre graf-after--p">They refuse to permit us to obtain the refuse permit.</pre><p name="c21f" id="c21f" class="graf graf--p graf-after--pre">The word <code class="markup--code markup--p-code">refuse</code> is being used twice in this sentence and has two different meanings here. <em class="markup--em markup--p-em">refUSE (/</em>r…ôÀàfyoÕûoz/)is a verb meaning ‚Äúdeny,‚Äù while <em class="markup--em markup--p-em">REFuse(/</em>ÀàrefÀåyoÕûos/) is a noun meaning ‚Äútrash‚Äù (that is, they are not homophones). Thus, we need to know which word is being used in order to pronounce the text correctly. (For this reason, text-to-speech systems usually perform POS-tagging.)</p><p name="47f9" id="47f9" class="graf graf--p graf-after--p">Have a look at the part-of-speech tags generated for this very sentence by the <a href="https://www.nltk.org/" data-href="https://www.nltk.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">NLTK</a> package.</p><pre name="314e" id="314e" class="graf graf--pre graf-after--p">&gt;&gt;&gt; text = word_tokenize(&quot;They refuse to permit us to obtain the refuse permit&quot;)<br>&gt;&gt;&gt; nltk.pos_tag(text)<br>[(&#39;They&#39;, &#39;PRP&#39;), (<strong class="markup--strong markup--pre-strong">&#39;refuse&#39;, &#39;VBP&#39;</strong>), (&#39;to&#39;, &#39;TO&#39;), (&#39;permit&#39;, &#39;VB&#39;), (&#39;us&#39;, &#39;PRP&#39;),<br>(&#39;to&#39;, &#39;TO&#39;), (&#39;obtain&#39;, &#39;VB&#39;), (&#39;the&#39;, &#39;DT&#39;), (<strong class="markup--strong markup--pre-strong">&#39;refuse&#39;, &#39;NN&#39;</strong>), (&#39;permit&#39;, &#39;NN&#39;)]</pre><p name="c7d6" id="c7d6" class="graf graf--p graf-after--pre">As we can see from the results provided by the NLTK package, POS tags for both <em class="markup--em markup--p-em">refUSE and REFuse </em>are different. Using these two different POS tags for our text to speech converter can come up with a different set of sounds.</p><p name="b0d3" id="b0d3" class="graf graf--p graf-after--p">Similarly, let us look at yet another classical application of POS tagging: word sense disambiguation.</p><h4 name="a7e6" id="a7e6" class="graf graf--h4 graf-after--p">Word Sense Disambiguation</h4><p name="6e57" id="6e57" class="graf graf--p graf-after--h4">Let‚Äôs talk about this kid called Peter. Since his mother is a neurological scientist, she didn‚Äôt send him to school. His life was devoid of science and math.</p><p name="93e3" id="93e3" class="graf graf--p graf-after--p">One day she conducted an experiment, and made him sit for a math class. Even though he didn‚Äôt have any prior subject knowledge, Peter thought he aced his first test. His mother then took an example from the test and published it as below. (Kudos to her!)</p><figure name="16f8" id="16f8" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 763px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 108.89999999999999%;"></div><img class="graf-image" data-image-id="1*nHkemiTJp9FJV-wjxBlGrA.png" data-width="1164" data-height="1268" src="https://cdn-images-1.medium.com/max/800/1*nHkemiTJp9FJV-wjxBlGrA.png"></div><figcaption class="imageCaption">Word-sense Disambiguation example‚Ää‚Äî‚ÄäMy son Peter‚Äôs first Maths¬†problem.</figcaption></figure><p name="d744" id="d744" class="graf graf--p graf-after--figure">Words often occur in different senses as different parts of speech. For example:</p><ul class="postList"><li name="9430" id="9430" class="graf graf--li graf-after--p">She saw a <strong class="markup--strong markup--li-strong">bear.</strong></li><li name="e92a" id="e92a" class="graf graf--li graf-after--li">Your efforts will <strong class="markup--strong markup--li-strong">bear</strong> fruit.</li></ul><p name="a38d" id="a38d" class="graf graf--p graf-after--li">The word <strong class="markup--strong markup--p-strong">bear </strong>in the above sentences has completely different senses, but more importantly one is a noun and other is a verb. Rudimentary word sense disambiguation is possible if you can tag words with their POS tags.</p><p name="4e2f" id="4e2f" class="graf graf--p graf-after--p">Word-sense disambiguation (WSD) is identifying which sense of a word (that is, which meaning) is used in a sentence, when the word has multiple meanings.</p><p name="c491" id="c491" class="graf graf--p graf-after--p">Try to think of the multiple meanings for this sentence:</p><p name="dbe2" id="dbe2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Time flies like an arrow</strong></p><p name="1614" id="1614" class="graf graf--p graf-after--p">Here are the various interpretations of the given sentence. The meaning and hence the part-of-speech might vary for each word.</p><figure name="6f87" id="6f87" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 638px; max-height: 479px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 75.1%;"></div><img class="graf-image" data-image-id="1*rOL82uvp5WPXtlhnFZEHKA.jpeg" data-width="638" data-height="479" src="https://cdn-images-1.medium.com/max/800/1*rOL82uvp5WPXtlhnFZEHKA.jpeg"></div><figcaption class="imageCaption">Part-of-speech tags define the meaning of a sentence based on the¬†context</figcaption></figure><p name="9b46" id="9b46" class="graf graf--p graf-after--figure">As we can clearly see, there are multiple interpretations possible for the given sentence. Different interpretations yield different kinds of part of speech tags for the words.This information, if available to us, can help us find out the exact version / interpretation of the sentence and then we can proceed from there.</p><p name="aae5" id="aae5" class="graf graf--p graf-after--p">The above example shows us that a single sentence can have three different POS tag sequences assigned to it that are equally likely. That means that it is very important to know what specific meaning is being conveyed by the given sentence whenever it‚Äôs appearing. <strong class="markup--strong markup--p-strong">This is word sense disambiguation, as we are trying to find out THE sequence.</strong></p><p name="72fe" id="72fe" class="graf graf--p graf-after--p">These are just two of the numerous applications where we would require POS tagging. There are other applications as well which require POS tagging, like Question Answering, Speech Recognition, Machine Translation, and so on.</p><p name="1977" id="1977" class="graf graf--p graf-after--p">Now that we have a basic knowledge of different applications of POS tagging, let us look at how we can go about actually assigning POS tags to all the words in our corpus.</p><h3 name="307d" id="307d" class="graf graf--h3 graf-after--p">Types of POS¬†taggers</h3><p name="5e20" id="5e20" class="graf graf--p graf-after--h3">POS-tagging algorithms fall into two distinctive groups:</p><ul class="postList"><li name="3b0e" id="3b0e" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Rule-Based POS Taggers</strong></li><li name="3539" id="3539" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Stochastic POS Taggers</strong></li></ul><p name="3f46" id="3f46" class="graf graf--p graf-after--li"><a href="https://en.wikipedia.org/wiki/Brill_tagger" data-href="https://en.wikipedia.org/wiki/Brill_tagger" class="markup--anchor markup--p-anchor" title="Brill tagger" rel="noopener" target="_blank">E. Brill‚Äôs tagger</a>, one of the first and most widely used English POS-taggers, employs rule-based algorithms. Let us first look at a very brief overview of what rule-based tagging is all about.</p><h4 name="bceb" id="bceb" class="graf graf--h4 graf-after--p">Rule-Based Tagging</h4><p name="fb8f" id="fb8f" class="graf graf--p graf-after--h4">Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods.</p><p name="2832" id="2832" class="graf graf--p graf-after--p">Typical rule-based approaches use contextual information to assign tags to unknown or ambiguous words. Disambiguation is done by analyzing the linguistic features of the word, its preceding word, its following word, and other aspects.</p><p name="2868" id="2868" class="graf graf--p graf-after--p">For example, if the preceding word is an article, then the word in question must be a noun. This information is coded in the form of rules.</p><p name="3aa7" id="3aa7" class="graf graf--p graf-after--p">Example of a rule:</p><blockquote name="c07b" id="c07b" class="graf graf--pullquote graf-after--p">If an ambiguous/unknown word X is preceded by a determiner and followed by a noun, tag it as an adjective.</blockquote><p name="4a5d" id="4a5d" class="graf graf--p graf-after--pullquote">Defining a set of rules manually is an extremely cumbersome process and is not scalable at all. So we need some automatic way of doing this.</p><p name="5476" id="5476" class="graf graf--p graf-after--p">The Brill‚Äôs tagger is a rule-based tagger that goes through the training data and finds out the set of tagging rules that best define the data and minimize POS tagging errors. The most important point to note here about Brill‚Äôs tagger is that the rules are not hand-crafted, but are instead found out using the corpus provided. The only feature engineering required is a <strong class="markup--strong markup--p-strong">set of rule templates </strong>that the model can use to come up with new features.</p><p name="6279" id="6279" class="graf graf--p graf-after--p">Let‚Äôs move ahead now and look at Stochastic POS tagging.</p><h4 name="6e3a" id="6e3a" class="graf graf--h4 graf-after--p">Stochastic Part-of-Speech Tagging</h4><p name="4d62" id="4d62" class="graf graf--p graf-after--h4">The term ‚Äòstochastic tagger‚Äô can refer to any number of different approaches to the problem of POS tagging. Any model which somehow incorporates frequency or probability may be properly labelled stochastic.</p><p name="488f" id="488f" class="graf graf--p graf-after--p">The simplest stochastic taggers disambiguate words based solely on the probability that a word occurs with a particular tag. In other words, the tag encountered most frequently in the training set with the word is the one assigned to an ambiguous instance of that word. The problem with this approach is that while it may yield a valid tag for a given word, it can also yield inadmissible sequences of tags.</p><p name="2beb" id="2beb" class="graf graf--p graf-after--p">An alternative to the word frequency approach is to calculate the probability of a given sequence of tags occurring. This is sometimes referred to as the <em class="markup--em markup--p-em">n-gram</em> approach, referring to the fact that the best tag for a given word is determined by the probability that it occurs with the n previous tags. This approach makes much more sense than the one defined before, because it considers the tags for individual words based on context.</p><p name="1208" id="1208" class="graf graf--p graf-after--p">The next level of complexity that can be introduced into a stochastic tagger combines the previous two approaches, using both tag sequence probabilities and word frequency measurements. This is known as the <strong class="markup--strong markup--p-strong">Hidden Markov Model (HMM)</strong>.</p><p name="cb43" id="cb43" class="graf graf--p graf-after--p">Before proceeding with what is a <strong class="markup--strong markup--p-strong">Hidden<em class="markup--em markup--p-em"> </em></strong>Markov Model, let us first look at what is a Markov Model. That will better help understand the meaning of the term <strong class="markup--strong markup--p-strong">Hidden<em class="markup--em markup--p-em"> </em></strong>in HMMs.</p><h4 name="73b6" id="73b6" class="graf graf--h4 graf-after--p">Markov Model</h4><p name="f813" id="f813" class="graf graf--p graf-after--h4">Say that there are only three kinds of weather conditions, namely</p><ul class="postList"><li name="518b" id="518b" class="graf graf--li graf-after--p">Rainy</li><li name="a5ed" id="a5ed" class="graf graf--li graf-after--li">Sunny</li><li name="5fe7" id="5fe7" class="graf graf--li graf-after--li">Cloudy</li></ul><p name="11f0" id="11f0" class="graf graf--p graf-after--li">Now, since our young friend we introduced above, Peter, is a small kid, he loves to play outside. He loves it when the weather is sunny, because all his friends come out to play in the sunny conditions.</p><p name="5f95" id="5f95" class="graf graf--p graf-after--p">He hates the rainy weather for obvious reasons.</p><p name="90fa" id="90fa" class="graf graf--p graf-after--p">Every day, his mother observe the weather in the morning (that is when he usually goes out to play) and like always, Peter comes up to her right after getting up and asks her to tell him what the weather is going to be like. Since she is a responsible parent, she want to answer that question as accurately as possible. But the only thing she has is a set of observations taken over multiple days as to how weather has been.</p><p name="1fe6" id="1fe6" class="graf graf--p graf-after--p">How does she make a prediction of the weather for today based on what the weather has been for the past N days?</p><p name="b040" id="b040" class="graf graf--p graf-after--p">Say you have a sequence. Something like this:</p><p name="d458" id="d458" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">Sunny, Rainy, Cloudy, Cloudy, Sunny, Sunny, Sunny, Rainy</code></p><p name="ffaa" id="ffaa" class="graf graf--p graf-after--p">So, the weather for any give day can be in any of the three states.</p><p name="a50a" id="a50a" class="graf graf--p graf-after--p">Let‚Äôs say we decide to use a Markov Chain Model to solve this problem. Now using the data that we have, we can construct the following state diagram with the labelled probabilities.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="a26e" id="a26e" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 508px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 50.8%;"></div><img class="graf-image" data-image-id="1*BW3GyPvHOg0zWbBb8VWaXA.png" data-width="1634" data-height="830" src="https://cdn-images-1.medium.com/max/1000/1*BW3GyPvHOg0zWbBb8VWaXA.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="bab1" id="bab1" class="graf graf--p graf-after--figure">In order to compute the probability of today‚Äôs weather given N previous observations, we will use the Markovian Property.</p><figure name="c31a" id="c31a" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 205px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 29.299999999999997%;"></div><img class="graf-image" data-image-id="1*mpCRGnxgzke50mjL0loOcw.png" data-width="1018" data-height="298" src="https://cdn-images-1.medium.com/max/800/1*mpCRGnxgzke50mjL0loOcw.png"></div></figure><p name="dca9" id="dca9" class="graf graf--p graf-after--figure">Markov Chain is essentially the simplest known Markov model, that is it obeys the Markov property.</p><p name="17fc" id="17fc" class="graf graf--p graf-after--p">The Markov property suggests that the distribution for a random variable in the future depends solely only on its distribution in the current state, and none of the previous states have any impact on the future states.</p><p name="e46b" id="e46b" class="graf graf--p graf-after--p">For a much more detailed explanation of the working of Markov chains, refer to <a href="https://towardsdatascience.com/introduction-to-markov-chains-50da3645a50d" data-href="https://towardsdatascience.com/introduction-to-markov-chains-50da3645a50d" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this</a> link.</p><p name="d537" id="d537" class="graf graf--p graf-after--p">Also, have a look at the following example just to see how probability of the current state can be computed using the formula above, taking into account the Markovian Property.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="eab7" id="eab7" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 528px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 52.800000000000004%;"></div><img class="graf-image" data-image-id="1*Gymjj1jZJHkwQXMf7rsNYw.png" data-width="1804" data-height="952" src="https://cdn-images-1.medium.com/max/1000/1*Gymjj1jZJHkwQXMf7rsNYw.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="2975" id="2975" class="graf graf--p graf-after--figure">Apply the Markov property in the following example.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="a97f" id="a97f" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 605px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 60.5%;"></div><img class="graf-image" data-image-id="1*FdMUaqeTj8dAsqJF2yAjQg.png" data-width="1762" data-height="1066" src="https://cdn-images-1.medium.com/max/1000/1*FdMUaqeTj8dAsqJF2yAjQg.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="a463" id="a463" class="graf graf--p graf-after--figure">We can clearly see that as per the Markov property, the probability of <code class="markup--code markup--p-code">tomorrow&#39;s</code> weather being Sunny depends solely on <code class="markup--code markup--p-code">today&#39;s</code> weather and not on <code class="markup--code markup--p-code">yesterday&#39;s</code>¬†.</p><p name="7ebf" id="7ebf" class="graf graf--p graf-after--p">Let us now proceed and see what is hidden in the Hidden Markov Models.</p><figure name="c72b" id="c72b" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://giphy.com/embed/3o85xJWLC5vZc34Pss/twitter/iframe" width="100%" height="300" frameborder="0" scrolling="no"></iframe></figure><h3 name="369e" id="369e" class="graf graf--h3 graf-after--figure">Hidden Markov¬†Model</h3><p name="9f91" id="9f91" class="graf graf--p graf-after--h3">It‚Äôs the small kid Peter again, and this time he‚Äôs gonna pester his new caretaker‚Ää‚Äî‚Ääwhich is you. (Ooopsy!!)</p><p name="511f" id="511f" class="graf graf--p graf-after--p">As a caretaker, one of the most important tasks for you is to tuck Peter into bed and make sure he is sound asleep. Once you‚Äôve tucked him in, you want to make sure he‚Äôs actually asleep and not up to some mischief.</p><p name="238d" id="238d" class="graf graf--p graf-after--p">You cannot, however, enter the room again, as that would surely wake Peter up. So all you have to decide are the noises that might come from the room. Either the room is <strong class="markup--strong markup--p-strong">quiet</strong> or there is <strong class="markup--strong markup--p-strong">noise</strong> coming from the room. These are your states.</p><p name="0f0c" id="0f0c" class="graf graf--p graf-after--p">Peter‚Äôs mother, before leaving you to this nightmare, said:</p><blockquote name="9046" id="9046" class="graf graf--pullquote graf-after--p">May the sound be with you¬†:)</blockquote><figure name="2fa9" id="2fa9" class="graf graf--figure graf--iframe graf-after--pullquote"><iframe src="https://giphy.com/embed/xlQNxSmaWxewM/twitter/iframe" width="100%" height="300" frameborder="0" scrolling="no"></iframe><figcaption class="imageCaption">Yes, thats what she meant¬†!</figcaption></figure><p name="3896" id="3896" class="graf graf--p graf-after--figure">His mother has given you the following state diagram. The diagram has some states, observations, and probabilities.</p><figure name="ea8f" id="ea8f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 331px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 47.3%;"></div><img class="graf-image" data-image-id="0*_i4JqwAZ9DdjO5Kt." data-width="748" data-height="354" src="https://cdn-images-1.medium.com/max/800/0*_i4JqwAZ9DdjO5Kt."></div><figcaption class="imageCaption">Hello Caretaker, this might help. ~Peters mother. Have fun¬†!</figcaption></figure><p name="6e80" id="6e80" class="graf graf--p graf-after--figure">Note that there is no direct correlation between sound from the room and Peter being asleep.</p><p name="db59" id="db59" class="graf graf--p graf-after--p">There are two kinds of probabilities that we can see from the state diagram.</p><ul class="postList"><li name="2ebe" id="2ebe" class="graf graf--li graf-after--p">One is the <strong class="markup--strong markup--li-strong">emission<em class="markup--em markup--li-em"> </em></strong>probabilities, which represent the probabilities of making certain observations given a particular state. For example, we have <code class="markup--code markup--li-code">P(noise | awake) = 0.5</code>¬†. This is an emission probability.</li><li name="80d4" id="80d4" class="graf graf--li graf-after--li">The other ones is <strong class="markup--strong markup--li-strong">transition<em class="markup--em markup--li-em"> </em></strong>probabilities, which represent the probability of transitioning to another state given a particular state. For example, we have <code class="markup--code markup--li-code">P(asleep | awake) = 0.4</code>¬†. This is a transition probability.</li></ul><p name="cbb7" id="cbb7" class="graf graf--p graf-after--li">The Markovian property applies in this model as well. So do not complicate things too much. Markov, your savior said:</p><blockquote name="b2d5" id="b2d5" class="graf graf--pullquote graf-after--p">Don‚Äôt go too much into the¬†history‚Ä¶</blockquote><p name="ef41" id="ef41" class="graf graf--p graf-after--pullquote">The Markov property, as would be applicable to the example we have considered here, would be that the probability of Peter being in a state depends ONLY on the previous state.</p><p name="c738" id="c738" class="graf graf--p graf-after--p">But there is a clear flaw in the Markov property. If Peter has been awake for an hour, then the probability of him falling asleep is higher than if has been awake for just 5 minutes. So, history matters. Therefore, the Markov state machine-based model is not completely correct. It‚Äôs merely a simplification.</p><p name="ada9" id="ada9" class="graf graf--p graf-after--p">The Markov property, although wrong, makes this problem very tractable.</p><p name="5c4b" id="5c4b" class="graf graf--p graf-after--p">We usually observe longer stretches of the child being awake and being asleep. If Peter is awake now, the probability of him staying awake is higher than of him going to sleep. Hence, the 0.6 and 0.4 in the above diagram.<code class="markup--code markup--p-code">P(awake | awake) = 0.6 and P(asleep | awake) = 0.4</code></p><figure name="255c" id="255c" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 145px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 20.7%;"></div><img class="graf-image" data-image-id="1*mJNBB5gdLR_Z6AlUKRfN6A.png" data-width="1340" data-height="278" src="https://cdn-images-1.medium.com/max/800/1*mJNBB5gdLR_Z6AlUKRfN6A.png"></div><figcaption class="imageCaption">The Transition probabilities matrix.</figcaption></figure><figure name="608a" id="608a" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 153px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 21.8%;"></div><img class="graf-image" data-image-id="1*YkEuELs83MGtCz2v_uEVXw.png" data-width="1422" data-height="310" src="https://cdn-images-1.medium.com/max/800/1*YkEuELs83MGtCz2v_uEVXw.png"></div><figcaption class="imageCaption">The Emission probabilities matrix.</figcaption></figure><p name="742f" id="742f" class="graf graf--p graf-after--figure">Before actually trying to solve the problem at hand using HMMs, let‚Äôs relate this model to the task of Part of Speech Tagging.</p><h4 name="c3cb" id="c3cb" class="graf graf--h4 graf-after--p">HMMs for Part of Speech¬†Tagging</h4><p name="4ad4" id="4ad4" class="graf graf--p graf-after--h4">We know that to model any problem using a Hidden Markov Model we need a set of observations and a set of possible states. The states in an HMM are hidden.</p><p name="b2a5" id="b2a5" class="graf graf--p graf-after--p">In the part of speech tagging problem, the <strong class="markup--strong markup--p-strong">observations</strong> are the words themselves in the given sequence.</p><p name="96e8" id="96e8" class="graf graf--p graf-after--p">As for the <strong class="markup--strong markup--p-strong">states</strong>, which are hidden, these would be the POS tags for the words.</p><p name="478c" id="478c" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">transition probabilities</strong> would be somewhat like <code class="markup--code markup--p-code">P(NP | VP)</code> that is, what is the probability of the current word having a tag of Verb Phrase given that the previous tag was a Noun Phrase.</p><p name="2870" id="2870" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Emission probabilities </strong>would be <code class="markup--code markup--p-code">P(john | NP) or P(will | VP)</code> that is, what is the probability that the word is, say, John given that the tag is a Noun Phrase.</p><p name="c2cb" id="c2cb" class="graf graf--p graf-after--p">Note that this is just an informal modeling of the problem to provide a very basic understanding of how the Part of Speech tagging problem can be modeled using an HMM.</p><h4 name="7e29" id="7e29" class="graf graf--h4 graf-after--p">How do we solve¬†this?</h4><p name="17c2" id="17c2" class="graf graf--p graf-after--h4">Coming back to our problem of taking care of Peter.</p><p name="1ce8" id="1ce8" class="graf graf--p graf-after--p">Irritated are we¬†? üòú.</p><p name="9deb" id="9deb" class="graf graf--p graf-after--p">Our problem here was that we have an initial state: Peter was awake when you tucked him into bed. After that, you recorded a sequence of observations, namely <strong class="markup--strong markup--p-strong">noise </strong>or <strong class="markup--strong markup--p-strong">quiet,</strong> at different time-steps. Using these set of observations and the initial state, you want to find out whether Peter would be awake or asleep after say N time steps.</p><p name="04ba" id="04ba" class="graf graf--p graf-after--p">We draw all possible transitions starting from the initial state. There‚Äôs an exponential number of branches that come out as we keep moving forward. So the model <strong class="markup--strong markup--p-strong">grows exponentially</strong> after a few time steps. Even without considering any observations. Have a look at the model expanding exponentially below.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="c35e" id="c35e" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 750px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 75%;"></div><img class="graf-image" data-image-id="1*pOOd-g0iFsLbjBIw1Fit2w.png" data-width="1234" data-height="926" src="https://cdn-images-1.medium.com/max/1000/1*pOOd-g0iFsLbjBIw1Fit2w.png"></div><figcaption class="imageCaption">S0 is Awake and S1 is Asleep. Exponential growth through the model because of the transitions.</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="15c7" id="15c7" class="graf graf--p graf-after--figure">If we had a set of states, we could calculate the probability of the sequence. But we don‚Äôt have the states. All we have are a sequence of observations. This is why this model is referred to as the <strong class="markup--strong markup--p-strong">Hidden </strong>Markov Model‚Ää‚Äî‚Ääbecause the actual states over time are hidden.</p><p name="cdf4" id="cdf4" class="graf graf--p graf-after--p">So, caretaker, if you‚Äôve come this far it means that you have at least a fairly good understanding of how the problem is to be structured. All that is left now is to use some algorithm / technique to actually solve the problem. For now, <strong class="markup--strong markup--p-strong">Congratulations on Leveling up!</strong></p><p name="284c" id="284c" class="graf graf--p graf-after--p graf--trailing">In the <a href="https://medium.freecodecamp.org/a-deep-dive-into-part-of-speech-tagging-using-viterbi-algorithm-17c8de32e8bc" data-href="https://medium.freecodecamp.org/a-deep-dive-into-part-of-speech-tagging-using-viterbi-algorithm-17c8de32e8bc" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">next article</a> of this two-part series, we will see how we can use a well defined algorithm known as the <strong class="markup--strong markup--p-strong">Viterbi Algorithm </strong>to decode the given sequence of observations given the model. See you there!</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@divyagodayal" class="p-author h-card">Divya Godayal</a> on <a href="https://medium.com/p/953d45338f24"><time class="dt-published" datetime="2018-06-08T19:31:14.123Z">June 8, 2018</time></a>.</p><p><a href="https://medium.com/@divyagodayal/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 15, 2018.</p></footer></article></body></html>
